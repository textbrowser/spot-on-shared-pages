<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html lang="en"> <!-- <![endif]-->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Higher-Order Theories of Consciousness (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Higher-Order Theories of Consciousness" />
<meta property="citation_author" content="Carruthers, Peter" />
<meta property="citation_author" content="Gennaro, Rocco" />
<meta property="citation_publication_date" content="2001/04/03" />
<meta name="DC.title" content="Higher-Order Theories of Consciousness" />
<meta name="DC.creator" content="Carruthers, Peter" />
<meta name="DC.creator" content="Gennaro, Rocco" />
<meta name="DCTERMS.issued" content="2001-04-03" />
<meta name="DCTERMS.modified" content="2026-01-09" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
                    <li role="menuitem"><a href="../../new.html">What's New</a></li>
                    <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li role="menuitem"><a href="../../published.html">Chronological</a></li>
                    <li role="menuitem"><a href="../../archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
                    <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
                    <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
                    <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
                    <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
                    <li role="menuitem"><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
                    <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
                    <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit" aria-label="search"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/consciousness-higher/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=consciousness-higher">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Higher-Order Theories of Consciousness</h1><div id="pubinfo"><em>First published Tue Apr 3, 2001; substantive revision Fri Jan 9, 2026</em></div>

<div id="preamble">

<p>
Higher-order theories of consciousness try to explain the difference
between unconscious and conscious mental states in terms of a relation
obtaining between the conscious state in question and a higher-order
representation of some sort (either a higher-order perception of that
state, or a higher-order thought about it). What is it that makes an
otherwise unconscious mental state a conscious state? According to
higher-order theories, what makes a mental state (M) conscious is a
higher-order representation directed at M. The most challenging
properties to explain are those involved in <em>phenomenal</em>
consciousness&mdash;the sort of state that has a <em>subjective</em>
dimension, that has &ldquo;feel&rdquo;, or that it is <em>like
something</em> to undergo. These properties will form the focus of
this article.</p>
</div> 

<div id="toc"><!--Entry Contents-->
<ul>
	<li><a href="#KindCons">1. Kinds of Consciousness</a></li>
	<li><a href="#MotiForHighOrdeAppr">2. The Motivation for a Higher-Order Approach</a></li>
	<li><a href="#HighOrdePercTheo">3. Higher-Order Perception Theory</a></li>
	<li><a href="#HighOrdeThouTheo1Actu">4. Higher-Order Thought Theory (1): Actualist</a></li>
	<li><a href="#HighOrdeThouTheo2Disp">5. Higher-Order Thought Theory (2): Dispositionalist</a></li>
	<li><a href="#SelfReprHybrHighOrdeTheo">6. Self-Representational and Hybrid Higher-Order Theories</a></li>
	<li><a href="#FurtObjeReplHighOrdeAppr">7. Further Objections and Replies to the Higher-Order Approach</a>
	<ul>
		<li><a href="#TargHOTs">7.1 Targetless HOTs </a></li>
		<li><a href="#ProbRock">7.2 The Problem of the Rock</a></li>
		<li><a href="#AnimInfa">7.3 Animals and Infants</a></li>
		<li><a href="#ExplPhenCons">7.4 Explaining Phenomenal Consciousness</a></li>
	</ul>
	</li>
	<li><a href="#HOTTheoPrefCortPFC">8. HOT Theory and the Prefrontal Cortex (PFC)</a></li>
	<li><a href="#Bib">Bibliography</a></li>
	<li><a href="#Aca">Academic Tools</a></li>
	<li><a href="#Oth">Other Internet Resources</a></li>
	<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr /></div>

<div id="main-text">

<h2 id="KindCons">1. Kinds of Consciousness</h2>

<p>
One of the advances made in the last few decades has been to
distinguish between different questions concerning consciousness (see
particularly: Rosenthal 1986; Dretske 1993; Block 1995; Lycan 1996).
Not everyone agrees on quite <em>which</em> distinctions need to be
drawn. But all are agreed that we should distinguish <em>creature</em>
consciousness from <em>mental-state</em> consciousness. It is one
thing to say <em>of an individual person or organism</em> that it is
conscious (either in general or of something in particular); and it is
quite another thing to say <em>of one of the mental states</em> of a
creature that it is conscious. Higher-order theories tend to focus
mainly on mental-state consciousness.</p>

<p>
It is also agreed that within creature-consciousness itself we should
distinguish between <em>intransitive</em> and <em>transitive</em>
variants. To say of an organism that it is conscious
<em>simpliciter</em> (intransitive) is to say just that it is awake,
as opposed to asleep or comatose. There don&rsquo;t appear to be any
deep philosophical difficulties lurking here (or at least, they
aren&rsquo;t difficulties specific to the topic of consciousness, as
opposed to mentality in general). But to say of an organism that it is
conscious <em>of such-and-such</em> (transitive) is normally to say at
least that it is <em>perceiving</em> such-and-such, or <em>aware
of</em> such-and-such. So we say of the mouse that it is conscious of
the cat outside its hole, in explaining why it doesn&rsquo;t come out;
meaning that it <em>perceives</em> the cat&rsquo;s presence. To
provide an account of transitive creature-consciousness would thus be
to attempt a theory of perception.</p>

<p>
There is a choice to be made concerning transitive
creature-consciousness, failure to notice which may be a potential
source of confusion. For we have to decide whether the perceptual
state in virtue of which an organism may be said to be
transitively-conscious of something must itself be a conscious one
(state-conscious&mdash;see below). If we say &ldquo;Yes&rdquo; then we
shall need to know more about the mouse than merely that it perceives
the cat if we are to be assured that it is conscious of the
cat&mdash;we shall need to establish that its percept of the cat is
itself conscious. If we say &ldquo;No&rdquo;, on the other hand, then
the mouse&rsquo;s perception of the cat will be sufficient for the
mouse to count as conscious of the cat; but we may have to say that
although it is conscious of the cat, the mental state in virtue of
which it is so conscious is not itself a conscious one! It may be best
to by-pass any danger of confusion here by avoiding the language of
transitive-creature-consciousness altogether. Nothing of importance
would be lost to us by doing this.</p>

<p>
Turning now to the notion of <em>mental-state consciousness</em>, the
major distinction here is between <em>phenomenal consciousness</em>,
on the one hand&mdash;which is a property of states that it is
<em>like something</em> to be in, that have a distinctive
&ldquo;feel&rdquo; (Nagel 1974)&mdash;and various
functionally-definable forms of <em>access consciousness</em>, on the
other (Block 1995). Most theorists believe that there are mental
states&mdash;such as occurrent thoughts or judgments&mdash;that are
access-conscious (in whatever is the correct functionally-definable
sense), but that are not phenomenally conscious. In contrast, there is
considerable dispute as to whether mental states can be
phenomenally-conscious without also being conscious in the
functionally-definable sense&mdash;and even more dispute about whether
phenomenal consciousness can be <em>reductively explained</em> in
functional and/or representational terms.</p>

<p>
There is nothing deeply problematic about functionally-definable
notions of mental-state consciousness, from a naturalistic
perspective. For mental functions and mental representations are the
staple fare of naturalistic accounts of the mind. But this leaves
plenty of room for dispute about the form that the correct functional
account should take. Some claim that for a state to be conscious in
the relevant sense is for it to be poised to have an impact on the
organism&rsquo;s decision-making processes (Kirk 1994; Dretske 1995;
Tye 1995, 2000), perhaps also with the additional requirement that
those processes should be distinctively <em>rational</em> ones (Block
1995). Others think that the relevant requirement for
access-consciousness is that the state should be suitably related to
higher-order representations&mdash;experiences and/or
thoughts&mdash;of that very state (Armstrong 1968, 1984; Rosenthal
1986, 1993, 2005; Dennett 1978a, 1991; Carruthers 1996, 2000, 2005;
Lycan 1987, 1996; Gennaro 2012).</p>

<p>
What <em>is</em> often thought to be naturalistically problematic, in
contrast, is phenomenal consciousness (Nagel 1974, 1986; Jackson 1982,
1986; McGinn 1991; Block 1995; Chalmers 1996). And what is really and
deeply controversial is whether phenomenal consciousness can be
<em>explained</em> in terms of some or other functionally-definable
notion. <em>Cognitive</em> (or <em>representational</em>) theories
maintain that it can. <em>Higher-order</em> cognitive theories
maintain that phenomenal consciousness can be reductively explained in
terms of representations (either experiences or thoughts) that are
higher-order. It is such theories that concern us here.</p>

<h2 id="MotiForHighOrdeAppr">2. The Motivation for a Higher-Order Approach</h2>

<p>
Higher-order theories, like cognitive/representational theories in
general, assume that the right <em>level</em> at which to seek an
explanation of phenomenal consciousness is a cognitive one, providing
an explanation in terms of some combination of <em>causal role</em>
and <em>intentional content</em>. All such theories claim that
phenomenal consciousness consists in a certain kind of intentional or
representational content (<em>analog</em> or
&ldquo;fine-grained&rdquo; in comparison with any concepts we possess)
figuring in a certain distinctive position in the causal architecture
of the mind. They must therefore maintain that these latter sorts of
mental property don&rsquo;t already implicate or presuppose phenomenal
consciousness. In fact, all cognitive accounts are united in rejecting
the thesis that the very properties of <em>mind</em> or
<em>mentality</em> already presuppose phenomenal consciousness, as
proposed by Searle (1992, 1995 [1997]) for example. The higher-order
approach does not attempt to reduce consciousness directly to
neurophysiology but rather its reduction is in mentalistic terms, that
is, by using such notions as thoughts and awareness. A
&ldquo;second-step&rdquo; reduction to neurophysiology is often
favored by higher-order theorists as well (see
 <a href="#HOTTheoPrefCortPFC">section 8</a>
 below as well).</p>

<p>
The major divide amongst representational theories of phenomenal
consciousness in general, is between accounts that are provided in
purely first-order terms and those that implicate higher-order
representations of one sort or another (see below). These higher-order
theorists will allow that first-order accounts&mdash;of the sort
defended by Dretske (1995) and Tye (1995), for example&mdash;can
already make some progress with the problem of consciousness.
According to first-order views, phenomenal consciousness consists in
analog or fine-grained contents that are available to the first-order
processes that guide thought and action. So a phenomenally-conscious
percept of red, for example, consists in a state with the analog
content <em>red</em> which is tokened in such a way as to feed into
thoughts about red, or into actions that are in one way or another
guided by redness. Now, the point to note in favor of such an account
is that it can explain the natural temptation to think that phenomenal
consciousness is in some sense <em>ineffable</em>, or
<em>indescribable</em>. This will be because such states have
fine-grained contents that can slip through the mesh of any conceptual
net. We can always distinguish many more shades of red than we have
concepts for, or could describe in language (other than
indexically&mdash;e.g., &ldquo;<em>That</em> shade&rdquo;).</p>

<p>
The main motivation behind higher-order theories of consciousness, in
contrast, derives from the belief that all (or at least most)
mental-state types admit of both conscious and unconscious varieties.
Almost everyone now accepts, for example, (post-Freud) that beliefs
and desires can be activated unconsciously. (Think, here, of the way
in which problems can apparently become resolved during sleep, or
while one&rsquo;s attention is directed to other tasks. Notice, too,
that appeals to unconscious intentional states are now routine in
cognitive science.) And then if we ask what makes the difference
between a conscious and an unconscious mental state, one natural
answer is that conscious states are states that we are <em>aware
of</em>. And if <em>awareness</em> is thought to be a form of
creature-consciousness (see
 <a href="#KindCons">section 1</a>
 above), then this will translate into the view that conscious states
are <em>states of which the subject is aware</em>, or states of which
the subject is creature-conscious. That is to say, these are states
that are the objects of some sort of higher-order
representation&mdash;whether a higher-order perception or experience,
or a higher-order thought. This is similar to the widely referenced
Transitivity Principle (TP):</p>

<dl class="sentag tag3em">
<dt>(TP)</dt>
<dd>A conscious mental state is a state whose subject is, in some way,
aware of being in it.</dd>
</dl>

<p>
On the other hand, a mental state of which a subject is completely
<em>un</em>aware is clearly an unconscious state. (See also
Lycan&rsquo;s (2001b) related &ldquo;simple argument&rdquo; for a
higher-order representation account of consciousness.)</p>

<p>
One crucial question, then, is whether perceptual states as well as
intentional states admit of both conscious and unconscious varieties.
The same would go for emotions, pains, and other mental states. Can
there be, for example, such a thing as an unconscious visual
perceptual state? Higher-order theorists are united in thinking that
there are. Armstrong (1968) uses the example of absent-minded driving
to make the point. Most of us at some time have had the rather
unnerving experience of &ldquo;coming to&rdquo; after having been
driving on &ldquo;automatic pilot&rdquo; while our attention was
directed elsewhere&mdash;perhaps having been day-dreaming or engaged
in intense conversation with a passenger. We were apparently not
consciously aware of much of the route we have recently taken, nor of
any of the obstacles we avoided on the way. Yet we must surely have
been <em>seeing</em>, or we would have crashed the car. Others have
used the example of blindsight (Carruthers 1989, 1996). This is a
condition in which subjects have had a portion of their primary visual
cortex destroyed, and apparently become blind in a region of their
visual field as a result. But it has now been known for some time that
if subjects are asked to <em>guess</em> at the properties of their
&ldquo;blind&rdquo; field (e.g., whether it contains a horizontal or
vertical grating, or whether it contains an &ldquo;X&rdquo; or an
&ldquo;O&rdquo;), they prove remarkably accurate. Subjects can also
reach out and grasp objects in their &ldquo;blind&rdquo; field with
something like 80% or more of normal accuracy, and can catch a ball
thrown from their &ldquo;blind&rdquo; side, all without conscious
awareness. (See Weiskrantz 1986, 1997, for details and
discussion.)</p>

<p>
A case for the existence of unconscious visual experience has also
been generated by the <em>two-systems theory</em> of vision proposed
and defended by Milner and Goodale (1995; see also Jacob &amp;
Jeannerod 2003; Glover 2004). They review a wide variety of kinds of
neurological and neuro-psychological evidence for the substantial
independence of two distinct visual systems, instantiated in the
temporal and parietal lobes respectively. They conclude that the
parietal lobes provide a set of specialized semi-independent modules
for the on-line visual control of <em>action</em>; whereas the
temporal lobes are primarily concerned with more off-line functions
such as <em>object recognition</em>. And only the perceptions
generated by the temporal-lobe system are phenomenally conscious, on
their account. (Note that this isn&rsquo;t the familiar distinction
between <em>what</em> and <em>where</em> visual systems but is rather
a successor to it. For the temporal-lobe system is supposed to have
access both to property information and to spatial information.
Instead, it is a distinction between a combined <em>what-where</em>
system located in the temporal lobes and a <em>how-to</em> or
action-guiding system located in the parietal lobes.)</p>

<p>
Milner and Goodale also point to a neurological syndrome called
<em>visual form agnosia</em>, which results from damage localized to
both temporal lobes, leaving primary visual cortex and the parietal
lobes intact. Such patients cannot recognize objects or shapes, and
may be capable of little conscious visual experience; but their
sensorimotor abilities remain largely intact.</p>

<p>
One particular patient&mdash;D.F.&mdash;has now been examined in
considerable detail. While D.F. is severely agnosic, she isn&rsquo;t
completely lacking in conscious visual experience. Her capacities to
perceive colors and textures are almost completely preserved. (Why
just these sub-modules in her temporal cortex should have been spared
isn&rsquo;t known.) As a result, she can sometimes guess the identity
of a presented object&mdash;recognizing a banana, say, from its yellow
color and the distinctive texture of its surface. But she is unable to
perceive the shape of the banana (whether straight or curved, say);
nor its orientation (upright or horizontal; pointing towards her or
across). Yet many of her sensorimotor abilities are close to
normal&mdash;she would be able to reach out and grasp the banana,
orienting her hand and wrist appropriately for its position and
orientation, and using a normal and appropriate finger grip. Under
experimental conditions it turns out that although D.F. is at chance
when identifying the orientation of a broad line or letter-box, she is
almost normal when posting a letter through a similarly-shaped slot
oriented at random angles. In the same way, although she is at chance
when trying to discriminate between rectangular blocks of very
different sizes, her reaching and grasping behaviors when asked to
pick up such a block are virtually indistinguishable from those of
normal controls. It is very hard to make sense of these data without
supposing that the sensorimotor perceptual system is functionally and
anatomically distinct from the object-recognition/conscious
system.</p>

<p>
But what implications does this have for phenomenal consciousness?
Must these unconscious percepts also be lacking in <em>phenomenal</em>
properties? Most people think so. While it may be possible to get
oneself to believe that the perceptions of the absent-minded car
driver can remain phenomenally conscious (perhaps lying outside of the
focus of attention, or being instantly forgotten), it is very hard to
believe that either blindsight percepts or D.F.&rsquo;s sensorimotor
perceptual states might be phenomenally conscious ones. For these
perceptions are ones to which the subjects of those states are
<em>blind</em>, and of which they <em>cannot</em> be aware. And the
question, then, is what makes the relevant difference? What is it
about a conscious perception that renders it <em>phenomenal</em>, that
a blindsight perceptual state would correspondingly lack? Higher-order
theorists are united in thinking that the relevant difference consists
in the presence of something <em>higher-order</em> in the first case
that is absent in the second. The same would go for the difference
between unconscious and conscious desires, emotions, pains, and so on.
The core intuition, again, is that a phenomenally conscious state will
be a state <em>of which the subject is aware</em>.</p>

<p>
What options does a first-order theorist have to resist this
conclusion? One is to deny that the data are as problematic as they
appear (as does Dretske 1995). It can be said that the unconscious
states in question lack the kind of fineness of grain and richness of
content necessary to count as genuinely <em>perceptual</em> states. On
this view, the contrast discussed above isn&rsquo;t really a
difference between conscious and unconscious perceptions, but rather
between conscious perceptions, on the one hand, and unconscious
belief-like states, on the other. Another option is to accept the
distinction between conscious and unconscious perceptions, and then to
explain that distinction in first-order terms. It might be said, for
example, that conscious perceptions are those that are available to
<em>belief</em> and <em>thought</em>, whereas unconscious ones are
those that are available to guide <em>movement</em> (Kirk 1994). A
final option is to bite the bullet, and insist that blindsight and
sensorimotor perceptual states are indeed phenomenally conscious while
not being <em>access</em>-conscious. (See Block 1995; Tye 1995; and
Nelkin 1996; all of whom defend versions of this view.) On this
account, blindsight percepts are phenomenally conscious states to
which the subjects of those states are blind. Higher-order theorists
will argue, of course, that none of these alternatives is acceptable
(see, e.g., Carruthers 2000; Rosenthal 2005).</p>

<p>
Most generally, then, higher-order theories of phenomenal
consciousness claim the following:</p>

<dl class="partag indent">
<dt class="bold">Higher Order Theory (In General):</dt>
<dd>A phenomenally conscious mental state is a mental state that is
(or is disposed to be) the object of a higher-order representation of
a certain sort (see below).</dd>
</dl>

<p>
Higher-order theorists do agree that one must normally become aware of
the lower-order state <em>non-inferentially</em> since mental states
can sometimes become targets of higher-order representation via
conscious inference without being phenomenally conscious. For example,
if I become aware of my unconscious desire to kill my boss because I
have consciously inferred it from a session with my psychiatrist, then
the characteristic phenomenal feel of such a conscious desire may be
absent.</p>

<p>
Still, there are then two main dimensions along which higher-order
theorists disagree amongst themselves. One concerns whether the
higher-order states in question are perception-like, on the one hand,
or thought-like, on the other. A thought is composed of or constituted
by concepts. Those taking the former option are higher-order
<em>perception</em> (often called &ldquo;inner-sense&rdquo;)
theorists, and those taking the latter option are higher-order
<em>thought</em> theorists. The two theories are therefore often
abbreviated as HOP (higher-order perception) and HOT (higher-order
thought) theory. The other general disagreement is internal to
higher-order thought approaches, and concerns whether a state is
conscious by virtue of being <em>disposed</em> to give rise to a
higher-order thought, or rather by virtue of being the <em>actual
target</em> of such a thought. These are the three main options that
will now concern us below in
 <a href="#HighOrdePercTheo">sections 3&ndash;5</a>.
 (Additional related theories will be considered in
 <a href="#SelfReprHybrHighOrdeTheo">section 6</a>.)</p>
 
<h2 id="HighOrdePercTheo">3. Higher-Order Perception Theory</h2>

<p>
According to this view, humans not only have first-order
non-conceptual and/or analog perceptions of states of their
environments and bodies, they also have second-order perceptions of
their first-order perceptions. The most popular version of
higher-order perception (HOP) theory holds, in addition, that humans
(and perhaps other animals) not only have sense-organs that scan the
environment/body to produce fine-grained representations, but they
also have <em>inner</em> senses which scan the first-order senses
(i.e., perceptual experiences) to produce equally fine-grained, but
higher-order, representations of those outputs. A version of this view
was first proposed by the British Empiricist philosopher John Locke
(1690). In our own time, it has been defended especially by Armstrong
(1968, 1984) and Lycan (1996, 2004).</p>

<p>
A terminological point: &ldquo;<em>inner</em>-sense theory&rdquo;
should more strictly be called &ldquo;<em>higher-order</em>-sense
theory&rdquo;, since we of course have senses that are physically
&ldquo;inner&rdquo;, such as pain-perception and internal
touch-perception, that aren&rsquo;t intended to fall under its scope.
For these are first-order senses on a par with vision and hearing,
differing only in that their purpose is to detect properties of the
body, rather than of the external world (Hill 2004). According to the
sort of higher-order theory that is presently under discussion, these
senses, too, will need to have their outputs scanned to produce
higher-order analog contents in order for those outputs to become
phenomenally conscious. In what follows, however, the term
&ldquo;inner sense&rdquo; will be used to mean, more strictly,
&ldquo;higher-order sense&rdquo;.</p>

<p>
We therefore have the following proposal to consider:</p>

<dl class="partag indent avoid-break">
<dt class="bold">Inner-Sense Theory:</dt>
<dd>A phenomenally conscious mental state is a state with
analog/non-conceptual intentional content, which is in turn the target
of a higher-order analog/non-conceptual intentional state, via the
operations of a faculty of &ldquo;inner sense&rdquo;.</dd>
</dl>

<p>
On this account, the difference between a phenomenally conscious
percept of red and the sort of unconscious percepts of red that guide
the guesses of a blindsighter and the activity of the sensorimotor
system, is as follows. The former is scanned by our inner senses to
produce a higher-order analog state with the content <em>experience of
red</em> or <em>seems red</em>, whereas the latter states
aren&rsquo;t&mdash;they remain <em>merely</em> first-order states with
the analog content <em>red</em>; and in so remaining, they lack any
dimension of <em>seeming</em> or <em>subjectivity</em>. According to
inner-sense theory, it is the higher-order perceptual contents
produced by the operations of our inner-senses that make some mental
states with analog contents, but not others, available to their
subjects.</p>

<p>
One advantage of inner-sense theory is that it can explain how it is
possible for us to acquire <em>purely recognitional concepts</em> of
experience. For if we possess higher-order perceptual contents, then
it should be possible for us to learn to recognize the occurrence of
our own perceptual states immediately grounded in those higher-order
analog contents. (Compare the way in which first-order perceptual
contents representing color and sound enable us the acquire
first-order recognitional concepts for colors and sounds.) And this
should be possible without those recognitional concepts thereby having
any conceptual connections with our beliefs about the content of the
states recognized, nor with any of our surrounding mental concepts.
This is then how inner-sense theory will claim to explain the familiar
philosophical thought-experiments concerning one&rsquo;s own
experiences, which are supposed to cause such problems for
physicalist/naturalistic accounts of the mind (Kripke 1972; Chalmers
1996). (For discussion of this &ldquo;phenomenal concept
strategy&rdquo; see Carruthers &amp; Veillet 2007.)</p>

<p>
For example, I can think,</p>

<blockquote>

<p>
<em>R</em> [an experience as of red] might have occurred in me, or
might normally occur in others, in the absence of any of its actual
causes and effects.</p>
</blockquote>

<p>
So on any view of intentional content that sees content as tied to
normal causes (i.e., to information carried) and/or to normal effects
(i.e., to teleological or inferential role), experience of type
<em>R</em> might occur without representing <em>red</em>. Likewise I
can think,</p>

<blockquote>

<p>
<em>R</em> might occur in someone without occupying the role of
<em>experience</em>, but rather (say) of belief.</p>
</blockquote>

<p>
In the same sort of way, I shall be able to think,</p>

<blockquote>

<p>
<em>P</em> [an experience of pain] might have occurred in me, or might
occur in others, in the absence of any of the usual causes and effects
of pain. There could be someone in whom <em>P</em> experiences occur
but who isn&rsquo;t bothered by them, and where those experiences are
never caused by tissue damage or other forms of bodily insult. And
conversely, there could be someone who behaves and acts just as I do
when in pain, and in response to the same physical causes, but who is
never subject to <em>P</em> types of experience.</p>
</blockquote>

<p>
If we possess purely recognitional concepts of experience such as
<em>R</em> and <em>P</em>, then the thinkability of such thoughts is
unthreatening to a naturalistic approach to the mind.</p>

<p>
Inner sense theorists thus seem well placed to respond to those who
claim that there is an unbridgeable explanatory gap between all
physical, functional, and intentional facts, on the one hand, and the
facts of phenomenal consciousness, on the other (Levine 1983; Chalmers
1996). And likewise they can explain the conceivability of zombies
without becoming committed to the existence of any non-physical
properties of experience (<em>contra</em> Chalmers 1996). It is the
conceptual isolation of our higher-order recognitional concepts of
experience that explains how there can be no <em>a priori</em>
entailment between physical, functional, and intentional facts and the
occurrence of states of type <em>R</em> or <em>P</em> (where
<em>R</em> and <em>P</em> express purely recognitional concepts).</p>

<p>
Inner-sense theory does face a number of difficulties, however. One
objection is as follows (see Dretske 1995; G&uuml;zeldere 1995). If
inner-sense theory were true, then how is it that there isn&rsquo;t
any phenomenology distinctive of inner sense, in the way that there is
a phenomenology associated with each outer sense? Since each of the
outer senses gives rise to a distinctive set of phenomenological
properties, one might expect that if there <em>were</em> such a thing
as inner sense, then there would also be a phenomenology distinctive
of its operation. But there doesn&rsquo;t appear to be any.</p>

<p>
This point turns on the so-called &ldquo;transparency&rdquo; of our
perceptual experience (Harman 1990). Concentrate as hard as you like
on your &ldquo;outer&rdquo; (first-order) experiences&mdash;you
won&rsquo;t find any <em>further</em> phenomenological properties
arising out of the attention you pay to them, beyond those already
belonging to the contents of the experiences themselves. Paying close
attention to your experience of the color of the red rose, for
example, just produces attention to the <em>redness</em>&mdash;a
property of the rose. Put like this, however, the objection just seems
to beg the question in favor of first-order theories of phenomenal
consciousness. It assumes that
first-order&mdash;&ldquo;outer&rdquo;&mdash;perceptions already have a
phenomenology independently of their targeting by inner sense. But
this is just what an inner-sense theorist will deny. And then in order
to explain the absence of any kind of higher-order phenomenology, an
inner-sense theorist only needs to maintain that our higher-order
perceptions are never themselves targeted by an inner-sense-organ
which might produce <em>third</em>-order analog representations of
them in turn.</p>

<p>
Another objection to inner-sense theory is as follows (see Sturgeon
2000). If there really were an organ of inner sense, then it ought to
be possible for it to malfunction, just as our first-order senses
sometimes do. And in that case, it ought to be possible for someone to
have a first-order percept with the analog content <em>red</em>
causing a higher-order percept with the analog content
<em>seems-orange</em>. Someone in this situation would be disposed to
judge, &ldquo;It&rsquo;s red&rdquo;, immediately and non-inferentially
(i.e., not influenced by beliefs about the object&rsquo;s normal color
or their own physical state). But at the same time they would be
disposed to judge, &ldquo;It <em>seems</em> orange&rdquo;. Not only
does this sort of thing never apparently occur, but the idea that it
might do so conflicts with a powerful intuition. This is that our
awareness of our own experiences is <em>immediate</em>, in such a way
that to <em>think</em> that you are undergoing an experience of a
certain sort <em>is</em> to be undergoing an experience of that sort.
But if inner-sense theory is correct, then it ought to be possible for
someone to believe that they are in a state of <em>seeming-orange</em>
when they are actually in a state of <em>seeming-red</em>. (The
problem of misrepresentation will addressed further below in
 <a href="#FurtObjeReplHighOrdeAppr">section 7</a>.)</p>
 
<p>
A different sort of objection to inner-sense theory is developed by
Carruthers (2000). It starts from the fact that the internal monitors
postulated by such theories would need to have considerable
computational complexity in order to generate the requisite
higher-order experiences. In order to perceive an experience, the
organism would need to have mechanisms to generate a set of internal
representations with an analog or non-conceptual content representing
the content of that experience, in all its richness and fine-grained
detail. And notice that any inner scanner would have to be a physical
device (just as the visual system itself is) which depends upon the
detection of those <em>physical</em> events in the brain that are the
outputs of the various sensory systems (just as the visual system is a
physical device that depends upon detection of physical properties of
surfaces via the reflection of light). For it is hard to see how any
inner scanner could detect the presence of an experience <em>qua</em>
experience. Rather, it would have to detect the physical
<em>realizations</em> of experiences in the brain and construct the
requisite higher-order representation of the experiences that those
physical events realize, on the basis of that physical-information
input. This makes it seem inevitable that the scanning device that
supposedly generates higher-order experiences of our first-order
visual experience would have to be almost as sophisticated and complex
as the visual system itself.</p>

<p>
Given this complexity in the operations of our organs of inner sense,
there should be some plausible story to tell about the evolutionary
pressures that led to their construction (Pinker 1994, 1997). But
there would seem to be no such stories on the market. The most
plausible suggestion is that inner-sense might have evolved to
subserve our capacity to think about the mental states of
conspecifics, thus enabling us to predict their actions and manipulate
their responses. (This is the so-called &ldquo;Machiavellian
hypothesis&rdquo; to explain the evolution of intelligence in the
great-ape lineage. See Byrne &amp; Whiten 1988; Whiten &amp; Byrne
1997; and see Goldman 2006, for a view of inner sense of this
sort.)</p>

<p>
Lycan no longer holds HOP theory (Sauret &amp; Lycan 2014) mainly
because he now thinks that some sort of <em>attention</em> to
first-order states is sufficient for an account of conscious states
and there is little reason to suppose that the attentional mechanism
in question is a higher-order representational state (see also Prinz
2012).</p>

<h2 id="HighOrdeThouTheo1Actu">4. Higher-Order Thought Theory (1): Actualist</h2>

<p>
A prominent version of higher-order theory is the (actualist)
higher-order thought (HOT) theory which is a proposal about the nature
of state-consciousness in general, of which phenomenal consciousness
is but one species. Its main proponent has been David Rosenthal (1986,
1993, 2005) and there are many variations to be found in the
literature. The basic proposal is this: a conscious mental state
<em>M</em>, of mine, is a state that is actually causing an activated
thought (generally a non-conscious one) that I have <em>M</em>, and
causing it non-inferentially. An account of phenomenal consciousness
can then be generated by stipulating that the mental state <em>M</em>
should have some causal role and/or content of a certain distinctive
sort in order to count as an experience (e.g., with an analog content,
perhaps), and that when <em>M</em> is an experience (or a mental
image, bodily sensation, or emotional feeling), it will be
phenomenally conscious when (and only when) suitably targeted. The HOT
is typically of the form: &ldquo;I am in mental state M&rdquo; (see
also Gennaro 1996, 2012).</p>

<p>
We therefore have the following proposal to consider:</p>

<dl class="partag indent">
<dt class="bold">Actualist Higher-Order Thought Theory:</dt>
<dd>A phenomenally conscious mental state is a state of a certain sort
(e.g., with analog/non-conceptual intentional content, perhaps) which
is the object of a higher-order thought, and which causes that thought
non-inferentially.</dd>
</dl>

<p>
As noted earlier, Rosenthal interprets the non-inferential requirement
as ruling out only <em>conscious</em> inferences in the generation of
a consciousness-making higher-order thought. This enables him to avoid
having to say that my unconscious motives become conscious when I
learn of them under psychoanalysis, or that my jealousy is conscious
when I learn of it by noticing and interpreting my own behavior. But
Rosenthal (2005) thinks that <em>unconscious</em> self-interpretation
is acceptable as a source of the conscious status of the states
thereby attributed. So if I arrive at the thought that I am feeling
cheerful by unconsciously noticing the spring in my own step and the
smile on my own face and drawing an unconscious inference, my
cheerfulness will thereby have been rendered conscious. This aspect of
Rosenthal&rsquo;s actualist form of HOT theory would appear to be
optional for a HOT theorist, however.</p>

<p>
In addition, and more controversially, Rosenthal (2005) seems to think
that the occurrence of a suitably caused HOT is <em>sufficient</em>
for consciousness, even in the absence of any targeted first-order
state (usually called &ldquo;targetless&rdquo; or &ldquo;empty&rdquo;
HOTs). So I am undergoing a conscious experience of red provided that
I <em>think</em> that I am undergoing an experience of red, even if I
am actually in no first-order perceptual state whatever. This aspect
of Rosenthal&rsquo;s view appears optional for an actualist HOT
theorist. Such a theorist can&mdash;and perhaps should&mdash;insist
that phenomenally conscious experience occurs when and only when a
first-order perceptual state causes a higher-order thought in the
existence of that state in a way that doesn&rsquo;t depend upon
self-interpretation. In recent years, the twin problems of
<em>misrepresentation</em> between HOTs and their first-order targets
as well as <em>targetless</em> HOT cases has led to significant
disagreement among HOT theorists (see also
 <a href="#FurtObjeReplHighOrdeAppr">section 7</a>
 below).</p>

<p>
The actualist HOT account avoids some of the difficulties inherent in
inner-sense theory, while retaining the latter&rsquo;s ability to
explain the distinction between conscious and unconscious perceptions.
(Conscious perceptions will be analog states that are targeted by a
HOT, whereas perceptions such as those involved in blindsight or
subliminal perceptions will be unconscious by virtue of <em>not</em>
being so targeted.) In particular, it is easy to see a function for
HOTs, in general, and to tell a story about their likely evolution. A
capacity to entertain HOTs about experiences would enable a creature
to negotiate the is/seems distinction, perhaps learning not to trust
its own experiences in certain circumstances, and also to induce
appearances in others, by deceit. And a capacity to entertain HOTs
about mental states (such as beliefs and desires) would enable a
creature to reflect on, and to alter, its own beliefs and patterns of
reasoning, as well as to predict and manipulate the thoughts and
behaviors of others. Indeed, it can plausibly be claimed that it is
our capacity to target higher-order thoughts on our own mental states
that underlies our status as rational agents (Burge 1996; Sperber
1996; Rolls 2004).</p>

<p>
A common initial objection to HOT theory is that it leads to an
<em>infinite regress</em>. It might seem that an infinite regress
results because a conscious mental state (M) must be accompanied by a
HOT, which, in turn must be accompanied by another HOT and so on.
However, the standard and widely accepted reply or explanation is that
when M is conscious, the HOT is <em>not</em> itself conscious
(Rosenthal 1986, 2005). When M is a first-order world-directed
conscious state, such as a desire or perception, it is accompanied by
an unconscious HOT. But when the HOT is itself conscious, there is a
yet another higher-order (or third-order) thought directed at the
conscious HOT. This would be a case of <em>introspection</em> such
that one&rsquo;s attention is directed inward at M (such as
introspecting my desire). (See
 <a href="#fig">Figure 1</a>.)</p>
 
<div class="figure" id="fig">
<img alt="a diagram: link to extended description below" src="HOT-figure.svg" />

<p id="fig1caption">
<span class="figlabel">Figure 1:</span> The Higher-Order Thought (HOT)
Theory of Consciousness
 <a href="figdesc.html#fig">extended description of figure 6</a>
 is in the supplement.]</p>
</div>

<p>
Another early objection to HOT theory is due to Dretske (1993). We are
asked to imagine a case in which we carefully examine two
line-drawings, say (or in Dretske&rsquo;s example, two patterns of
differently-sized spots). These drawings are similar in almost all
respects, but differ in just one aspect&mdash;in Dretske&rsquo;s
example, one of the pictures contains a black spot that the other
lacks. It is surely plausible that, in the course of examining these
two pictures, one will have enjoyed a conscious visual experience of
the respect in which they differ&mdash;e.g., of the offending spot.
But, as is familiar, one can be in this position while not knowing
<em>that</em> the two pictures are different, or in what <em>way</em>
they are different. In which case, since one can have a conscious
experience (e.g., of the spot) without being aware that one is having
it, consciousness cannot require higher-order awareness.</p>

<p>
Replies to this objection have been made by Seager (1994), Byrne
(1997), and Rosenthal (2005), among others. They point out that it is
one thing to have a conscious experience of the aspect that
differentiates the two pictures, and quite another to consciously
experience <em>that</em> the two pictures are <em>differentiated</em>
by that aspect. That is, consciously seeing the extra spot in one
picture needn&rsquo;t mean seeing that this is the difference between
the two pictures. So while scanning the two pictures one will enjoy
conscious experience of the extra spot. A HOT theorist will say that
this means undergoing a percept with the content <em>spot here</em>
that forms the target of a HOT that one is undergoing a perception
with that content. But this can perfectly well be true without one
undergoing a percept with the content <em>spot here in this picture
but absent here in that one</em>. And it can also be true without one
forming any HOT to the effect that one is undergoing a perception with
the content <em>spot here</em> when looking at a given picture but not
when looking at the other.</p>

<p>
A different sort of problem with the actualist version of higher-order
thought theory relates to the huge number of thoughts that would have
to be caused by any given phenomenally conscious experience. (This is
the analogue of the &ldquo;computational complexity&rdquo; objection
to inner-sense theory, sketched in
 <a href="#HighOrdePercTheo">section 3</a>
 above). Consider just how rich and detailed a conscious experience
can be. It would seem that there can be an immense amount of which we
can be consciously aware at any one time. Imagine looking down on a
city from a window high up in a tower-block, for example. In such a
case you can have phenomenally conscious percepts of a complex
distribution of trees, roads, and buildings; colors on the ground and
in the sky above; moving cars and pedestrians; and so on. And you
can&mdash;it seems&mdash;be conscious of all of this simultaneously.
According to actualist HOT theory, then, it seems you would need to
have a distinct activated HOT for each distinct aspect of your
experience&mdash;either that, or just a few such thoughts with
immensely complex contents. Either way, the objection is the same. For
it seems implausible that all of this higher-order activity should be
taking place (albeit non-consciously) every time someone is the
subject of a complex conscious experience. What would be the point?
And think of the amount of cognitive/neural space that these thoughts
would take up! (In contrast, we know that neural tissue and activity
are expensive; see Aiello &amp; Wheeler 1995; and we also know that as
a result of such constraints, the wiring diagram for the brain is
about as efficient as it is possible for it to be; see Cherniak <em>et
al.</em> 2004.)</p>

<p>
This objection to actualist forms of HOT theory is considered at some
length in Carruthers (2000), where a variety of possible replies are
discussed and evaluated. Perhaps the most plausible and challenging
such reply would be to deny the main premise lying behind the
objection, concerning the rich nature of phenomenally conscious
experience. The theory could align itself with Dennett&rsquo;s (1991)
conception of consciousness as highly fragmented, with multiple
streams of perceptual content being processed in parallel in different
regions of the brain, and with no stage at which all of these contents
are routinely integrated into a phenomenally conscious perceptual
manifold. Rather, contents become conscious on a piecemeal basis, as a
result of internal or external <em>probing</em> that gives rise to a
HOT about the content in question. This serves to convey to us the
mere <em>illusion</em> of riches, because wherever we direct our
attention, there we find a conscious perceptual content. (For a
related reply, see Gennaro 2012: ch. 6).</p>

<p>
It is difficult to know whether this sort of &ldquo;fragmentist&rdquo;
account can really explain the phenomenology of our experience,
however. For it still faces the objection that the objects of
attention can be immensely rich and varied at any given moment, hence
requiring there to be an equally rich and varied repertoire of HOTs
tokened at the same time. Think of immersing yourself in the colors
and textures of a Van Gogh painting, for example, or the scene as you
look out at your garden&mdash;it would seem that one can be
phenomenally conscious of a <em>highly</em> complex set of properties,
which one could not even begin to describe or conceptualize in any
detail. However, since the issues here are large and controversial, it
cannot yet be concluded that actualist forms of HOT theory have been
refuted. This is particularly the case when one considers such
phenomena as change and inattentional blindness where subjects often
do not even <em>notice</em> somewhat significant changes occurring in
an image or video even within one&rsquo;s focal visual field (Simons
2000; Simons &amp; Chabris 1999).</p>

<p>
Another difficulty for actualist forms of HOT theory takes the form of
a puzzle: how can the targeting of a perceptual state by HOT make the
former &ldquo;light up&rdquo;, and acquire the properties of
&ldquo;feel&rdquo; or <em>what it is like-ness</em>? Suppose, for
example, that I am undergoing an unconscious perception of red. How
could such a percept then acquire the properties distinctive of
phenomenal consciousness merely by virtue of me coming to think (in
non-inferential fashion) that I am undergoing an experience of
red?</p>

<p>
Rosenthal (2005) replies to this objection by pointing to cases in
which (he says) the acquisition and application of novel higher-order
concepts to our experience transforms the phenomenal properties of the
latter. Thus a course in wine-tasting can lead me to have experiences
of the wine that are phenomenally quite distinct from any that I
enjoyed previously (see also Siegel 2010; Gennaro 2012: ch. 6). And a
course in classical music appreciation might lead to changes in my
experience of the sound of the orchestra, perhaps distinguishing
between the sounds of the oboes and the clarinets for the first time.
Since changes in higher-order concepts can lead to changes in
phenomenal consciousness, Rosenthal thinks, it is plausible that it is
the presence of higher-order thoughts targeting our perceptual states
that is responsible for the latter&rsquo;s phenomenal properties
<em>tout court</em>.</p>

<p>
In response, an opponent of the theory might observe that some of the
concepts that one acquires in such cases do not appear to be
higher-order ones at all. Thus the concepts <em>oaky</em> and
<em>tanniny</em> that one acquires when wine-tasting pick out
secondary qualities <em>of the wine</em> (which are first-order), not
higher-order properties of our experience of the wine. And likewise
the concept <em>oboe</em> when applied in an experience is a
first-order concept of a sound type, not a higher-order concept of
one&rsquo;s experience of sound. The phenomenon here is quite general:
acquiring and applying new concepts in one&rsquo;s perception can
transform the similarity spaces and organization of one&rsquo;s
perceptual states. (Think here of the familiar duck/rabbit.) But it
appears to be a first-order phenomenon, not a higher-order one. At any
rate, there is considerable work for a HOT theorist to do here in
making out the case to the contrary.</p>

<h2 id="HighOrdeThouTheo2Disp">5. Higher-Order Thought Theory (2): Dispositionalist</h2>

<p>
One variant of HOT theory is the &ldquo;dispositionalist HOT
theory&rdquo;, which says that the conscious status of a perceptual
state consists in its <em>availability</em> to higher-order thought
(Dennett 1978a; Carruthers 1996, 2000, 2005). In its simplest form we
have here a quite general proposal concerning the conscious status of
any type of occurrent mental state, which becomes an account of
phenomenal consciousness when the states in question are experiences
(or images, emotions, etc.) with analog content. The proposal is this:
a conscious mental event <em>M</em>, of mine, is one that is disposed
to cause an activated thought (generally a non-conscious one) that I
have <em>M</em>, and to cause it non-inferentially.</p>

<p>
Carruthers&rsquo; more specific proposal is as follows:</p>

<dl class="partag indent">
<dt class="bold">Dispositionalist Higher-Order Thought Theory:</dt>
<dd>A phenomenally conscious mental state is a state of a certain sort
(perhaps with analog/non-conceptual intentional content, and perhaps
held in a special-purpose short-term memory store) which is available
to cause (non-inferentially) higher-order thoughts about itself (or
perhaps about any of the contents of the memory store).</dd>
</dl>

<p>
In contrast with the actualist form of theory, the higher-order
thoughts that render a percept conscious are not necessarily actual,
but potential. So the objection that a large amount of cognitive space
would have to be taken up with every conscious experience would now
seem to disappear. There need not <em>actually</em> be <em>any</em>
HOT occurring, in order for a given perceptual state to count as
phenomenally conscious. Perhaps we might even be able to retain our
belief in the rich and integrated nature of phenomenally conscious
experience&mdash;we just have to suppose that all of the contents in
question are simultaneously <em>available</em> to higher-order
thought. (Such availability might be realized by the &ldquo;global
broadcast&rdquo; of perceptual representations to a wide range of
conceptual systems in the brain, for drawing inferences, for forming
memories, and for planning, as well as for forming higher-order
beliefs. See Baars 1988, 1997, 2002.)</p>

<p>
But how would their mere <em>availability</em> to higher-order
thoughts could confer on our perceptual states the positive properties
distinctive of phenomenal consciousness&mdash;that is, of states
having a <em>subjective</em> dimension, or a distinctive subjective
<em>feel</em>? Carruthers argues that the answer lies in the theory of
content. Suppose that one agrees with Millikan (1984) that the
representational content of a state depends, in part, upon the powers
of the systems that <em>consume</em> that state. That is, suppose one
thinks that <em>what</em> a state represents will depend, in part, on
the kinds of inferences that the cognitive system is prepared to make
in the presence of that state, or on the kinds of behavioral control
that it can exert. In that case the presence of first-order perceptual
representations to a consumer-system that can deploy a &ldquo;theory
of mind&rdquo;, and that is capable of recognitional applications of
theoretically-embedded concepts of experience, may be sufficient to
render those representations <em>at the same time</em> as higher-order
ones. This would be what confers on our phenomenally conscious
experiences the dimension of subjectivity. Each experience would at
the same time (while also representing some state of the world, or of
our own bodies) be a representation that we are undergoing just such
an experience, by virtue of the powers of the &ldquo;theory of
mind&rdquo; system. Each percept of green, for example, would at one
and the same time be an analog representation of <em>green</em> and an
analog (non-conceptual) representation of <em>seems green</em> or
<em>experience of green</em>. (Consumer semantics embraces not only a
number of different varieties of <em>teleosemantics</em>, but also
various forms of <em>inferential role semantics</em>. For the former,
see Millikan 1984, 1986, 1989; Papineau 1987, 1993. For the latter,
see Loar 1990; McGinn 1982, 1989; Block 1986; Peacocke 1986,
1992).</p>

<p>
As an independent illustration of how consumer systems can transform
perceptual contents, consider prosthetic vision (Bach-y-Rita 1995;
Bach-y-Rita &amp; Kercel 2003). Blind subjects can be fitted with a
device that transduces the output from a hand-held or head-mounted
video-camera into patterns of electrically-induced tactile stimulation
across the subject&rsquo;s back or tongue. Initially, of course, the
subjects just feel patterns of gentle tickling sensations spreading
over the area in question, while the camera scans what is in front of
them. But provided that they are allowed to control the movements of
the camera themselves, their experiences after a time acquire
three-dimensional distal intentional contents, representing the
positions and movements of objects in space. (Note that the patterns
of tactile simulations themselves become imbued with spatial content.
The subjects in question say that it has come to <em>seem</em> to them
that there is a spherical object moving towards them, for example.)
Here everything on the input side remains the same as it was when
subjects first began to wear the device; but the planning and
action-controlling systems have learned to interpret those states
differently. And as a result, the subjects&rsquo; first-order
intentional perceptual contents have become quite different. Likewise,
according to dispositional HOT theory, when the &ldquo;theory of
mind&rdquo; system has learned to interpret the subject&rsquo;s
perceptual states <em>as</em> perceptual states: they all acquire a
dimension of <em>seeming</em> or subjectivity.</p>

<p>
So perhaps this account achieves the benefits of inner-sense theory,
but without the associated costs such as &ldquo;inner scanners&rdquo;
or organs of &ldquo;inner sense&rdquo;. (Some potential drawbacks will
be noted in a moment.) Moreover, there can arguably be no question of
our higher-order contents misrepresenting their first-order
counterparts, in such a way that one might be disposed to make
recognitional judgments of <em>red</em> and <em>seems orange</em> at
the same time. This is because the content of the higher-order
experience is parasitic on the content of the first-order one.
Carruthers, therefore, also refers to this view as <em>dual
content</em> theory.</p>

<p>
On the downside, the account isn&rsquo;t neutral on questions of
semantic theory. On the contrary, it requires us to reject any form of
pure input semantics, in favor of some sort of consumer semantics. We
cannot then accept that intentional content reduces to informational
content, nor that it can be explicated purely in terms of causal
co-variance relations to the environment. So anyone who finds such
views attractive will think that the account is a hard one to swallow.
(For discussion of various different versions of input semantics, see
Dretske 1981, 1986; Fodor 1987, 1990; and Loewer &amp; Rey 1991.)</p>

<p>
Moreover, Rosenthal (2005) has objected that dispositional HOT theory
can&rsquo;t account for our <em>actual <strong>awareness</strong></em>
of our conscious mental states, since mere dispositions to entertain
thoughts don&rsquo;t make us aware of anything. One might reply that
there does seem to be a perfectly good dispositional sense of
&ldquo;know&rdquo; and &ldquo;aware&rdquo;. As Dennett pointed out
long ago (1978b), I can be said to know, or to be aware, that zebras
in the wild don&rsquo;t wear overcoats, even though I have never
actually considered the matter, because I am <em>disposed</em> to
assent to that proposition in light of what I occurrently know.</p>

<p>
In addition, Rowlands (2001) and Jehle and Kriegel (2006) have
objected that dispositional HOT theory can&rsquo;t explain the sense
in which the phenomenal properties of experience are
<em>categorical</em>. For the higher-order analog intentional contents
that our conscious perceptual states possess&mdash;and that are
identified with the &ldquo;feel&rdquo; of experience&mdash;are said to
be constituted by the dispositional property that such states have, of
giving rise to HOTs about themselves. This objection, however, appears
to beg the question in favor of irreducible and intrinsic qualia as an
account of the distinctive properties of phenomenally conscious
states. In any case it doesn&rsquo;t seem to be an objection against
dispositional HOT theory as such, since it will count equally against
any representationalist theory of consciousness. (For example, Tye
1995, explains consciousness in terms of the <em>poisedness</em> of
perceptual states to have an impact on belief and reasoning, which is
a dispositional notion.) Any theory that proposes to reductively
explain phenomenal consciousness in terms of some combination of
intentional content and causal role will be explaining consciousness
in terms that are at least partly dispositional.</p>

<p>
Carruthers no longer holds dispositional HOT theory or, for that
matter, any form of higher-order theory and more recently defends a
version of first-order representationalism instead (Carruthers 2017).
He responds to his own previous main lines of argument against
first-order representationalism and finds it unnecessary to propose a
higher-order theory in order to explain the difference between
unconscious and conscious states. Still, Carruthers thinks that
dispositional HOT theory is preferable to actualist HOT theory.</p>

<p>
In the next section, we will first examine a somewhat related theory
which also supposes that a HOT accompanies each conscious state,
albeit one that is self-directed.</p>

<h2 id="SelfReprHybrHighOrdeTheo">6. Self-Representational and Hybrid Higher-Order Theories</h2>

<p>
The two most familiar forms of higher-order theory postulate the
existence of a pair of distinct mental states: a first-order
perceptual or quasi-perceptual state with a given content, and a HOT
or HOP representing the presence of that first-order state, thereby
rendering it conscious. Either one of these states can occur without
the other, although there may be a reliable causal relation between
them, such that certain types of first-order perception (e.g.,
attended outputs of the temporal-lobe visual system) regularly cause
higher-order representations of themselves to be formed. In recent
years, however, a cluster of different proposals have been made that
would reject this independent-state assumption. Rather, the
relationship between the conscious state in question and the
higher-order state is said to be <em>constitutive</em>, or
<em>internal</em>. To some extent, this view is inspired by Brentano
(1874 [1973]) and the phenomenological tradition, including Sartre
(1943 [1956]). (See Kriegel 2006, 2018; Gennaro 2002; Kriegel &amp;
Williford 2006; Zahavi 2004; Miguens et al. 2015.) We can refer to
these as &ldquo;self-representational&rdquo; higher-order theories.
(Kriegel initially coined the term &ldquo;same-order monitoring
theory&rdquo; but this was potentially misleading).</p>

<p>
We therefore have the following proposal to consider:</p>

<dl class="partag indent">
<dt class="bold">Self-Representational Theory:</dt>
<dd>A phenomenally conscious mental state is a state of a certain sort
(perhaps with analog/non-conceptual intentional content) which also,
at the same time, possesses an intentional content, thereby in some
sense representing <em>itself</em> to the person who is the subject of
that state.</dd>
</dl>

<p>
There are two basic types of self-representational theory, depending
on whether the constitutive relation between the conscious state and
the higher-order state is one of <em>identity</em>, on the one hand,
or <em>part-whole</em>, on the other. According to the former type of
account, it is one and the same perceptual state that is both
first-order (representing the world to us) and higher-order
(presenting itself to us). (Caston 2002, argues that Aristotle had a
theory of conscious perception of this sort.) Kriegel (2006) claims
that such accounts are rather mysterious from a naturalistic
perspective, but Carruthers (2000, 2005) and perhaps also Van Gulick
(2000, 2004) purport to provide naturalistic explanations of just this
sort of view. According to Carruthers, a first-order perceptual state
with analog content acquires, at the same time, a higher-order analog
content by virtue of its availability to a &ldquo;theory of
mind&rdquo; faculty, together with the truth of some suitable form of
consumer semantics (as explained in
 <a href="#HighOrdeThouTheo2Disp">section 5</a>
 above). Van Gulick can be interpreted as defending a similar view,
which likewise relies on a form of consumer semantics/functional role
semantics, which he labels a &ldquo;Higher-Order Global State (HOGS)
theory&rdquo;. On this account, globally broadcast first-order
perceptual states acquire at the same time a higher-order
<em>seeming</em> dimension though their availability to, and
incorporation into, higher-order models of the self and its relation
to the perceived environment. (What isn&rsquo;t entirely clear is
whether Van Gulick thinks that the resulting perceptual state
<em>is</em> the HOG state, or is rather a component <em>part</em> of
the HOG state&mdash;in which case he would be advocating a kind of
part-whole self-representational account.)</p>

<p>
Kriegel&rsquo;s (2009) eventual view emphasizes the claim that there
is a ubiquitous conscious (but inattentive or peripheral)
self-awareness which accompanies all first-order (attentive and
outer-directed) conscious states. Gennaro (2012: ch. 5) objects that,
among other things, it is difficult to make sense of such alleged
pervasive peripheral self-awareness especially when one is focused on
outer-directed tasks (such as putting together a bookcase or watching
a movie). It is at least not as clearly present as, say,
outer-directed peripheral vision in normal visual perception. Of
course, it is notoriously difficult to settle these sorts of
disagreements between competing phenomenological claims.</p>

<p>
Some varieties of part-whole self-representational theory take the
same general form as actualist kinds of HOT theory, in which a
first-order perceptual state with the content <em>analog-red</em> (as
it might be) gives rise to a higher-order thought that one is
experiencing red. But rather than claiming that it is the first-order
perception that becomes phenomenally conscious because of the presence
of the higher-order thought, what is said that the complex state made
up of <em>both</em> the first-order perception <em>and</em> the
higher-order thought becomes conscious. Gennaro (1996,&nbsp;2012)
defends such a hybrid view which he calls the <em>wide intrinsicality
view</em> (WIV) such that the HOT is better thought of as belonging to
the same overall complex state as its target. It is, however, not
always clear how this theory could offer any substantive benefits not
already obtainable from actualist HOT theory. Rather, the claim is
merely that a conscious state is one that contains two parts, one of
which is an awareness of the other. The idea is to treat the HOT as
intrinsic, as opposed to extrinsic, to the target state. Gennaro
(2012) argues that one benefit is to reflect the proper interplay
between sensory first-order states and conceptual HOTs. Kriegel
himself (2003, 2006, 2009) and (as Kriegel interprets him) Van Gulick
(2000, 2004) also emphasize that the first-order perception and the
higher-order judgment need to be <em>integrated</em> with one another
in order for the resulting complex state to be phenomenally conscious.
Kriegel argues that there needs to be a kind of integration resulting
from a psychologically real process (as opposed to a theorist&rsquo;s
definition) in order for the resulting state to have causal powers
that differ from those of the first-order state/higher-order state
pair.</p>

<p>
Kriegel and Van Gulick do not give fully developed accounts of just
<em>why</em> the integration of first-order perceptions with
higher-order judgments should give rise to the properties that are
distinctive of phenomenal consciousness. But one plausible
reconstruction is as follows, modeled on the way that the
conceptualization of analog (non-conceptual) first-order perceptual
content can transform the latter&rsquo;s properties. Consider, for
example, the familiar duck/rabbit. When someone sees this figure for
the first time she may just experience a complex of curved lines,
representing nothing. But when she comes to see it <em>as</em> a
rabbit, those lines take on a certain distinctive organization (the
figure now has both a front and a back, for example), thereby
transforming the represented properties of the figure. Arguably what
happens in such cases is that the conceptual systems succeed in
deploying a recognitional template for the concept <em>rabbit</em>,
finding a &ldquo;best match&rdquo; with the incoming non-conceptual
representations. Indeed, there is reason to think that just such a
process routinely takes place in perception, with conceptual systems
seeking matches against incoming data, and with the resulting states
possessing contents that integrate both conceptual and non-conceptual
(analog) representations (Kosslyn 1994; Carruthers 2000). The result
is a single perceptual state that represents <em>both</em> a
particular analog shape <em>and</em> a rabbit. Now suppose that when
such states are globally broadcast and are made available to the
systems responsible for higher-order thought, a similar process takes
place. Those systems bring to bear the concept <em>experience</em> or
the concept <em>seeing</em> to produce a further integrated perceptual
state. This single state will not only have first-order contents
representing the lines on the page, and representing a rabbit, they
will also have a higher-order content representing that one is
<em>experiencing</em> something rabbit-like. Hence the perceptual
state in question becomes &ldquo;self-presenting&rdquo;, and acquires,
as part of its content, a dimension of <em>seeming</em> or
<em>subjectivity</em>.</p>

<p>
Picciuto (2011) points out, however, that Kriegel&rsquo;s form of
self-representational theory still permits a mismatch between the
first-order and higher-order components of the integrated state. For
there seems to be nothing in the structure of the account to rule out
the possibility of a first-order analog content <em>green</em>
becoming integrated with the higher-order judgment <em>I am
experiencing yellow</em>, for example. In order to avoid this
difficulty, Picciuto (2011) proposes an alternative form of part-whole
self-representational theory. (See also Coleman 2015) He does so by
appropriating, and deploying for a novel purpose, the idea of a
<em>quotational</em> phenomenal concept, originally introduced by
Papineau (2002) and Balog (2009) as part of their defense of
physicalism against the arguments of Chalmers (1996) and others.
Picciuto&rsquo;s idea is that the relevant sort of complex
self-representational state will consist of a first-order perceptual
content combined with a higher-order concept like <em>experience</em>
that embeds, or &ldquo;quotes&rdquo; that very perceptual content. The
higher-order component of the complex state is not a <em>judgment
about</em> the experience component but rather a concept that
<em>quotes</em> the experience component. He calls his view
&ldquo;QHOT&rdquo; for &ldquo;quotational higher-order theory&rdquo;
whereby the HOT merely &ldquo;points to&rdquo; the target state (see
also Picciuto 2011).</p>

<p>
All part-whole self-representational accounts differ from the
dual-content theory of Carruthers (2000, 2005) in the following way,
however: on Carruthers&rsquo; account, the end-product can be entirely
non-conceptual. And in particular, the higher-order content possessed
by a conscious percept is a non-conceptual one, representing a
<em>seeming</em> of the first-order content of the state by virtue of
its availability to higher-order consumer systems. On all of the
part-whole accounts sketched above, in contrast, a conscious
perception is always partially conceptual, containing the higher-order
concept <em>experience</em> (or something similar) as part of its
content. There are probably multiple dimensions along which these two
sorts of theory could be compared, and each may have its own
advantages.</p>

<h2 id="FurtObjeReplHighOrdeAppr">7. Further Objections and Replies to the Higher-Order Approach</h2>

<p>
There have, of course, been a whole host of objections raised against
higher-order theories of phenomenal consciousness over the years.
(See, e.g., Aquila 1990; Jamieson &amp; Bekoff 1992; Dretske 1993,
1995; Goldman 1993, 2000; G&uuml;zeldere 1995; Tye 1995; Chalmers
1996; Byrne 1997; Siewert 1998; Levine 2001; Rowlands 2001; Seager
2004; Block, 2011.) Many of these objections, although perhaps
intended as objections to higher-order theories as such, are often
framed in terms of HOT theory (recall from
 <a href="#HighOrdePercTheo">section 3</a>
 above). A general moral to be taken away from the present discussion
is that different versions of a higher-order theory of phenomenal
consciousness need to be kept distinct from one another, and critics
should take care to state which version of the approach is under
attack.</p>

<p>
A few objections against specific versions of higher-order theory have
already been discussed above. Thus, in
 <a href="#HighOrdePercTheo">section 3</a>,
 we discussed Dretske&rsquo;s (1995) &ldquo;lack of any higher-order
phenomenology&rdquo; objection to inner sense (HOP) theory (which
<em>only</em> targets inner sense theory). And, in
 <a href="#HighOrdeThouTheo1Actu">section 4</a>,
 we discussed Dretske&rsquo;s (1993) &ldquo;spot&rdquo; objection to
actualist HOT theory. Of course, some of the objections discussed
above target more than one version of higher-order theory, while still
not being fully general in scope. Thus, the cognitive/computational
complexity objections discussed in sections 3 and 4 apply to inner
sense theories and to actualist HOT theories.</p>

<h3 id="TargHOTs">7.1 Targetless HOTs</h3>

<p>
Another objection (which is a variant of the misrepresentation problem
discussed in connection with inner sense theory in
 <a href="#HighOrdePercTheo">section 3</a>
 above) is the targetless higher-order representation problem (Byrne
1997; Neander 1998; Levine 2001). This is confronted by both inner
sense theory and actualist HOT theory (but not by either
dispositionalist HOT or self-representational theories, according to
which the relevant higher-order state can&rsquo;t exist in the absence
of the targeted state). In each case it seems that a higher-order
experience of a perception of red, say, or a HOT about a perception of
red, might exist in the absence of any such perception occurring. So
it would <em>seem</em> to the subject that she is experiencing red, or
she might <em>think</em> that she is experiencing red, in the absence
of any such experience. (Note that the point isn&rsquo;t just that she
might undergo such a seeming in the absence of anything really red.
Rather, the point is that she might not really be undergoing any sort
of visual experience <em>as of red</em> at all.) In which case, does
the subject have a phenomenally conscious experience <em>as of</em>
red, or not?</p>

<p>
Both Lycan (1996) and Rosenthal (2005) are sanguine in the face of
this objection. Each allows that targetless higher-order
representations are a possibility (albeit rare, perhaps), and each
opts to say that the subject in such a case is phenomenally conscious.
But each denies that this is a problem for their account. Lycan, for
example, insists that it is surely possible that it might
<em>seem</em> to someone that she is feeling pain when really no
relevant first-order representation of pain is present. (He suggests
that the effects of morphine, which leaves patients saying that their
pain feels just as it was, but that they no longer care, might
constitute such a case.) And surely such a person would have a
phenomenally conscious experience <em>as of</em> pain. Rosenthal,
likewise, uses pain as an example. He points to cases of dental
patients who initially experience pain in the dentist&rsquo;s chair
despite the fact that the relevant nerves are completely destroyed. It
seems that their fear, combined with the noise and vibration of the
drill, causes them to mistakenly think that they are feeling pain.
(When the drilling is stopped, and their dead nerves are explained to
them, they thereafter experience only the sound and the vibration.) So
this would be a case in which a HOT about experiencing pain is alone
sufficient to induce a phenomenally conscious experience <em>as
of</em> being in pain. A critic, however, might respond that the
illusion is caused, instead, by a vivid <em>imagining</em> of pain,
rather than by a HOT about feeling pain. Alternatively, if a HOT is
causally involved it might be that a top-down expectation of pain
<em>causes</em> a first-order experience of pain, as opposed to being
<em>constitutive</em> of the feeling of pain. It might be that
<em>introspective</em> anticipation of pain causes the pain in the
first place. (Note that this seems perfectly possible, since it is the
opposite of well-known placebo effects of expectation in reducing
pain.)</p>

<p>
The targetless and misrepresentation problems have become a very
significant topic of debate among HOT theorists as well as some
critics (Block 2011; Rosenthal 2005, 2011, 2022; Weisberg 2011;
Gennaro 2004a, 2012: ch. 4; Wilberg 2010; Berger 2014, 2017; Brown
2015; Lau &amp; Brown 2019; Prettyman 2020; Peebles 2022) which has
also led some to advocate for variants of HOT theory or to clarify
their own theories. Gennaro (2012) argues, for example, that since the
HOTs in question are typically themselves unconscious (except during
introspection), it makes little sense to suppose that they are
phenomenally conscious. Maintaining that an unconscious HOT would
yield the same subjective experience without any target state also
arguably runs counter to a central initial motivation of HOT theory,
namely, to explain what makes a <em>first-order</em> state
(intransitively) conscious. Thus, the first-order state must exist
first in order for a subject to become aware of it by an appropriate
and accompanying HOT with some sort of corresponding conceptual
content. If both aren&rsquo;t present, then no relevant conscious
state occurs (see also Wilberg 2010).</p>

<p>
Berger (2014), however, argues that consciousness is not really (or
primarily) a property of states at all; instead, it is a property of
individual <em>persons</em> (that is, how my mental states appear to
<em>me</em>). He relies to some extent on the Transitivity Principle
here but, in doing so, lessens the emphasis on
&ldquo;state-consciousness&rdquo;. Brown (2015) challenges the common
basic assumption that HOT theory is even a <em>relational</em> theory
at all in the way that many have interpreted it (i.e., as including
two distinct mental states related to each other). Instead, HOT theory
is better construed as a HOROR theory, that is, higher-order
representation of a representation, regardless of whether or not the
target mental state exists. In this sense, HOT theory is perhaps
better understood as a <em>non-relational</em> theory. Still, if the
qualitative conscious experience <em>always</em> goes with the HOT
(including in cases of misrepresentation noted in
 <a href="#HighOrdeThouTheo1Actu">section 4</a>),
 then it seems the first-order state plays no relevant role in the
theory. Nonetheless, Rosenthal (2022) points out that that
Brown&rsquo;s modified view conflates</p>

<blockquote>

<p>
a state&rsquo;s being qualitatively conscious with a necessary
condition for qualitative consciousness&hellip;there&rsquo;s rarely
anything it&rsquo;s like to be in a HO state, and HO states are almost
never conscious&hellip;.[i]t&rsquo;s the first-order state
that&rsquo;s qualitatively conscious. (Rosenthal 2022:
251&ndash;252)</p>
</blockquote>

<p>
The debate continues and various interesting related objections are
discussed in the literature (Kirkeby-Hinrup 2014; Brinck &amp;
Kirkeby-Hinrup 2017; Coleman 2018; Rosenthal 2018; Gottlieb 2020;
Berger &amp; Brown 2021; Weisberg 2022).</p>

<p>
In response specifically to Block (2011), Rosenthal (2011) and
Weisberg (2011) stress that the mere <em>seeming</em> of, say, being
in pain (provided by the HOT that one is in pain in the absence of
first-order pain) is sufficient for phenomenally conscious pain.
Consciousness is about mental <em>appearance</em>. It is not clear
that this fully addresses Block&rsquo;s point, which is that one would
not expect the mere thoughts that one feels pain to <em>matter</em> to
us in all the ways that pain matters. Block develops this point with
respect to the <em>awfulness</em> of pain. It would be remarkable
(indeed, mysterious) if a higher-order thought should have all of the
causal powers of the mental state that the thought is about. And, in
particular, there is no reason to expect that a HOT that one is in
pain should possess the negative valence and high-arousal properties
of pain itself. But the latter are surely crucial components of
phenomenally conscious pain. If so, then a HOT that one feels pain in
the absence of first-order pain will <em>not</em> be sufficient for
the conscious feeling of pain. It is also again important not to
conflate introspection (= conscious HOTs) with mere unconscious HOTs
accompanying a first-order mental state (see also Shepherd 2013).</p>

<h3 id="ProbRock">7.2 The Problem of the Rock</h3>

<p>
This objection, which can probably be recast to apply to any
higher-order theory, is the so-called &ldquo;rock&rdquo; objection
(Goldman 1993; Stubenberg 1998). We don&rsquo;t think that when we
become aware of a rock (either perceiving it, or entertaining a
thought about it) that the rock thereby becomes conscious. So why
should our higher-order awareness of a mental state render that mental
state conscious? Thinking about a rock doesn&rsquo;t make the rock
&ldquo;light up&rdquo; and become phenomenally conscious. So why
should thinking about my perception of the rock make the latter
phenomenally conscious, either?</p>

<p>
An initial reply to this objection involves pointing out that my
perception of the rock is a <em>mental</em> state, whereas the rock
itself isn&rsquo;t (Lycan 1996). Since phenomenal consciousness is a
property that (some) mental states possess, we can then say that the
reason why the rock isn&rsquo;t rendered phenomenally conscious by my
awareness of it is that it isn&rsquo;t the right <em>sort</em> of
thing to <em>be</em> phenomenally conscious, whereas my perception of
the rock is. This reply may be apt to strike the objector as trite.
But perhaps more can be said from the perspective of inner sense
theory, at least. Notice that my perception of the rock does, in one
sense, confer on the latter a subjective aspect. For example, the rock
is represented from one particular spatial perspective, and only some
of its properties (e.g., color) and not others (e.g., mass) are
represented. Likewise, then, with my perception of the rock.</p>

<p>
Similar replies to the rock objection are given by Van Gulick (2000)
and Gennaro (2005). Both point out that a rock isn&rsquo;t the kind of
thing that can be incorporated <em>into</em> a complex mental state
that involves higher-order representations in the way required by
self-representational or HOT theory. In contrast, whether actualist
HOT theory can reply adequately to the rock objection will depend on
whether or not there is an adequate reply to the problem considered in
 <a href="#HighOrdeThouTheo1Actu">section 4</a>,
 which challenges the actualist HOT theorist to say why targeting a
mental state with a HOT about that state should cause the latter to
&ldquo;light up&rdquo; and acquire a subjective dimension or
<em>feel</em>.</p>

<h3 id="AnimInfa">7.3 Animals and Infants</h3>

<p>
Another common objection is that higher-order theories, when combined
with plausible empirical claims about the mental abilities of
non-human animals, will conflict with our common-sense intuition that
such animals enjoy phenomenally conscious experience (Jamieson &amp;
Bekoff 1992; Dretske 1995; Tye 1995; Seager 2004). This objection can
be pressed most forcefully against higher-order <em>thought</em>
theories, of either variety, and against self-representational
theories; but it is also faced by inner-sense theory (depending on
what account can be offered of the evolutionary function of organs of
inner sense). Are cats and dogs really capable of having such
apparently complex HOTs which presumably contain mental state
concepts? Since there has been considerable dispute as to whether even
chimpanzees (and other primates) have the kind of sophisticated
&ldquo;theory of mind&rdquo; to enable them to entertain thoughts
about experiential states as such (Byrne &amp; Whiten 1988; Whiten
&amp; Byrne 1997; Povinelli 2000), it seems implausible that many
other species of mammal (let alone reptiles, birds, and fish) would
qualify as phenomenally conscious, on these accounts (Carruthers 2000,
2005). Yet the intuition that such creatures enjoy phenomenally
conscious experiences is a powerful one, for many people. (Witness
Nagel&rsquo;s classic 1974 paper, which argues that there must be
something that it is like to be a bat.)</p>

<p>
Many higher-order theorists have attempted to resist the claim that
their theory has any such entailment (e.g., Gennaro 1996, 2004a; Van
Gulick 2006). In each case, a common strategy is to claim that the
relevant higher-order representations are somehow <em>simpler</em>
than those tested for by those who do comparative &ldquo;theory of
mind&rdquo; research, hence leaving it open that these simpler
representations might be widespread in the animal kingdom. Gennaro
(1996), for example, suggests that although animals might lack the
concept <em>experience</em>, they can nevertheless be capable of
higher-order indexical thoughts of the form &ldquo;<em>this</em> is
different from <em>that</em>&rdquo; (where &ldquo;this&rdquo; and
&ldquo;that&rdquo; might refer to experiences of red and of green,
respectively). The trouble here, however, is to explain what makes
these indexicals higher-order in content without attributing concepts
like <em>experience of green</em> to the animal.</p>

<p>
Gennaro (2004a) takes a somewhat different tack. While allowing that
animals lack the concept <em>experience of green</em>, he thinks that
they might nevertheless possess the (simpler) concept <em>seeing
green</em>. But here he faces a dilemma. There is, indeed, a simpler
concept of seeing, grounded in the capacity to track eye-direction and
line of sight. But this isn&rsquo;t necessarily a higher-order
concept. To say, in this sense, that someone sees green in just to say
that there is some green in the line in which their eyes are
pointed&mdash;no mental state needs to be attributed. In contrast, it
appears that any concept of seeing that is genuinely higher-order will
be one that it would be less plausible to attribute to most species of
animal (given the comparative evidence). Perhaps a first-order
explanation of experimental observations of animals is virtually
always possible (see, for example, Carruthers 2008). But Gennaro
(2012: ch. 8) ultimately argues that there is plenty of evidence that
many animals are capable of metacognition (thinking about their own
mental states) as well as mindreading (thinking about other minds).
For example, in the case of mindreading, rhesus monkeys seem to
attribute visual and auditory perceptions to others in more
competitive paradigms (Flombaum &amp; Santos 2005) and crows and scrub
jays return alone to caches seen by other animals and recache them in
new places (Emery &amp; Clayton 2001). Any evidence of deception or
empathy in animals would also seem to indicate some kind of
mindreading ability. In addition, many animals seem capable of
metacognition (including possessing self-concepts) as evidenced by the
presence of episodic memory, for example (Dere et al. 2006; see also
the essays in Terrace &amp; Metcalf 2005; Hurley &amp; Nudds 2006). A
related debate takes place with respect to infant consciousness and
the capacity of infants to have metacognitive and mindreading
abilities (see Gennaro 2012; 2022; Berger &amp; Mylopoulos 2024).</p>

<p>
Van Gulick (2006), in contrast, suggests that all of the higher-order
representing sufficient to render an experience phenomenally conscious
can be left merely <em>implicit</em> in the way that the experience
enters into relationships with other mental states and the control of
behavior. So animals that lack the sorts of explicit higher-order
concepts tested for in comparative &ldquo;theory of mind&rdquo;
research can nevertheless be phenomenally conscious. The difficulty
here, however, is to flesh out the relevant notion of implicitness in
such a way that not every mental state, possessed by every creature
(no matter how simple), will count as phenomenally conscious. For
since mental states can&rsquo;t occur singly, but are always part of a
network of other related states, mental states will always carry
information about others, thus implicitly representing them. It is
implicit in the behavior of any creature that drinks, for example,
that it is thirsty; so the drinking behavior implicitly represents the
occurrence of the mental state of thirst.</p>

<p>
Of course, the basis for the common-sense intuition that animals
possess phenomenally conscious states can even be challenged. (How,
after all, are we supposed to <em>know</em> whether it is like
something to be a bat?) And that intuition can perhaps be explained
away as a mere by-product of imaginative identification with the
animal. (Since our <em>images</em> of their experiences are
phenomenally conscious, we may naturally assume that the experiences
<em>imaged</em> are similarly conscious (Carruthers 1999, 2000). But
there is no doubt that one major source of resistance to higher-order
theories will lie here, for many people, especially given various
moral considerations about animal pain and suffering. (For one set of
attempts to defuse this resistance, arguing that a higher-order
account need have few if any implications for our moral practices or
for comparative psychology, see Carruthers 2005: ch. 9; 2019: ch. 8.)
Of course, some will point out that there are also enough
<em>neurophysiological similarities</em> between (at least some parts
of) human and animal brains to justify attributions of, say, pains,
desires, emotions, and basic perceptual states. It is worth
emphasizing here that HOT theory does <em>not</em> say that having
conscious states requires having <em>introspective</em> states, that
is, having conscious HOTs. Conflating introspection with having mere
unconscious HOTs (and therefore simply having first-order conscious
states) may lead to a misguided straw man argument against HOT
theory.</p>

<h3 id="ExplPhenCons">7.4 Explaining Phenomenal Consciousness</h3>

<p>
Another objection is that higher-order approaches cannot really
<em>explain</em> the distinctive properties of phenomenal
consciousness (Chalmers 1996; Siewert 1998; Levine 2006). Whereas the
argument from animals is that higher-order representations
aren&rsquo;t <em>necessary</em> for phenomenal consciousness, the
argument here is that such representations aren&rsquo;t
<em>sufficient</em>. It is claimed, for example, that we can easily
conceive of creatures who enjoy the postulated kinds of higher-order
representation, related in the right sort of way to their first-order
perceptual states, but where those creatures are wholly
<em>lacking</em> in phenomenal consciousness.</p>

<p>
In response to this objection, higher-order theorists will join forces
with first-order theorists and others in claiming that these objectors
pitch the standards for explaining phenomenal consciousness too high
(Block &amp; Stalnaker 1999; Tye 1999; Carruthers 2000, 2005; Lycan
2001b). They will insist that a reductive explanation of
something&mdash;and of phenomenal consciousness in
particular&mdash;doesn&rsquo;t have to be such that we cannot conceive
of the <em>explanandum</em> (that which is being explained) in the
absence of the <em>explanans</em> (that which does the explaining).
(Indeed, we can also <em>explain why</em> no such explanation can be
forthcoming, in terms of our possession of purely recognitional
concepts of experience.) Rather, we just need to have good reason to
think that the explained properties are <em>constituted by</em> the
explaining ones, in such a way that nothing <em>else</em> needed to be
added to the world once the explaining properties were present, in
order for the world to contain the target phenomenon. But this is
hotly contested territory. And it is on this ground that the battle
for phenomenal consciousness may ultimately be won or lost.</p>

<p>
Objections have also been raised as to how (or if) HOT theory and
self-representational theories can account for various pathologies of
self-awareness or &ldquo;depersonalization disorders&rdquo;, such as
somatoparaphrenia or thought insertion in schizophrenia. These
psychopathologies imply that some of my body parts or thoughts are not
&ldquo;mine&rdquo;, which seems to run counter to the HOT theory claim
that the thought that &ldquo;I am in mental state M&rdquo; accompanies
each and every conscious state. (See Gennaro 2021, 2015 for some
discussion.)</p>

<h2 id="HOTTheoPrefCortPFC">8. HOT Theory and the Prefrontal Cortex (PFC)</h2>

<p>
As was noted in
 <a href="#MotiForHighOrdeAppr">section 2</a>,
 part of the motivation for HOT theory has been empirically oriented
and concerns how the theory can be extended into a theory about the
brain basis of consciousness based on various experimental results.
Thus, we might think of this work as further developing a
neurobiological theory of consciousness, albeit based on higher-order
theory.</p>

<p>
A &ldquo;prefrontal HOT theory&rdquo; says that the prefrontal cortex
(PFC), or at least sub-areas of the PFC, is the likely site of HOTs in
the brain and PFC activity is essential to having conscious mental
states. Some evidence comes from neuroimaging studies which have
systematically found increased activity in the prefrontal and parietal
cortex when comparing conscious versus unconscious conditions, often
even when performance capacity is controlled for (Lau &amp; Passingham
2006; Lau &amp; Rosenthal 2011; Odegaard et al. 2017; Boly et al.
2017). Rounis et al. (2010) find that transcranial magnetic
stimulation (TMS) directed at the PFC, which disrupts its activity,
has a significant impact on people&rsquo;s meta-visual awareness, but
without impairing first-order task performance.</p>

<p>
Higher-order theorists also, for example, sometimes point out more
generally how the structure of conscious states has some affinity to
the neural layering one finds in the brain (including comparisons to
other animal brains). There are lower-level sensory areas, such as
visual and auditory, and higher-level associative and more cognitive
brain areas. Consciousness would emerge from a hierarchical
architecture in which unconscious visual information processed in
early sensory areas is selected by higher-level mechanisms (such as in
the PFC). One of HOT&rsquo;s main predictions, then, seems to be that
disrupting the activity in the PFC should affect or eliminate sensory
experiences without affecting performance (since performance is
typically driven by unconscious first-order representations in early
sensory cortex).</p>

<p>
Disruptions to the PFC should affect the conscious experiences
themselves, not just the report or access to visual experiences. It is
crucial to keep in mind that there are many neural connections in the
&ldquo;top down&rdquo; direction via &ldquo;feedback loops&rdquo;. Lau
and Passingham (2006) demonstrate using carefully controlled stimuli
that there are circumstances in which people&rsquo;s subjective
reports of visual experience are impaired while their first-order
discrimination abilities remain fully intact. They also find that
visual consciousness in these conditions is specifically associated
with activity in a region of dorsolateral prefrontal cortex.</p>

<p>
On the other hand, the degree to which the PFC is <em>required</em>
for having various conscious states as well as the claim that the PFC
is the likely site of all or most higher-order thoughts is the subject
of vigorous continuing debate (Block 1995; Gennaro 2012; Kozuch 2014;
Odegaard, Knight, &amp; Lau 2017; Raccah, Block, &amp; Fox 2021;
Michel &amp; Malach 2022; Lau 2022).</p>

<p>
Part of the issue centers around the ongoing project of discovering
the much discussed &ldquo;neural correlates of consciousness&rdquo;
(NCCs). Block (2007: 489), for example, explains that a NCC is a
&ldquo;minimal neural basis is a necessary part of a neural sufficient
condition for conscious experience&rdquo;, and Koch (2004: 16)
similarly tells us that the NCC is &ldquo;the minimal set of neuronal
events and mechanisms jointly sufficient for a specific conscious
percept&rdquo; (see also Chalmers 2000). Various theories of
consciousness tend to differ with respect to predictions about where
and how conscious states are likely realized in the brain (Morales
&amp; Lau 2020; Michel 2022; Seth &amp; Bayne 2022). That is, do
conscious mental states <em>require</em> widespread
(&ldquo;global&rdquo;) brain activation or can at least some conscious
states occur in smaller (&ldquo;local&rdquo;) areas of the brain (such
as in basic sensory areas)? These two camps are often called the
&ldquo;globalists&rdquo; and &ldquo;localists&rdquo;. We can see this,
for example, via the contrast between &ldquo;Global Workspace
Theory&rdquo; (Baars 1997; Dehaene &amp; Changeux 2011) and so-called
&ldquo;Local Recurrence Theory&rdquo; (Lamme 2004) where the former
implies greater neural distribution for conscious states and the
latter urges that lower-level sensory brain areas can be sufficient
for producing sensory states. LeDoux and Brown (2017) further apply
HOROR theory to <em>emotional</em> states of consciousness, such as
fear, which are sometimes neglected in the literature. Unlike
perceptual states, emotions are typically construed as dependent upon
the amygdala which is part of the limbic system and deeper within the
brain.</p>

<p>
Perhaps most interesting for higher-order theories (and other
neurobiological theories for that matter) is whether or not the
prefrontal cortex (PFC) is <em>required</em> for having conscious
states. Some theorists, including HOT theorists, suppose that it is
(Kriegel 2007; Lau &amp; Rosenthal 2011; Brown, Lau, &amp; LeDoux
2019). Once again, there is some evidence of PFC activity involved in
having conscious states. Of course, there are many nonequivalent and
ambiguous claims being made along these lines, such as the PFC is
&ldquo;involved in&rdquo; having basic sensory states, the PFC is
&ldquo;constitutively necessary&rdquo; for conscious states, the PFC
&ldquo;causally affects&rdquo; the <em>content</em> of conscious
states, and so on. This makes it difficult at times to evaluate the
claims and experimental results.</p>

<p>
It is, however, uncontroversial that the more sophisticated
<em>introspective</em> states (and performing executive functions)
require PFC activity but, according to HOT theory, these are not
required for having first-order sensory states. In addition, some
&ldquo;localist&rdquo; foes of HOT theory point to counterevidence
which tends to show that the PFC is not implicated in having, say,
basic conscious visual perceptions <em>and thus</em> argue that HOT
theory is empirically flawed (Block 2007; Kozuch 2014, 2022; Raccah,
Block, &amp; Fox 2021; Malach 2022). In other words, tying HOTs to the
PFC is sometimes treated as leading to an <em>objection</em> to
prefrontal HOT theory. That is, if localism is correct, then
higher-order theories are probably wrong. Still, the premise that HOT
NCC&rsquo;s can only be found in the PFC is open to debate. It is
true, however, that stimulation to various areas of the PFC can result
in hallucinations and experiencing emotions.</p>

<p>
A HOT theorist might disagree with globalists (in the most broad
sense) and simply urge that <em>many</em> first-order conscious states
with their (unconscious) HOTs at most require the medial and inferior
parietal cortices, the temporoparietal cortex, the posterior cingulate
cortex, and the anterior cingulate cortex (Gennaro 2012). Lau (2022)
treats this as a sort of &ldquo;middle&rdquo; position since the
localist would likely not typically extend these NCC&rsquo;s out to
all these areas (Lau 2022: ch. 6, calls it a &ldquo;centrist
manifesto&rdquo;). Gennaro cites data from the &ldquo;theory of
mind&rdquo; and &ldquo;mindreading&rdquo; literature (Newen &amp;
Vogeley 2003; Saxe unpublished&nbsp;[see Other Internet Resources])
indicating that meta-representation is best located in the anterior
cingulate cortex as well as in the right temporo-parietal junction and
the superior parietal lobe.</p>

<p>
In addition, there is some evidence that conscious states occur
without much PFC activity and that conscious states are minimally
affected by damage to the PFC. When subjects are engaged in a
perceptual task or absorbed in watching a movie, there is widespread
neural activation but little PFC activity (Goldberg, Harel, &amp;
Malach 2006). Furthermore, conscious experience is not eliminated
entirely when there is extensive PFC damage, even in lobotomies
(Pollen 2008), whereas damage to very specific areas in the visual
cortex causes outright blindness and the inability to detect motion.
Damage to the fusiform gyrus in the posterior cortex is widely known
to result in prosopagnosia, i.e., the inability of recognizing faces.
Thus, lesion evidence is often used to bolster the localist position
(Malach 2022) and to oppose the globalist view (Kozuch 2014, 2022).
Lesions to the PFC do not affect, for example, visual experience of
color and shape. Some of the detected PFC activity in various
experiments are also perhaps the products of the demand for subsequent
verbal responses to questions. However, there has also been major
pushback to this line of argument (Michel &amp; Morales 2020; Tsuchiya
et al. 2015). For a localist, it is crucial to show that the activity
detected in the PFC during imaging studies pertains primarily to
attention and report, not consciousness per se. Some of the recent
discussion comes in the context of &ldquo;adversarial
collaborations&rdquo; where numerous co-authors, who hold different
theories of consciousness, agree on a set of experiments (and their
predicted results) designed to show which theory has an advantage
(Ball 2019). Thus, the debate continues with increased attention to
experimental designs and the best interpretation of results (Lau 2022;
Michel &amp; Malach 2022; Block 2023).</p>

<p>
This issue also has important implications for whether or not animals
and infants have conscious states (see also above in
 <a href="#FurtObjeReplHighOrdeAppr">section 7</a>).
 After all, <em>if</em> PFC activity is <em>required</em> even for
basic conscious states, such as sensory perceptions and pains, then it
is unlikely that most animals or even infants are conscious since many
animals and infants do not have the requisite PFC activity or do not
have a PFC at all.&nbsp;</p>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Aiello, Leslie C. and Peter Wheeler, 1995, &ldquo;The
Expensive-Tissue Hypothesis: The Brain and the Digestive System in
Human and Primate Evolution&rdquo;, <em>Current Anthropology</em>,
36(2): 199&ndash;221. doi:10.1086/204350</li>

<li>Aquila, Richard E., 1990, &ldquo;Consciousness as Higher-Order
Thought: Two Objections&rdquo;, <em>American Philosophical
Quarterly</em>, 27(1): 81&ndash;87.</li>

<li>Armstrong, D. M., 1968, <em>A Materialist Theory of the Mind</em>
(International Library of Philosophy and Scientific Method),
London/New York: Routledge &amp; K. Paul, Humanities Press.</li>

<li>&ndash;&ndash;&ndash;, 1984, &ldquo;Consciousness and
Causality&rdquo; (Great Debates in Philosophy), in <em>Consciousness
and Causality: A Debate on the Nature of Mind</em>, by D. M. Armstrong
and Norman Malcolm, Oxford/New York: B. Blackwell, 103&ndash;192.</li>

<li>Baars, Bernard J., 1988, <em>A Cognitive Theory of
Consciousness</em>, Cambridge/New York: Cambridge University
Press.</li>

<li>&ndash;&ndash;&ndash;, 1997, <em>In the Theater of
Consciousness</em>, New York: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;The Conscious Access
Hypothesis: Origins and Recent Evidence&rdquo;, <em>Trends in
Cognitive Sciences</em>, 6(1): 47&ndash;52.
doi:10.1016/S1364-6613(00)01819-2</li>

<li>Bach-y-Rita, P., 1995. <em>Non-Synaptic Diffusion
Neurotransmission and Late Brain Reorganization</em>, New York: Demos
Press.</li>

<li>Bach-y-Rita, Paul and Stephen W. Kercel, 2003, &ldquo;Sensory
Substitution and the Human&ndash;Machine Interface&rdquo;, <em>Trends
in Cognitive Sciences</em>, 7(12): 541&ndash;546.
doi:10.1016/j.tics.2003.10.013</li>

<li>Balog, Katalin, 2009, &ldquo;Phenomenal Concepts&rdquo;, in
<em>The Oxford Handbook of Philosophy of Mind</em>, Brian P.
McLaughlin with Ansgar Beckermann and Sven Walter (eds), Oxford/New
York: Clarendon Press, 292&ndash;312 (ch. 17).
doi:10.1093/oxfordhb/9780199262618.003.0018</li>

<li>Ball, Philip, 2019, &ldquo;Neuroscience Readies for a Showdown
Over Consciousness Ideas&rdquo;, <em>Quanta Magazine</em>, 6 March
2019.
 [<a href="https://www.quantamagazine.org/neuroscience-readies-for-a-showdown-over-consciousness-ideas-20190306/" target="other">Ball 2019 available online</a>]</li>
 
<li>Berger, Jacob, 2014, &ldquo;Consciousness Is Not a Property of
States: A Reply to Wilberg&rdquo;, <em>Philosophical Psychology</em>,
27(6): 829&ndash;842. doi:10.1080/09515089.2013.771241</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;How Things Seem to
Higher-Order Thought Theorists&rdquo;, <em>Dialogue</em>, 56(3):
503&ndash;526. doi:10.1017/S0012217317000440</li>

<li>Berger, Jacob and Richard Brown, 2021, &ldquo;Conceptualizing
Consciousness&rdquo;, <em>Philosophical Psychology</em>, 34(5):
637&ndash;659. doi:10.1080/09515089.2021.1914326</li>

<li>Berger, Jacob and Myrto Mylopoulos, 2024, &ldquo;HOTT and Heavy:
Higher-Order Thought Theory and the Theory-Heavy Approach to Animal
Consciousness&rdquo;, <em>Synthese</em>, 203(3): article 98.
doi:10.1007/s11229-024-04529-8</li>

<li>Block, Ned, 1986, &ldquo;Advertisement for a Semantics for
Psychology&rdquo;, <em>Midwest Studies in Philosophy</em>, 10:
615&ndash;678. doi:10.1111/j.1475-4975.1987.tb00558.x</li>

<li>&ndash;&ndash;&ndash;, 1995, &ldquo;On a Confusion about a
Function of Consciousness&rdquo;, <em>Behavioral and Brain
Sciences</em>, 18(2): 227&ndash;247.
doi:10.1017/S0140525X00038188</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Consciousness, Accessibility,
and the Mesh between Psychology and Neuroscience&rdquo;,
<em>Behavioral and Brain Sciences</em>, 30(5&ndash;6): 481&ndash;499.
doi:10.1017/S0140525X07002786</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;The Higher Order Approach to
Consciousness Is Defunct&rdquo;, <em>Analysis</em>, 71(3):
419&ndash;431. doi:10.1093/analys/anr037</li>

<li>&ndash;&ndash;&ndash;, 2023, <em>The Border between Seeing and
Thinking</em> (Philosophy of Mind Series), New York: Oxford University
Press. doi:10.1093/oso/9780197622223.001.0001</li>

<li>Block, Ned and Robert Stalnaker, 1999, &ldquo;Conceptual Analysis,
Dualism, and the Explanatory Gap&rdquo;, <em>The Philosophical
Review</em>, 108(1): 1&ndash;40. doi:10.2307/2998259</li>

<li>Boly, Melanie, Marcello Massimini, Naotsugu Tsuchiya, Bradley R.
Postle, Christof Koch, and Giulio Tononi, 2017, &ldquo;Are the Neural
Correlates of Consciousness in the Front or in the Back of the
Cerebral Cortex? Clinical and Neuroimaging Evidence&rdquo;, <em>The
Journal of Neuroscience</em>, 37(40): 9603&ndash;9613.
doi:10.1523/JNEUROSCI.3218-16.2017</li>

<li>Brentano, Franz, 1874 [1973], <em>Psychologie vom empirischen
Standpunkt</em>, Leipzig: Duncker &amp; Humblot. Translated as
<em>Psychology from an Empirical Standpoint</em> (International
Library of Philosophy and Scientific Method), Oskar Kraus and Linda L.
McAlister (eds) Antos C. Rancurello, D. B. Terrell, and Linda L.
McAlister (trans), London/New York: Routledge and Kegan
Paul/Humanities Press, 1973.</li>

<li>Brinck, Ingar and Asger Kirkeby-Hinrup, 2017, &ldquo;Change
Blindness in Higher-Order Thought: Misrepresentation or Good
Enough?&rdquo;, <em>Journal of Consciousness Studies</em>,
24(5&ndash;6): 50&ndash;73.</li>

<li>Brown, Richard, 2015, &ldquo;The HOROR Theory of Phenomenal
Consciousness&rdquo;, <em>Philosophical Studies</em>, 172(7):
1783&ndash;1794. doi:10.1007/s11098-014-0388-7</li>

<li>Brown, Richard, Hakwan Lau, and Joseph E. LeDoux, 2019,
&ldquo;Understanding the Higher-Order Approach to
Consciousness&rdquo;, <em>Trends in Cognitive Sciences</em>, 23(9):
754&ndash;768. doi:10.1016/j.tics.2019.06.009</li>

<li>Burge, Tyler, 1996, &ldquo;Our Entitlement to Self-Knowledge, Part
I&rdquo;, <em>Proceedings of the Aristotelian Society</em>, 96:
91&ndash;116. doi:10.1093/aristotelian/96.1.91</li>

<li>Byrne, Alex, 1997, &ldquo;Some like It HOT: Consciousness and
Higher-Order Thoughts&rdquo;, <em>Philosophical Studies</em>, 86(2):
103&ndash;129. doi:10.1023/A:1017959107565</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;What Phenomenal Consciousness
Is Like&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 203&ndash;226 (ch. 9).</li>

<li>Byrne, Richard W. and Andrew Whiten (eds), 1988, <em>Machiavellian
Intelligence: Social Expertise and the Evolution of Intellect in
Monkeys, Apes, and Humans</em>, Oxford/New York: Clarendon Press.</li>

<li>Carruthers, Peter, 1989, &ldquo;Brute Experience&rdquo;, <em>The
Journal of Philosophy</em>, 86(5): 258&ndash;269.
doi:10.2307/2027110</li>

<li>&ndash;&ndash;&ndash;, 1996, <em>Language, Thought and
Consciousness: An Essay in Philosophical Psychology</em>,
Cambridge/New York: Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 1999, &ldquo;Sympathy and
Subjectivity&rdquo;, <em>Australasian Journal of Philosophy</em>,
77(4): 465&ndash;482. doi:10.1080/00048409912349231</li>

<li>&ndash;&ndash;&ndash;, 2000, <em>Phenomenal Consciousness: A
Naturalistic Theory</em>, Cambridge/New York: Cambridge University
Press. doi:10.1017/CBO9780511487491</li>

<li>&ndash;&ndash;&ndash;, 2005, <em>Consciousness: Essays from a
Higher-Order Perspective</em>, Oxford/New York: Clarendon Press.
doi:10.1093/0199277362.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Meta&#8208;cognition in
Animals: A Skeptical Look&rdquo;, <em>Mind &amp; Language</em>, 23(1):
58&ndash;89. doi:10.1111/j.1468-0017.2007.00329.x</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;In Defence of First-Order
Representationalism&rdquo;, <em>Journal of Consciousness Studies</em>,
24(5&ndash;6): 74&ndash;87.</li>

<li>&ndash;&ndash;&ndash;, 2019, <em>Human and Animal Minds: The
Consciousness Questions Laid to Rest</em>, Oxford/New York: Oxford
University Press. doi:10.1093/oso/9780198843702.001.0001</li>

<li>Carruthers, Peter and B&eacute;n&eacute;dicte Veillet, 2007,
&ldquo;The Phenomenal Concept Strategy&rdquo;, <em>Journal of
Consciousness Studies</em>, 14(9&ndash;10): 212&ndash;236.</li>

<li>Caston, Victor, 2002, &ldquo;Aristotle on Consciousness&rdquo;,
<em>Mind</em>, 111(444): 751&ndash;815.
doi:10.1093/mind/111.444.751</li>

<li>Chalmers, David John, 1996, <em>The Conscious Mind: In Search of a
Fundamental Theory</em> (Philosophy of Mind Series), New York: Oxford
University Press.</li>

<li>&ndash;&ndash;&ndash;, 2000, &ldquo;What Is a Neural Correlate of
Consciousness?&rdquo;, in <em>Neural Correlates of Consciousness</em>,
Thomas Metzinger (ed.), Cambridge, MA: The MIT Press, 17&ndash;40.
doi:10.7551/mitpress/4928.003.0004</li>

<li>Cherniak, Christopher, Zekeria Mokhtarzada, Raul
Rodriguez-Esteban, and Kelly Changizi, 2004, &ldquo;Global
Optimization of Cerebral Cortex Layout&rdquo;, <em>Proceedings of the
National Academy of Sciences</em>, 101(4): 1081&ndash;1086.
doi:10.1073/pnas.0305212101</li>

<li>Coleman, Sam, 2015, &ldquo;Quotational Higher-Order Thought
Theory&rdquo;, <em>Philosophical Studies</em>, 172(10):
2705&ndash;2733. doi:10.1007/s11098-015-0441-1</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;The Merits of Higher-Order
Thought Theories&rdquo;, <em>Trans/Form/A&ccedil;&atilde;o</em>,
41(spe): 31&ndash;48. doi:10.1590/0101-3173.2018.v41esp.04.p31</li>

<li>Dehaene, Stanislas, and Jean-Piere Changeaux, 2011, "Experimental
and Theoretical Approaches to Conscious Processing", <em>Neuron</em>
70: 200-227.</li>

<li>Dennett, Daniel C., 1978a, &ldquo;Toward a Cognitive Theory of
Consciousness&rdquo;, in <em>Perception and Cognition: Issues in the
Foundations of Psychology</em> (Minnesota Studies in the Philosophy of
Science 9), C. Wade Savage (ed.), Minneapolis, MN: University of
Minnesota Press, 201&ndash;228.
 [<a href="https://hdl.handle.net/11299/185300" target="other">Dennett 1978a available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 1978b, <em>Brainstorms: Philosophic Essays
on Mind and Psychology</em>, Montgomery, VT: Bradford Books.</li>

<li>&ndash;&ndash;&ndash;, 1991, <em>Consciousness Explained</em>,
Boston: Little, Brown and Co.</li>

<li>Dere, E., E. Kartteke, J. Huston, and M. Desouzasilva, 2006,
&ldquo;The Case for Episodic Memory in Animals&rdquo;,
<em>Neuroscience &amp; Biobehavioral Reviews</em>, 30(8):
1206&ndash;1224. doi:10.1016/j.neubiorev.2006.09.005</li>

<li>Dretske, Fred I., 1981, <em>Knowledge &amp; the Flow of
Information</em>, Cambridge, MA: MIT Press.</li>

<li>&ndash;&ndash;&ndash;, 1986, &ldquo;Misrepresentation&rdquo;, in
<em>Belief: Form, Content, and Function</em>, Radu J. Bogdan (ed.),
Oxford/New York: Clarendon Press, 17&ndash;36.</li>

<li>&ndash;&ndash;&ndash;, 1988, <em>Explaining Behavior: Reasons in a
World of Causes</em>, Cambridge, MA: MIT Press.
doi:10.7551/mitpress/2927.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1993, &ldquo;Conscious Experience&rdquo;,
<em>Mind</em>, 102(406): 263&ndash;283.
doi:10.1093/mind/102.406.263</li>

<li>&ndash;&ndash;&ndash;, 1995, <em>Naturalizing the Mind</em> (Jean
Nicod Lectures 1994), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/4872.001.0001</li>

<li>Emery, N. J. and N. S. Clayton, 2001, &ldquo;Effects of Experience
and Social Context on Prospective Caching Strategies by Scrub
Jays&rdquo;, <em>Nature</em>, 414(6862): 443&ndash;446.
doi:10.1038/35106560</li>

<li>Flombaum, Jonathan I. and Laurie R. Santos, 2005, &ldquo;Rhesus
Monkeys Attribute Perceptions to Others&rdquo;, <em>Current
Biology</em>, 15(5): 447&ndash;452. doi:10.1016/j.cub.2004.12.076</li>

<li>Fodor, Jerry A., 1987, <em>Psychosemantics: The Problem of Meaning
in the Philosophy of Mind</em> (Explorations in Cognitive Science 2),
Cambridge, MA: MIT Press. doi:10.7551/mitpress/5684.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1990, <em>A Theory of Content and Other
Essays</em>, Cambridge, MA: MIT Press.
doi:10.7551/mitpress/6765.001.0001</li>

<li>Gennaro, Rocco J., 1996, <em>Consciousness and Self-Consciousness:
A Defense of the Higher-Order Thought Theory of Consciousness</em>
(Advances in Consciousness Research, 6), Amsterdam/Philadelphia: John
Benjamins Pub.</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Jean-Paul Sartre and the HOT
Theory of Consciousness&rdquo;, <em>Canadian Journal of
Philosophy</em>, 32(3): 293&ndash;330.</li>

<li>&ndash;&ndash;&ndash;, 2004a, &ldquo;Higher-Order Thoughts, Animal
Consciousness, and Misrepresentation: A Reply to Carruthers and
Levine&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 45&ndash;66 (ch. 3).</li>

<li id="Gennaro2004">&ndash;&ndash;&ndash; (ed.), 2004b,
<em>Higher-Order Theories of Consciousness: An Anthology</em>
(Advances in Consciousness Research 56), Amsterdam/Philadelphia, PA:
John Benjamins.</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;The HOT Theory of
Consciousness: Between a Rock and a Hard Place?&rdquo;, <em>Journal of
Consciousness Studies</em>, 12(2): 3&ndash;21.</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Between Pure
Self-Referentialism and the Extrinsic HOT Theory of
Consciousness&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 221&ndash;248 (ch. 10). doi:10.7551/mitpress/6155.003.0012</li>

<li>&ndash;&ndash;&ndash;, 2012, <em>The Consciousness Paradox:
Consciousness, Concepts, and Higher-Order Thoughts</em>
(Representation and Mind), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/9780262016605.001.0001</li>

<li>&ndash;&ndash;&ndash; (ed.), 2015, <em>Disturbed Consciousness:
New Essays on Psychopathology and Theories of Consciousness</em>
(Philosophical Psychopathology), Cambridge, MA/London, UK: The MIT
Press. doi:10.7551/mitpress/9780262029346.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Inserted Thoughts and the
Higher-Order Thought Theory of Consciousness&rdquo;, in <em>Psychiatry
and Neuroscience Update</em>, Pascual &Aacute;ngel Gargiulo and
Humberto Luis Mesones Arroyo (eds), Cham: Springer International
Publishing, 61&ndash;71. doi:10.1007/978-3-030-61721-9_7</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;Consciousness&rdquo;, in
<em>Encyclopedia of Animal Cognition and Behavior</em>, Jennifer Vonk
and Todd K. Shackelford (eds), Cham: Springer International
Publishing, 1625&ndash;1638. doi:10.1007/978-3-319-55065-7_1611</li>

<li>Glover, Scott, 2004, &ldquo;Separate Visual Representations in the
Planning and Control of Action&rdquo;, <em>Behavioral and Brain
Sciences</em>, 27(1): 3&ndash;24. doi:10.1017/S0140525X04000020</li>

<li>Goldberg, Ilan I., Michal Harel, and Rafael Malach, 2006,
&ldquo;When the Brain Loses Its Self: Prefrontal Inactivation during
Sensorimotor Processing&rdquo;, <em>Neuron</em>, 50(2): 329&ndash;339.
doi:10.1016/j.neuron.2006.03.015</li>

<li>Goldman, Alvin I., 1993, &ldquo;Consciousness, Folk Psychology,
and Cognitive Science&rdquo;, <em>Consciousness and Cognition</em>,
2(4): 364&ndash;382. doi:10.1006/ccog.1993.1030</li>

<li>&ndash;&ndash;&ndash;, 2000, &ldquo;Can Science Know When
You&rsquo;re Conscious? Epistemological Foundations of Consciousness
Research&rdquo;, <em>Journal of Consciousness Studies</em>, 7(5):
3&ndash;22.</li>

<li>&ndash;&ndash;&ndash;, 2006, <em>Simulating Minds: The Philosophy,
Psychology, and Neuroscience of Mindreading</em> (Philosophy of Mind),
Oxford/New York: Oxford University Press.
doi:10.1093/0195138929.001.0001</li>

<li>Gottlieb, Joseph, 2020, &ldquo;On Ambitious Higher-Order Theories
of Consciousness&rdquo;, <em>Philosophical Psychology</em>, 33(3):
421&ndash;441. doi:10.1080/09515089.2020.1731445</li>

<li>Graziano, Michael S. A., 2013, <em>Consciousness and the Social
Brain</em>, Oxford/New York: Oxford University Press.</li>

<li>G&uuml;zeldere, G&uuml;ven, 1995 [1997], &ldquo;Is Consciousness
the Perception of What Passes in One&rsquo;s Own Mind?&rdquo;, in
<em>Conscious Experience</em>, Thomas Metzinger (ed.), Paderborn:
Sch&ouml;ningh, 335&ndash;358 (ch. 49). Reprinted in <em>The Nature of
Consciousness: Philosophical Debates</em>, Ned Joel Block, Owen
Flanagan, and G&uuml;ven G&uuml;zeldere (eds.), Cambridge, MA: MIT
Press, 1997, 789&ndash;806.</li>

<li>Harman, Gilbert, 1990, &ldquo;The Intrinsic Quality of
Experience&rdquo;, <em>Philosophical Perspectives</em>, 4:
31&ndash;52. doi:10.2307/2214186</li>

<li>Hellie, Benj, 2007, &ldquo;Higher-Order Intentionality and
Higher-Order Acquaintance&rdquo;, <em>Philosophical Studies</em>,
134(3): 289&ndash;324. doi:10.1007/s11098-005-0241-0</li>

<li>Hill, Christopher S., 2004, &ldquo;Ouch! An Essay on Pain&rdquo;,
in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 339&ndash;362 (ch. 15).</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Perceptual Consciousness: How
It Opens Directly onto the World, Preferring the World to the
Mind&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 249&ndash;272 (ch. 11). doi:10.7551/mitpress/6155.003.0013</li>

<li>Hurley, Susan L. and Matthew Nudds (eds), 2006, <em>Rational
Animals?</em>, Oxford/New York: Oxford University Press.
doi:10.1093/acprof:oso/9780198528272.001.0001</li>

<li>Jacob, Pierre and Marc Jeannerod, 2003, <em>Ways of Seeing: The
Scope and Limits of Visual Cognition</em>, Oxford: Oxford University
Press. doi:10.1093/acprof:oso/9780198509219.001.0001</li>

<li>Jackson, Frank, 1982, &ldquo;Epiphenomenal Qualia&rdquo;, <em>The
Philosophical Quarterly</em>, 32(127): 127&ndash;136.
doi:10.2307/2960077</li>

<li>&ndash;&ndash;&ndash;, 1986, &ldquo;What Mary Didn&rsquo;t
Know&rdquo;, <em>The Journal of Philosophy</em>, 83(5): 291&ndash;295.
doi:10.2307/2026143</li>

<li>Jamieson, Dale and Marc Bekoff, 1992, &ldquo;Carruthers on
Nonconscious Experience&rdquo;, <em>Analysis</em>, 52(1): 23&ndash;28.
doi:10.1093/analys/52.1.23</li>

<li>Jehle, David and Uriah Kriegel, 2006, &ldquo;An Argument Against
Dispositionalist HOT Theory&rdquo;, <em>Philosophical Psychology</em>,
19(4): 463&ndash;476. doi:10.1080/09515080600729348</li>

<li>Kirk, Robert, 1994, <em>Raw Feeling: A Philosophical Account of
the Essence of Consciousness</em>, Oxford/New York: Clarendon Press.
doi:10.1093/acprof:oso/9780198236795.001.0001</li>

<li>Kirkeby-Hinrup, Asger, 2014, &ldquo;Why the Rare Charles Bonnet
Cases Are Not Evidence of Misrepresentation&rdquo;, <em>Journal of
Philosophical Research</em>, 39: 301&ndash;308.
doi:10.5840/jpr20148420</li>

<li>Koch, Christof, 2004, <em>The Quest for Consciousness: A
Neurobiological Approach</em>, Englewood Cliffs, NJ: Roberts and
Company Publishers.</li>

<li>Kosslyn, Stephen Michael, 1994, <em>Image and Brain: The
Resolution of the Imagery Debate</em>, Cambridge, MA: MIT Press.
doi:10.7551/mitpress/3653.001.0001</li>

<li>Kozuch, Benjamin, 2014, &ldquo;Prefrontal Lesion Evidence against
Higher-Order Theories of Consciousness&rdquo;, <em>Philosophical
Studies</em>, 167(3): 721&ndash;746.
doi:10.1007/s11098-013-0123-9</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;Underwhelming Force:
Evaluating the Neuropsychological Evidence for Higher&#8208;order
Theories of Consciousness&rdquo;, <em>Mind &amp; Language</em>, 37(5):
790&ndash;813. doi:10.1111/mila.12363</li>

<li>Kriegel, Uriah, 2003, &ldquo;Consciousness as Intransitive
Self-Consciousness: Two Views and an Argument&rdquo;, <em>Canadian
Journal of Philosophy</em>, 33(1): 103&ndash;132.
doi:10.1080/00455091.2003.10716537</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;The Same-Order Monitoring
Theory of Consciousness&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 143&ndash;170 (ch. 7). doi:10.7551/mitpress/6155.003.0008</li>

<li>___, 2007, "A Cross-Order Integration Hypothesis for the Neural
Correlate of Consciousness", <em>Consciousness and Cognition</em> 16 :
897 &ndash; 912 .</li>

<li>&ndash;&ndash;&ndash;, 2009, <em>Subjective Consciousness: A
Self-Representational Theory</em>, Oxford/New York: Oxford University
Press. doi:10.1093/acprof:oso/9780199570355.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Brentano&rsquo;s
Dual&#8208;Framing Theory of Consciousness&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 97(1): 79&ndash;98.
doi:10.1111/phpr.12327</li>

<li id="KW2006">Kriegel, Uriah and Kenneth Williford (eds), 2006,
<em>Self-Representational Approaches to Consciousness</em>, Cambridge,
MA: MIT Press. doi:10.7551/mitpress/6155.001.0001</li>

<li>Kripke, Saul A., 1972 [1980], &ldquo;Naming and Necessity:
Lectures Given to the Princeton University Philosophy
Colloquium&rdquo;, in <em>Semantics of Natural Language</em> (Synthese
Library), Donald Davidson and Gilbert Harman (eds), Dordrecht: Reidel,
253&ndash;355. Revised version in book form as <em>Naming and
Necessity</em>, Cambridge, MA: Harvard University Press and Oxford:
Blackwell, 1980. doi:10.1007/978-94-010-2557-7_9</li>

<li>Lamme, Victor, 2004, "Separate Neural Definitions of Visual
Consciousness and Visual Attention:&nbsp;A Case for Phenomenal
Awareness", <em>Neural Networks</em> 17 : 861 &ndash; 872 .</li>

<li>Lau, Hakwan, 2022, <em>In Consciousness We Trust: The Cognitive
Neuroscience of Subjective Experience</em>, Oxford/New York: Oxford
University Press. doi:10.1093/oso/9780198856771.001.0001</li>

<li>Lau, Hakwan and Richard Brown, 2019, &ldquo;The Emperor&rsquo;s
New Phenomenology? The Empirical Case for Conscious Experiences
without First-Order Representations&rdquo;, in <em>Blockheads! Essays
on Ned Block&rsquo;s Philosophy of Mind and Consciousness</em>, Adam
Pautz and Daniel Stoljar (eds), Cambridge, MA: MIT Press,
171&ndash;198. doi:10.7551/mitpress/9196.003.0012</li>

<li>Lau, Hakwan C. and Richard E. Passingham, 2006, &ldquo;Relative
Blindsight in Normal Observers and the Neural Correlate of Visual
Consciousness&rdquo;, <em>Proceedings of the National Academy of
Sciences</em>, 103(49): 18763&ndash;18768.
doi:10.1073/pnas.0607716103</li>

<li>Lau, Hakwan and David Rosenthal, 2011, &ldquo;Empirical Support
for Higher-Order Theories of Conscious Awareness&rdquo;, <em>Trends in
Cognitive Sciences</em>, 15(8): 365&ndash;373.
doi:10.1016/j.tics.2011.05.009</li>

<li>LeDoux, Joseph E. and Richard Brown, 2017, &ldquo;A Higher-Order
Theory of Emotional Consciousness&rdquo;, <em>Proceedings of the
National Academy of Sciences</em>, 114(10).
doi:10.1073/pnas.1619316114</li>

<li>Levine, Joseph, 1983, &ldquo;Materialism and Qualia: The
Explanatory Gap&rdquo;, <em>Pacific Philosophical Quarterly</em>,
64(4): 354&ndash;361. doi:10.1111/j.1468-0114.1983.tb00207.x</li>

<li>&ndash;&ndash;&ndash;, 2001, <em>Purple Haze: The Puzzle of
Consciousness</em> (Philosophy of Mind Series), New York: Oxford
University Press. doi:10.1093/0195132351.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Conscious Awareness and
(Self-)Representation&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 173&ndash;198 (ch. 8). doi: 10.7551/mitpress/6155.003.0010</li>

<li>Loar, Brian, 1990. "Phenomenal States", <em>Philosophical
Perspectives</em> 4: 81-108</li>

<li>Locke, John, 1690, <em>An Essay Concerning Human
Understanding</em>, London: Eliz. Holt. Many editions now
available.</li>

<li>Loewer, Barry and Georges Rey (eds), 1991, <em>Meaning in Mind:
Fodor and His Critics</em>, Oxford/Cambridge, MA: Blackwell.</li>

<li>Lycan, William G., 1987, <em>Consciousness</em>, Cambridge, MA:
MIT Press.</li>

<li>&ndash;&ndash;&ndash;, 1996, <em>Consciousness and
Experience</em>, Cambridge, MA: MIT Press.</li>

<li>&ndash;&ndash;&ndash;, 2001a, &ldquo;Have We Neglected Phenomenal
Consciousness?&rdquo;, <em>Journal Psyche</em>, 7: article 3.
 [<a href="https://journalpsyche.org/files/0xaa8f.pdf" target="other">Lycan 2001 available online (pdf)</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 2001b, &ldquo;A Simple Argument for a
Higher-Order Representation Theory of Consciousness&rdquo;,
<em>Analysis</em>, 61(1): 3&ndash;4. doi:10.1093/analys/61.1.3</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;The Superiority of HOP to
HOT&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 93&ndash;114.</li>

<li>Malach, Rafael, 2022, &ldquo;The Role of the Prefrontal Cortex in
Conscious Perception: The Localist Perspective&rdquo;, <em>Journal of
Consciousness Studies</em>, 29(7): 93&ndash;114.
doi:10.53765/20512201.29.7.093</li>

<li>McGinn, Colin, 1982, &ldquo;The Structure of Content&rdquo;, in
<em>Thought and Object: Essays on Intentionality</em>, Andrew
Woodfield (ed.), Oxford /New York: Clarendon Press,
207&ndash;258.</li>

<li>&ndash;&ndash;&ndash;, 1989, <em>Mental Content</em>, Oxford/New
York: Blackwell.</li>

<li>&ndash;&ndash;&ndash;, 1991, <em>The Problem of Consciousness:
Essays toward a Resolution</em>, Oxford/Cambridge, MA: B.
Blackwell.</li>

<li>Mehta, Neil, 2013, &ldquo;Is There a Phenomenological Argument for
Higher-Order Representationalism?&rdquo;, <em>Philosophical
Studies</em>, 164(2): 357&ndash;370.
doi:10.1007/s11098-012-9859-x</li>

<li>Michel, Matthias, 2022, &ldquo;Conscious Perception and the
Prefrontal Cortex A Review&rdquo;, <em>Journal of Consciousness
Studies</em>, 29(7): 115&ndash;157.
doi:10.53765/20512201.29.7.115</li>

<li>Michel, Matthias and Rafael Malach, 2022, &ldquo;Making Progress
on the Prefrontal Debate&rdquo;, <em>Journal of Consciousness
Studies</em>, 29(7): 158&ndash;164.
doi:10.53765/20512201.29.7.158</li>

<li>Michel, Matthias and Jorge Morales, 2020, &ldquo;Minority Reports:
Consciousness and the Prefrontal Cortex&rdquo;, <em>Mind &amp;
Language</em>, 35(4): 493&ndash;513. doi:10.1111/mila.12264</li>

<li>Miguens, Sofia, Gerhard Preyer, and Clara Morando (eds), 2015,
<em>Pre-Reflective Consciousness: Sartre and Contemporary Philosophy
of Mind</em>, Abingdon/New York: Routledge.
doi:10.4324/9781315681146</li>

<li>Millikan, Ruth Garrett, 1984, <em>Language, Thought, and Other
Biological Categories: New Foundations for Realism</em>, Cambridge,
MA: MIT Press. doi:10.7551/mitpress/4124.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1986, &ldquo;Thoughts Without Laws;
Cognitive Science with Content&rdquo;, <em>The Philosophical
Review</em>, 95(1): 47&ndash;80. doi:10.2307/2185132</li>

<li>&ndash;&ndash;&ndash;, 1989, &ldquo;Biosemantics&rdquo;, <em>The
Journal of Philosophy</em>, 86(6): 281&ndash;297.
doi:10.2307/2027123</li>

<li>Milner, A. D. and Melvyn A. Goodale, 1995, <em>The Visual Brain in
Action</em> (Oxford Psychology Series 27), Oxford/New York: Oxford
University Press.</li>

<li>Morales, Jorge and Hakwan Lau, 2020, &ldquo;The Neural Correlates
of Consciousness&rdquo;, in <em>The Oxford Handbook of the Philosophy
of Consciousness</em>, Uriah Kriegel (ed.), Oxford: Oxford University
Press, 231&ndash;260 (ch. 11).
doi:10.1093/oxfordhb/9780198749677.013.11</li>

<li>Nagel, Thomas, 1974, &ldquo;What Is It Like to Be a Bat?&rdquo;,
<em>The Philosophical Review</em>, 83(4): 435&ndash;450.
doi:10.2307/2183914</li>

<li>&ndash;&ndash;&ndash;, 1986, <em>The View from Nowhere</em>, New
York: Oxford University Press.</li>

<li>Neander, Karen, 1998, &ldquo;The Division of Phenomenal Labor: A
Problem for Representational Theories of Consciousness&rdquo;,
<em>Language, Mind, and Ontology</em>, James E. Tomberlin (ed.), issue
of <em>Philosphical Perspectives</em>, 12: 411&ndash;434.
doi:10.1111/0029-4624.32.s12.18</li>

<li>Nelkin, Norton, 1996, <em>Consciousness and the Origins of
Thought</em> (Cambridge Studies in Philosophy), Cambridge/New York:
Cambridge University Press. doi:10.1017/CBO9780511597992</li>

<li>Newen, Albert and Kai Vogeley, 2003, &ldquo;Self-Representation:
Searching for a Neural Signature of Self-Consciousness&rdquo;,
<em>Consciousness and Cognition</em>, 12(4): 529&ndash;543.
doi:10.1016/S1053-8100(03)00080-1</li>

<li>Odegaard, Brian, Robert T. Knight, and Hakwan Lau, 2017,
&ldquo;Should a Few Null Findings Falsify Prefrontal Theories of
Conscious Perception?&rdquo;, <em>The Journal of Neuroscience</em>,
37(40): 9593&ndash;9602. doi:10.1523/JNEUROSCI.3217-16.2017</li>

<li>Papineau, David, 1987, <em>Reality and Representation</em>
(Philosophical Theory), Oxford/New York: B. Blackwell.</li>

<li>&ndash;&ndash;&ndash;, 1993, <em>Philosophical Naturalism</em>,
Oxford/Cambridge, MA: Blackwell.</li>

<li>&ndash;&ndash;&ndash;, 2002, <em>Thinking about
Consciousness</em>, Oxford/New York: Clarendon Press.
doi:10.1093/0199243824.001.0001</li>

<li>Peacocke, Christopher, 1986, <em>Thoughts: An Essay on
Content</em> (Aristotelian Society Series 4), Oxford/New York: B.
Blackwell.</li>

<li>&ndash;&ndash;&ndash;, 1992, <em>A Study of Concepts</em>
(Representation and Mind), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/6537.001.0001</li>

<li>Peebles, Graham, 2022, &ldquo;The Problem of Higher-Order
Misrepresentation&rdquo;, <em>Philosophical Psychology</em>, 35(6):
842&ndash;861. doi:10.1080/09515089.2021.2016677</li>

<li>Phillips, Ben, 2014, &ldquo;Indirect Representation and the
Self-Representational Theory of Consciousness&rdquo;,
<em>Philosophical Studies</em>, 167(2): 273&ndash;290.
doi:10.1007/s11098-012-0087-1</li>

<li>Picciuto, Vincent, 2011, &ldquo;Addressing Higher-Order
Misrepresentation with Quotational Thought&rdquo;, <em>Journal of
Consciousness Studies</em>, 18(3&ndash;4): 109&ndash;136.</li>

<li>Pinker, Steven, 1994, <em>The Language Instinct</em>, New York: W.
Morrow and Co.</li>

<li>&ndash;&ndash;&ndash;, 1997, <em>How the Mind Works</em>, New
York: W.W. Norton.</li>

<li>Pollen, Daniel A., 2008, &ldquo;Fundamental Requirements for
Primary Visual Perception&rdquo;, <em>Cerebral Cortex</em>, 18(9):
1991&ndash;1998. doi:10.1093/cercor/bhm226</li>

<li>Povinelli, Daniel J., 2000, <em>Folk Physics for Apes: The
Chimpanzee&rsquo;s Theory of How the World Works</em>, New York:
Oxford University Press.
doi:10.1093/acprof:oso/9780198572190.001.0001</li>

<li>Prettyman, Adrienne, 2020, &ldquo;The Persistent Problem of
Targetless Thought&rdquo;, <em>Consciousness and Cognition</em>,
82(July): article 102918. doi:10.1016/j.concog.2020.102918</li>

<li>Prinz, Jesse J., 2012, <em>The Conscious Brain: How Attention
Engenders Experience</em> (Philosophy of Mind), New York: Oxford
University Press. doi:10.1093/acprof:oso/9780195314595.001.0001</li>

<li>Raccah, Omri, Ned Block, and Kieran C.R. Fox, 2021, &ldquo;Does
the Prefrontal Cortex Play an Essential Role in Consciousness?
Insights from Intracranial Electrical Stimulation of the Human
Brain&rdquo;, <em>The Journal of Neuroscience</em>, 41(10):
2076&ndash;2087. doi:10.1523/JNEUROSCI.1141-20.2020</li>

<li>Rey, Georges, 2008, &ldquo;(Even Higher-Order) Intentionality
Without Consciousness&rdquo;:, <em>Revue Internationale de
Philosophie</em>, 62(243/1): 51&ndash;78.
doi:10.3917/rip.243.0051</li>

<li>Rolls, Edmund T., 2004, &ldquo;A Higher-Order Syntactic Thought
(HOST) Theory of Consciousness&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 137&ndash;172 (ch. 7).</li>

<li>Rosenthal, David M., 1986, &ldquo;Two Concepts of
Consciousness&rdquo;, <em>Philosophical Studies</em>, 49(3):
329&ndash;359. doi:10.1007/BF00355521</li>

<li>&ndash;&ndash;&ndash;, 1993, &ldquo;Thinking That One
Thinks&rdquo;, in <em>Consciousness: Psychological and Philosophical
Essays</em> (Readings in Mind and Language 2), Martin Davies and Glyn
W. Humphreys (eds), Malden, MA: Blackwell Publishing,
197&ndash;223.</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;Varieties of Higher-Order
Theory&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 17&ndash;44 (ch. 2).</li>

<li>&ndash;&ndash;&ndash;, 2005, <em>Consciousness and Mind</em>,
Oxford/New York: Oxford University Press.
doi:10.1093/oso/9780198236979.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Exaggerated Reports: Reply to
Block&rdquo;, <em>Analysis</em>, 71(3): 431&ndash;437.
doi:10.1093/analys/anr039</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Misrepresentation and Mental
Appearance&rdquo;, <em>Trans/Form/A&ccedil;&atilde;o</em>, 41(spe):
49&ndash;74. doi:10.1590/0101-3173.2018.v41esp.05.p49</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;Mental Appearance and Mental
Reality&rdquo;, in
 <a href="#Weisberg2022">Weisberg 2022</a>:
 243&ndash;271 (ch. 14). doi:10.1017/9781108768085.018</li>

<li>Rounis, Elisabeth, Brian Maniscalco, John C. Rothwell, Richard E.
Passingham, and Hakwan Lau, 2010, &ldquo;Theta-Burst Transcranial
Magnetic Stimulation to the Prefrontal Cortex Impairs Metacognitive
Visual Awareness&rdquo;, <em>Cognitive Neuroscience</em>, 1(3):
165&ndash;175. doi:10.1080/17588921003632529</li>

<li>Rowlands, Mark, 2001, &ldquo;Consciousness and Higher&#8208;Order
Thoughts&rdquo;, <em>Mind &amp; Language</em>, 16(3): 290&ndash;310.
doi:10.1111/1468-0017.00171</li>

<li>Sartre, Jean-Paul, 1943 [1956], <em>L&rsquo;&ecirc;tre et le
n&eacute;ant: essai d&rsquo;ontologie
ph&eacute;nom&eacute;nologique</em> (Biblioth&egrave;que des
id&eacute;es), Paris: Gallimard. Translated as <em>Being and
Nothingness: An Essay on Phenomenological Ontology</em>, Hazel E.
Barnes (trans.), New York: Philosophical Library, 1956.</li>

<li>Sauret, Wesley and William G. Lycan, 2014, &ldquo;Attention and
Internal Monitoring: A Farewell to HOP&rdquo;, <em>Analysis</em>,
74(3): 363&ndash;370. doi:10.1093/analys/anu055</li>

<li>Searle, John R., 1992, <em>The Rediscovery of the Mind</em>
(Representation and Mind), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/5834.001.0001</li>

<li>Searle, John R., 1995 [1997], &ldquo;<em>The Mystery of
Consciousness</em>, Review of works by Francis Crick, Daniel C.
Dennett, Gerald Edelman, Roger Penrose, and Israel Rosenfield&rdquo;,
<em>The New York Review of Books</em>, 2 November 1995. Collected in
<em>The Mystery of Consciousness</em>, John R. Searle with Daniel C.
Dennett and David J. Chalmers, London: Granta Books, 1997.</li>

<li>Seager, William, 1994, &ldquo;Dretske on HOT Theories of
Consciousness&rdquo;, <em>Analysis</em>, 54(4): 270&ndash;276.
doi:10.1093/analys/54.4.270</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;A Cold Look at HOT
Theory&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 255&ndash;276 (ch. 11).</li>

<li>Seth, Anil, and Tim Bayne, 2022. "Theories of Consciousness",
<em>Nature Reviews Neuroscience</em> 23: 439-452.</li>

<li>Shepherd, Joshua, 2013, &ldquo;Why Block Can&rsquo;t Stand the
HOT&rdquo;, <em>Journal of Consciousness Studies</em>, 20(3&ndash;4):
183&ndash;195.</li>

<li>Siegel, Susanna, 2010, <em>The Contents of Visual Experience</em>,
Oxford/New York: Oxford University Press.
doi:10.1093/acprof:oso/9780195305296.001.0001</li>

<li>Siewert, Charles P., 1998, <em>The Significance of
Consciousness</em>, Princeton, NJ: Princeton University Press.</li>

<li>Simons, Daniel J., 2000, &ldquo;Current Approaches to Change
Blindness&rdquo;, <em>Visual Cognition</em>, 7(1&ndash;3): 1&ndash;15.
doi:10.1080/135062800394658</li>

<li>Simons, Daniel J and Christopher F. Chabris, 1999, &ldquo;Gorillas
in Our Midst: Sustained Inattentional Blindness for Dynamic
Events&rdquo;, <em>Perception</em>, 28(9): 1059&ndash;1074.
doi:10.1068/p281059</li>

<li>Sperber, Dan, 1996, <em>Explaining Culture: A Naturalistic
Approach</em>, Cambridge, MA: Blackwell.</li>

<li>Stubenberg, Leopold, 1998, <em>Consciousness and Qualia</em>
(Advances in Consciousness Research, 5), Amsterdam/Philadelphia: J.
Benjamins.</li>

<li>Sturgeon, Scott, 2000, <em>Matters of Mind: Consciousness, Reason
and Nature</em> (International Library of Philosophy), London/New
York: Routledge.</li>

<li>Terrace, Herbert S. and Janet Metcalfe (eds), 2005, <em>The
Missing Link in Cognition: Origins of Self-Reflective
Consciousness</em>, New York: Oxford University Press.
doi:10.1093/acprof:oso/9780195161564.001.0001</li>

<li>Tsuchiya, Naotsugu, Melanie Wilke, Stefan Fr&auml;ssle, and Victor
A.F. Lamme, 2015, &ldquo;No-Report Paradigms: Extracting the True
Neural Correlates of Consciousness&rdquo;, <em>Trends in Cognitive
Sciences</em>, 19(12): 757&ndash;770.
doi:10.1016/j.tics.2015.10.002</li>

<li>Tye, Michael, 1995, <em>Ten Problems of Consciousness: A
Representational Theory of the Phenomenal Mind</em> (Representation
and Mind), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/6712.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1999, &ldquo;Phenomenal Consciousness: The
Explanatory Gap as a Cognitive Illusion&rdquo;, <em>Mind</em>,
108(432): 705&ndash;725. doi:10.1093/mind/108.432.705</li>

<li>&ndash;&ndash;&ndash;, 2000, <em>Consciousness, Color, and
Content</em> (Representation and Mind), Cambridge, MA: MIT Press.
doi:10.7551/mitpress/2110.001.0001</li>

<li>Van Gulick, Robert, 2000, &ldquo;Inward and Upward: Reflection,
Introspection, and Self-Awareness&rdquo;, <em>Philosophical
Topics</em>, 28(2): 275&ndash;305.
doi:10.5840/philtopics200028222</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;Higher-Order Global States
(HOGS): An Alternative Higher-Order Model of Consciousness&rdquo;, in
 <a href="#Gennaro2004">Gennaro 2004b</a>:
 67&ndash;92 (ch. 4).</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Mirror, Mirror&mdash;Is That
All?&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 11&ndash;40 (ch. 2). doi:10.7551/mitpress/6155.003.0003</li>

<li>Weisberg, Josh, 2011, &ldquo;Abusing the Notion of
What-It&rsquo;s-like-Ness: A Response to Block&rdquo;,
<em>Analysis</em>, 71(3): 438&ndash;443.
doi:10.1093/analys/anr040</li>

<li id="Weisberg2022">&ndash;&ndash;&ndash; (ed.), 2022,
<em>Qualitative Consciousness: Themes from the Philosophy of David
Rosenthal</em>, Cambridge/New York: Cambridge University Press.
doi:10.1017/9781108768085</li>

<li>Weiskrantz, Lawrence, 1986, <em>Blindsight: A Case Study and
Implications</em> (Oxford Psychology Series 12), Oxford/New York:
Clarendon Press. doi:10.1093/acprof:oso/9780198521921.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1997, <em>Consciousness Lost and Found: A
Neuropsychological Exploration</em>, Oxford/New York: Oxford
University Press. doi:10.1093/acprof:oso/9780198524588.001.0001</li>

<li>Whiten, Andrew and Richard W. Byrne (eds), 1997, <em>Machiavellian
Intelligence II: Extensions and Evaluations</em>, Cambridge: Cambridge
University Press. doi:10.1017/CBO9780511525636</li>

<li>Wilberg, Jonah, 2010, &ldquo;Consciousness and False HOTs&rdquo;,
<em>Philosophical Psychology</em>, 23(5): 617&ndash;638.
doi:10.1080/09515089.2010.514567</li>

<li>Williford, Kenneth, 2006, &ldquo;The Self-Representational
Structure of Consciousness&rdquo;, in
 <a href="#KW2006">Kriegel and Williford 2006</a>:
 111&ndash;142 (ch. 6). doi:10.7551/mitpress/6155.003.0007</li>

<li>Zahavi, Dan, 2004, &ldquo;Back to Brentano?&rdquo;, <em>Journal of
Consciousness Studies</em>, 11(10&ndash;11): 66&ndash;87.</li>
</ul>
</div> 

<div id="academic-tools">
<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=consciousness-higher" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/consciousness-higher/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=consciousness-higher&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/consciousness-higher/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>
</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<ul>

<li>Saxe, Rebecca, unpublished,
 &ldquo;<a href="https://saxelab.mit.edu/wp-content/uploads/2018/11/Saxe_RTPJChapter.pdf" target="other">The Right Temporo-Parietal Junction: A Specific Brain Region for Thinking about Thoughts</a>&rdquo;,
 unpublished manuscript, 2010.</li>
 <li><a href="https://philpapers.org/browse/higher-order-theories-of-consciousness" target="other">Bibliography on Higher-Order Theories of Consciousness</a>,
 at PhilPapers.</li>
 <li><a href="https://web-archive.southampton.ac.uk/cogprints.org/" target="other">Cognitive Science E-print Archive</a></li>
 </ul>
</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>
 <a href="../consciousness-animal/">animal: consciousness</a> |
 <a href="../consciousness/">consciousness</a> |
 <a href="../consciousness-intentionality/">consciousness: and intentionality</a> |
 <a href="../consciousness-representational/">consciousness: representational theories of</a>
 </p>
</div> 

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>As of the 2020 update, Rocco Gennaro has taken over responsibility
for updating and maintaining this entry.</p>
</div>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2026</a> by

<br />
<a href="http://www.philosophy.umd.edu/Faculty/pcarruthers/" target="other">Peter Carruthers</a>
<br />
Rocco Gennaro
&lt;<a href="m&#97;ilto:rjgennaro&#37;40usi&#37;2eedu"><em>rjgennaro<abbr title=" at ">&#64;</abbr>usi<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
            <li role="menuitem"><a href="../../new.html">What's New</a></li>
            <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li role="menuitem"><a href="../../published.html">Chronological</a></li>
            <li role="menuitem"><a href="../../archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
            <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
            <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
            <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
            <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
            <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
            <li role="menuitem"><a href="../../accessibility.html">Accessibility</a></li>
            <li role="menuitem"><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
            <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
            <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2026</a> by <a href="https://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>
</html>
