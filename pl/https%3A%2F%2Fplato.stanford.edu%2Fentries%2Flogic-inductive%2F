<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html lang="en"> <!-- <![endif]-->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Inductive Logic (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Inductive Logic" />
<meta property="citation_author" content="Hawthorne, James" />
<meta property="citation_publication_date" content="2004/09/06" />
<meta name="DC.title" content="Inductive Logic" />
<meta name="DC.creator" content="Hawthorne, James" />
<meta name="DCTERMS.issued" content="2004-09-06" />
<meta name="DCTERMS.modified" content="2025-02-24" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
                    <li role="menuitem"><a href="../../new.html">What's New</a></li>
                    <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li role="menuitem"><a href="../../published.html">Chronological</a></li>
                    <li role="menuitem"><a href="../../archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
                    <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
                    <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
                    <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
                    <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
                    <li role="menuitem"><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
                    <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
                    <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit" aria-label="search"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/logic-inductive/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-inductive">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Inductive Logic</h1><div id="pubinfo"><em>First published Mon Sep 6, 2004; substantive revision Mon Feb 24, 2025</em></div>

<div id="preamble">

<p>
An inductive logic is a system of reasoning that extends deductive
logic to less-than-certain inferences. A logic represents inferences
in terms of arguments, where each argument consists of premises and a
conclusion. The essence of a logic is the arguments it endorses. A
logic labels some arguments as <em>good</em> and others as <em>not
good</em>, depending on the extent to which the truth of an
argument&rsquo;s premises support the truth of its conclusion. In a
deductive logic the truth of the premises of a <em>good</em> argument
guarantees the truth of its conclusion. These good deductive arguments
are called <em>deductively valid</em>; their premises are said to
<em>logically entail</em> their conclusions, where <em>logical
entailment</em> means that every logically possible state of affairs
that makes the premises true also makes the conclusion true. In an
inductive logic the truth of the premises of a <em>good</em> argument
support the truth of its conclusion to some appropriate degree. That
is, the truth of the premises provides some <em>degree-of-support</em>
for (or against) the truth of its conclusion. This
<em>degree-of-support</em> is typically measured on some numerical
scale. By analogy with the notion of deductive logical entailment, the
notion of inductive degree-of-support may be taken to mean something
like this: among the logically possible states of affairs that make
the premises true, the conclusion is true in proportion <em>r</em> of
them.</p>

<p>
This article explicates the inductive logic most widely studied by
logicians and epistemologists in recent years. The logic employs
conditional probability functions to represent the degree to which an
argument&rsquo;s premises support its conclusion. This approach is
often called a <em>Bayesian</em> inductive logic, because a theorem of
probability theory called Bayes&rsquo; Theorem plays a central role in
articulating how evidence claims inductively support hypotheses.</p>

<p>
Ultimately, any adequate inductive logic should provide a mechanism
whereby evidence may legitimately refute false hypotheses and endorse
true ones. That is, any legitimate inductive logic should provide at
least a modest version of the most famous epistemological remark
attributed to Sherlock Holmes:</p>

<blockquote>
When you have eliminated all which is impossible, then whatever
remains, however improbable, must be the truth.
</blockquote>

<p>
Although this remark overstates what an inductive logic can usually
accomplish, the underlying idea is basically right. That is, a logic
of evidential support aspires to endorse the following more modest
principle:</p>

<blockquote>
When a rigorous body of evidence shows all of the credible alternative
hypotheses to be extremely unlikely, then whatever hypothesis remains,
however initially implausible, must very probably be true.
</blockquote>

<p>
This idea, that evidence may come to support the truth of a hypothesis
by eliminating its competitors, is central to the workings of a
Bayesian logic of evidential support. This article will describe in
some detail how a Bayesian inductive logic works. </p>

<p>
Section 1 explicates the most important inference rules for a Bayesian
inductive logic. These rules articulate how some probabilistic
arguments may be combined to determine the degree to which evidence
weighs for or against hypotheses (as expressed by other probabilistic
arguments). Section 2 provides examples of the application of these
inference rules.</p>
</div> 

<div id="toc">
<!--Entry Contents-->
<ul>

<li><a href="#Inference-Rules">1. Principal Inference Rules for the Logic of Evidential Support</a>
	<ul>
	<li><a href="#L-Notation">1.1 Logical Notation</a></li>
	<li><a href="#L-Axioms">1.2 Logical Axioms for Support Functions</a></li>
	<li><a href="#Components">1.3 Elements of the Inference Rules for Inductive Logic</a></li>
        <li><a href="#Rule-RB">1.4 Inference Rule <em>RB</em>: the Ratio Form of Bayes&rsquo; Theorem</a></li>
        <li><a href="#Rule-OB">1.5 Inference Rule <em>OB</em>: the Odds Form of Bayes&rsquo; Theorem</a></li>
        <li><a href="#Estimation">1.6 Inference Rules for Bayesian Interval Estimation</a></li>
        <li><a href="#Auxiliaries">1.7 On the Epistemic Status of Auxiliary Hypotheses</a></li>
        </ul></li>

<li><a href="#Examples">2. Examples</a>
        <ul>
	<li><a href="#Stat-Evid">2.1 Testing Scientific Hypothesis with Statistical Evidence</a></li>
        <li><a href="#Med-test">2.2 An Application to Medical Tests: Covid-19 Self-Tests</a></li>
        <li><a href="#Vague-Likelihoods">2.3 Imprecise Likelihoods</a></li>
        <li><a href="#Disjunction-Stat-Hypoth">2.4 Bayesian Estimation for Disjunctions of Alternative Hypotheses</a></li>
        <li><a href="#Continuous-Stat-Hypoth">2.5 Bayesian Estimation for a Continuous Range of Alternative Hypotheses</a></li>
	</ul></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->
<hr />

</div>

<div id="main-text">

<h2 id="Inference-Rules">1. Principal Inference Rules for the Logic of Evidential Support</h2>

<p>
This section lays out the fundamental elements of a probabilistic
(Bayesian) inductive logic. We first develop appropriate notation and
specify the logical axioms for the conditional probability functions.
These conditional probability functions will be used to represent
inductive arguments. Next we briefly describe the two most fundamental
component arguments in the inference rules for Bayesian inductive
inferences: (1) the <em>evidential likelihoods</em>, and (2) the
<em>prior plausibility assessments</em> of hypotheses. Then we
explicate four of the most important inference rules for this kind of
inductive logic, rules that employ the probability values from
likelihood arguments and the prior plausibility arguments to determine
the probability values for arguments from evidential premises to
hypotheses.</p>

<p>
In the main body of this article we will forgo a discussion of the
historical origins of probabilistic inductive logic. See the appendix
 <a href="appendix1.html">Historical Origins and Interpretations of Probabilistic Inductive Logic</a>
 for an overview of the origins, and for a brief summary of views
about the nature of probabilistic inductive logic. </p>

<h3 id="L-Notation">1.1 Logical Notation</h3>

<p>
In a probabilistic argument, the degree to which a premise statement
\(D\) supports the truth or falsehood of a conclusion statement \(C\)
is expressed in terms of a conditional probability function \(P\). A
formula of form \(P[C \mid D] = r\) expresses the claim that premise
\(D\) supports conclusion \(C\) to degree \(r\), where \(r\) is a real
number between 0 and 1. Notice that the conclusion \(C\) is placed on
the left-hand side of the conditional probability expression, followed
by the premise \(D\) on the right-hand side. This reverses the order
of premise and conclusion employed in the standard expressions for
deductive logical entailment, where the logical entailment of a
conclusion \(C\) by premise \(D\) is usually represented by an
expression of form \(D \vDash C\).</p>

<p>
In applications of deductive logic the main challenge is to determine
whether or not a logical entailment, \(D \vDash C\), holds for
arguments consisting of premises \(D\) and conclusions \(C\).
Similarly, the main challenge in a probabilistic inductive logic is to
determine the appropriate values of \(r\) such that \(P[C \mid D] =
r\) holds for arguments consisting of premises \(D\) and conclusions
\(C\). The probabilistic formula \(P[C \mid D] = r\) may be read in
either of two ways: literally <i>the probability of \(C\) given \(D\)
is \(r\)</i>; but also, <i>apropos</i> the application of probability
functions <i>P</i> to represent argument strengths, <i>the degree to
which \(C\) is supported by \(D\) is \(r\)</i>.</p>

<p>
Throughout our discussion we use common logical notation for
conjunctions, disjunctions, and negations. We use a <i>dot</i> between
sentences, \((A \cdot B)\), to represent their conjunction, (\(A\)
<i>and</i> \(B\)); and we use a <i>wedge</i> between sentences, \((A
\vee B)\), to represent their disjunction, (\(A\) <i>or</i> \(B\)).
Disjunction is taken to be inclusive: \((A \vee B)\) means that <i>at
least one</i> of \(A\) or \(B\) is true. We use the <i>not</i> symbol
\(\neg\) in front of a sentence to represent its negation: \(\neg C\)
means <i>it&rsquo;s not the case that</i> \(C\).</p>

<h3 id="L-Axioms">1.2 Logical Axioms for Conditional Probability Functions</h3>

<p>
Here are standard logical axioms for conditional probabilities. They
supply minimal rules for probabilistic support functions. That is,
support functions should satisfy at least these axioms, and perhaps
some additional rules as well.</p>

<div class="indent">

<p>
Let \(L\) be a language of interest &mdash; i.e. any bit of language
in which the inductive arguments of interest may be expressed &mdash;
and let \(\vDash\) be the logical entailment relation for this
language. A conditional probability function (i.e. a probabilistic
support function) is a function \(P\) from pairs of statements of
\(L\) to real numbers that satisfies (at least) the following axioms.
</p>

<ol class="left">

<li>There are statements \(U\), \(V\), \(X\), and \(Y\) such that
\(P[U \mid V] \neq P[X \mid Y]\)
<br />
this <i>nontrivality axiom</i> rules out the function \(P\) that
assigns probability value 1 to every argument;</li>
</ol>

<p>
For all statements \(A\), \(B\), and \(C\) in \(L\):</p>

<ol start="2">

<li>\(0 \le P[A \mid B] \le 1\)
<br />
premises support conclusions to some degree measured by real numbers
between 0 and 1;</li>

<li>If \(B \vDash A\), then \(P[A \mid B] = 1\)
<br />
the premises of a logical entailment support its conclusion to degree
1;</li>

<li>If \(C \vDash B\) and \(B \vDash C\), then \(P[A \mid B] = P[A
\mid C]\)
<br/>
logically equivalent premises support a conclusion to the same
degree;</li>

<li>If \(C \vDash \neg(A \cdot B)\), then \(P[(A \vee B) \mid C] = P[A
\mid C] + P[B \mid C]\), unless \(P[D \mid C] = 1\) for every
statement \(D\);</li>

<li>\(P[(A \cdot B) \mid C] = P[A \mid (B \cdot C)] \times P[B \mid
C]\).</li>
</ol>
</div>

<p>
These axioms do not presuppose that logically equivalent statements
have the same probability. Rather, that can be proved from these
axioms.</p>

<p>
Axioms 1-4 should be clear enough as stated. Axiom 5 says that when
\(C \vDash \neg(A \cdot B)\) (i.e. when \(C\) logically entails that
\(A\) and \(B\) cannot both be true), the support-strength of \(C\)
for their disjunction, \((A \vee B)\), must equal the sum of its
support-strengths for each of them individually. The only exception to
this additivity condition occurs when \(C\) supports every statement
\(D\) to degree 1. That can happen, for example, when \(C\) is
logically inconsistent, since (according to standard deductive logic)
logically inconsistent statements must logically entail every
statement \(D\).</p>

<p>
The following four rules follow easily from axioms 2, 3, and 5:</p>

<ol class="left">

<li>\(P[\neg A \mid C] = 1 - P[A \mid C]\), unless \(P[D \mid C] = 1\)
for every statement \(D\).</li>

<li>If \((C \cdot B) \vDash A\), then \(P[A \mid C] \ge P[B \mid
C]\).</li>

<li>If \((C \cdot B) \vDash A\) and \((C \cdot A) \vDash B\), then
\(P[A \mid C] = P[B \mid C]\).</li>

<li>Let \(A_1\), \(A_2\), &hellip;, \(A_n\) be \(n\) statements such
that, for each pair of them \(A_i\) and \(A_j\), \(C \vDash \neg(A_i
\cdot A_j)\). Then \(P[(A_1 \vee A_2 \vee \ldots \vee A_n) \mid C]\
=\) \(P[A_1 \mid C] + P[A_2 \mid C] + \ldots + P[A_n \mid C]\), unless
\(P[D \mid C] = 1\) for every statement \(D\).</li>
</ol>

<p>
These results are derived in the appendix,
 <a href="appendix2.html">Axioms and Some Theorems for Conditional Probability</a>.
 This appendix also includes an alternative way to axiomatize
conditional probability, which draws on much weaker axioms to arrive
at the same results (i.e. all the above axioms and theorems are
derivable from these weaker axioms). </p>

<p>
Axiom 6 expresses a fundamental relationship between conditional
probabilities. Think of it like this. Call the collection of logically
possible states of affairs where a statement \(C\) is true <i>the
\(C\) states</i>. Consider the proportion \(p\) of \(C\) states that
are also \(B\) states: \(P[B \mid C] = p\). A certain fraction \(f\)
of those \((B \cdot C)\) states are also \(A\) states: \(P[A \mid (B
\cdot C)] = f\). Then, the proportion of the \(C\) states that are
\((A \cdot B)\) states, \(P[(A \cdot B) \mid C]\), <i>should be</i>
the fraction \(f\) of proportion \(p\), which is given by \(f \times
p\). That is, the proportion of the \(C\) states that are \((A \cdot
B)\) states <i>should be</i> the fraction of \((B \cdot C)\) states
that are also \(A\) states, \(f\), of the proportion of \(C\) states
that are \(B\) states, \(p\):</p> 
\[P[(A \cdot B) \mid C] = f \times p = P[A \mid (B \cdot C)] \times
P[B \mid C].\]

<p>
From axiom 6, together with axioms 3 and 5, a simple form of
Bayes&rsquo; Theorem follows: if \(P[B \mid C] \gt 0\), then</p>

\[P[A \mid (B \cdot C)] = \dfrac{P[B \mid (A \cdot C)] \times P[A \mid
C]}{P[B \mid C]}.\]

<p>
To see how Bayes&rsquo; Theorem can represent an inference rule
governing the evidential support for a hypothesis, replace \(A\) by
some hypothesis \(h\), replace \(B\) by some relevant body of evidence
\(e\), and let \(c\) represent some appropriate conjunction of
background and auxiliary conditions, including whatever experimental
or observational conditions (a.k.a. <i>initial conditions</i>) may be
required to link \(h\) to \(e\) (more about this below). Then, the
appropriate version of Bayes&rsquo; Theorem takes the following form:
if \(P[e \mid c] \gt 0\), then</p> 
\[P[h \mid (e \cdot c)] = \dfrac{P[e \mid (h \cdot c)] \times P[h \mid
c]}{P[e \mid c]}.\]

<p>
Thus, Bayes&rsquo; Theorem represents the way in which the strength of
the evidential support for a hypothesis, \(P[h \mid (e \cdot c)]\),
can be calculated from the strengths of three other probabilistic
arguments: \(P[e \mid (h \cdot c)]\), \(P[h \mid c]\), and \(P[e \mid
c]\). Stated this way, Bayes&rsquo; Theorem may not look much like an
inference rule. So, let&rsquo;s articulate more precisely how an
equation like this may be construed as an inference rule. It
represents a rule that draws on the strengths of three probabilistic
arguments to infer the strength of a further argument. Thus, as an
inference rule, Bayes&rsquo; Theorem may be expressed as follows:</p>

<dl class="sentag tag3em indent">
<dt><b>if</b>:</dt>
<dd> the strength of the argument from \(c\) to \(e\) is \(q\), for
\(q \gt 0\)
<br />
&nbsp; (i.e. \(P[e \mid c] = q \gt 0\)), and
<br />
the strength of the argument from \((h \cdot c)\) to \(e\) is \(r\)
<br />
&nbsp; (i.e. \(P[e \mid (h \cdot c)] = r\)), and
<br />
the strength of the argument from \(c\) to \(h\) is \(s\)
<br />
&nbsp; (i.e. \(P[h \mid c] = s\)),</dd>
<dt><b>then</b>:</dt>
<dd> the strength of the argument from \((e \cdot c)\) to \(h\) is \(t
= r \times s / q\)
<br />
&nbsp; (i.e. then \(P[h \mid (e \cdot c)] = t\), where \(t = r \times
s / q\)).</dd>
</dl>

<p>
Each of the inference rules for the inductive logic of evidential
support presented in this article is based on this basic Bayesian
idea. However, it usually turns out that the numerical value \(q\) of
the strength of the argument \(P[e \mid c] = q\) is especially
difficult to evaluate. So, the Bayesian inference rules provided
throughout the remainder of this article will not depend on
probabilistic arguments of the form \(P[e \mid c] = q\). Furthermore,
the strengths \(s\) of arguments of form \(P[h \mid c] = s\) are often
quite vague or indeterminate. This issue will receive special
attention as we proceed. </p>

<p>
We now proceed to consider four basic rules of Bayesian inference for
an inductive logic. Each of these rules follows from the above axioms.
However, before getting into the rules themselves, we need to first
investigate more carefully the two kinds of argumentative components
that will be employed by each of these rules: \(P[e \mid (h \cdot c)]
= r\) and \(P[h \mid c] = s\). </p>

<h3 id="Components">1.3 Components of the Inference Rules for Inductive Logic</h3>

<p>
In nearly all applications of probabilistic inductive logic, the
arguments of interest involve an assessment of the degree to which
observable or detectable evidence \(e\) tells for or against a
hypothesis and its competing alternatives. Let \(h_1\), \(h_2\),
\(h_3\), &hellip;, etc., represent a collection of two or more
competing alternative hypotheses. Hypotheses count as <i>competing
alternatives</i> when they address the same subject matter, but
disagree with regard to at least some claims about that subject
matter. Thus, we take any two alternative hypotheses from the
collection, \(h_i\) and \(h_j\), to be logically incompatible:
\(\vDash \neg (h_i \cdot h_j)\) &mdash; i.e. it is logically true that
\(\neg (h_i \cdot h_j)\).</p>

<p>
The bearing of evidence on the probable truth or falsehood of a
hypothesis can seldom, if ever, be assessed on the basis of evidential
results alone. For one thing, the bearing of evidential results \(e\)
on hypothesis \(h_j\) depends on the conditions under which the
observations were made, or on how the experiment was set up and
conducted. Let \(c\) represent (a conjunction of) statements that
describe the observational or experimental conditions (sometimes
called the <i>initial conditions</i>) that give rise to evidential
results described by (conjunction of) statements \(e\).</p>

<p>
Furthermore, the bearing of evidential conditions and their outcomes,
\((c \cdot e)\), on a hypothesis \(h_j\) will often depend on
auxiliary hypotheses &mdash; e.g. auxiliary claims about how measuring
devices produce outcomes relevant to \(h_j\) under conditions like
\(c\). Let \(b\) represent the conjunction of all such auxiliary
claims that connect each competing hypothesis, \(h_i\), \(h_j\), etc.
to outcomes \(e\) of conditions \(c\). For example, suppose the
various hypotheses propose alternative medical disorders that may be
afflicting a particular patient. Conditions \(c\) may describe a body
of medical tests performed on the patient (e.g. blood drawn and
submitted to various specific tests), and \(e\) may state the precise
outcomes of those tests (e.g. precise values for white cell count,
blood sugar level, AFP level, etc.). However, descriptions of medical
tests and their outcomes can only weigh for or against the presence of
a disorder in light of auxiliary hypotheses about the ways in which
each disorder \(h_j\) is likely to influence those test outcomes (e.g.
how each possible medical disorder is likely to influence white cell
counts, blood sugar levels, AFP levels, etc.). The expression \(b\),
for <i>b</i>ackground claims, represents the conjunction of such
auxiliaries. (Many of the claims in \(b\) should themselves be subject
to evidential support in contexts where they compete with alternative
claims about their own subject matters. More on this later.)</p>

<p>
A comprehensive assessment of the probable truth of a hypothesis
should also depend on some body of plausibility considerations &mdash;
on how much more (or less) plausible \(h_j\) is than alternatives
\(h_i\), based on considerations <i>prior</i> to bringing the evidence
to bear. A reasonable inductive logic should reflect the idea that
<i>extraordinary claims require extraordinarily evidence</i>. That is,
a hypothesis that makes extraordinary claims requires exceptionally
strong evidence to overcome its initial implausibility. So, it makes
good sense that the logic should have a way to accommodate how much
more or less plausible one hypothesis is than an alternative, prior to
taking the evidence into account. For example, in diagnosing a medical
disorder, it makes good sense to take into account how commonly (or
rarely) each alternative disorder occurs within the most relevant
sub-population to which the patient belongs. This is called the
<i>base rates</i> of disorders in the relevant sub-population.
We&rsquo;ll soon see how such considerations figure into the inference
rules of inductive logic. For the purpose of describing the logic, we
also let symbol \(b\) represent the conjunction of whatever relevant
plausibility considerations are brought to bear on the initial
plausibilities of hypotheses, along with whatever relevant auxiliary
hypotheses are employed.</p>

<p>
Expressed in these terms, a primary objective of a probabilistic
inductive logic is to assess the degree-of-support for (or against)
each competing hypothesis \(h_j\) by a premise of form \((c \cdot
e\cdot b)\), consisting of evidential condition \(c\) together with
its observable outcome \(e\), in conjunction with relevant auxiliary
hypotheses and plausibility claims \(b\). That is, the objective is to
determine the numerical value \(t\) for a probabilistic argument of
form \(P[h_j \mid c \cdot e\cdot b] = t\). This expression is usually
called the <em>posterior probability</em> of hypothesis \(h_j\) on
evidence \((c \cdot e)\), given background \(b\). Thus, the primary
objective of the logic is to assess the values \(t\) of the
<em>posterior probabilities</em> of such evidential arguments.</p>

<p>
The most basic inference rule for the Bayesian logic of evidential
support is comparative in nature. That is, this most basic rule does
not directly provide values for individual posterior probabilities.
Rather, it provides <i>ratio comparisons</i> of the posterior
probabilities (the argument weights) for competing hypotheses.</p>

<p>
Let \(h_i\) and \(h_j\) be any two distinct hypotheses from a list of
competing alternatives. The <em>comparative degrees-of-support</em>
for these two hypotheses is given by a numerical value \(q\) for the
ratio of their posterior probabilities: \(P[h_i \mid c \cdot e\cdot b]
/ P[h_j \mid c \cdot e\cdot b] = q\). This ratio measures how much
more (or less) strongly the premise \((c \cdot e \cdot b)\) supports
\(h_i\) than it supports \(h_j\). The most basic rule for the logic
states a direct way to calculate the values \(q\) for such ratios; and
it does this without providing values for the individual posterior
probabilities, \(P[h_i \mid c \cdot e \cdot b]\) and \(P[h_j \mid c
\cdot e \cdot b]\), themselves. We&rsquo;ll see how this works when we
introduce the relevant inference rule, in the next subsection.</p>

<p>
The inference rule for determining the value \(q\) of a posterior
probability ratio draws on only two distinct kinds of probabilistic
arguments:</p>

<div class="indent">

<p>
<b>1. The <em>likelihoods</em> of the evidence according to various
hypotheses</b>: A <em>likelihood</em> is a probabilistic argument of
form \(P[e \mid h_k \cdot c \cdot b] = r\). It is a probabilistic
argument from premises \((h_k \cdot c \cdot b)\) to a conclusion
\(e\). This argument expresses what hypothesis \(h_k\) <em>says</em>
about <em>how likely it is</em> that evidence claim \(e\) should be
true when evidential conditions \(c\) and auxiliary claims stated
within \(b\) are also true. Likelihoods express the empirical content
of a hypothesis, what it <em>says</em> an observable part of the world
is probably like. In order for two hypotheses, \(h_i\) and \(h_j\), to
differ in empirical content (given \(b\)), there must be some
<i>possible</i> evidential conditions \(c\) that have possible
outcomes \(e\) on which the likelihoods for the two hypotheses
disagree:</p>

<blockquote>
\(P[e \mid h_i \cdot c \cdot b] = r \neq s = P[e \mid h_j \cdot c
\cdot b].\)
</blockquote>

<p>
It turns out that Bayesian inductive inference rules don&rsquo;t
depend directly on the individual values of likelihoods, but only on
the values \(v\) of <i>ratios of likelihoods</i>: </p>

<blockquote>
\(v = P[e \mid h_i \cdot c \cdot b] / P[e \mid h_j \cdot c \cdot b]\).
</blockquote>

<p>
These <em>likelihood ratios</em> (a.k.a. <em>Bayes Factors</em>)
represent how much more (or less) likely the evidential outcome \(e\)
should be if hypothesis \(h_i\) is true than if alternative hypothesis
\(h_j\) is true. They embody the means by which empirical content
evidentially distinguishes between two competing hypotheses.</p>

<p>
In many scientific contexts the exact values of individual likelihoods
are calculable, often via some explicit statistical model on which the
hypothesis together with auxiliaries, \((h_k \cdot b)\), draws.
Clearly, in contexts where the exact values of likelihoods are
calculable, exact values of these likelihood ratios are calculable as
well. However, even in cases where the individual hypotheses, \(h_i\)
and \(h_j\), provide somewhat vague or imprecise information regarding
the values for individual likelihoods, it may be possible to assess
reasonable estimates of upper and lower bounds on their likelihood
ratios. We will see how such bounds on likelihood ratios may provide
important evidential inputs for the inductive inference rules.</p>

<p>
When the evidence consists of a collection of \(m\) distinct
experiments or observations and their outcomes, \((c_1 \cdot e_1)\),
\((c_2 \cdot e_2)\), &hellip;, \((c_m \cdot e_m)\), we use the term
\(c\) to represent the conjunction of these experimental or
observational conditions, \((c_1 \cdot c_2 \cdot \ldots \cdot c_m)\),
and we use the term \(e\) to represent the conjunction of their
respective outcomes, \((e_1 \cdot e_2 \cdot \ldots \cdot e_m)\). For
notational convenience we may employ the term \(c^m\) to abbreviate
the conjunction of the \(m\) experimental conditions, and we use the
term \(e^m\) to abbreviate the corresponding conjunction of their
outcomes. Given a specific hypothesis \(h_k\) together with relevant
auxiliaries \(b\), the evidential outcomes of these distinct
experiments or observations will usually be probabilistically
independent of one another, and will also be independent of the
experimental conditions for one another&rsquo;s outcomes. In that case
the likelihood \(P[e \mid h_k \cdot c \cdot b]\) decomposes into the
following terms:</p>
</div> 
\[\begin{align}
&amp;P[e \mid h_k \cdot c \cdot b] = P[e^m \mid h_k \cdot c^m \cdot b] \\
&amp;~ = P[e_1 \mid h_k \cdot c_1 \cdot b] \times P[e_2 \mid h_k \cdot c_2 \cdot b] \times \cdots \times P[e_m \mid h_k \cdot c_m \cdot b].
\end{align}\]

<p class="indent">
Thus, when the likelihoods represent evidence that consists of a
collection of \(m\) distinct probabilistically independent experiments
(or observations) and their respective outcomes, the likelihood ratios
may take the following form:</p> 
\[\begin{align}
&amp;\frac{P[e \mid h_i \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} = \frac{P[e^m \mid h_i \cdot c^m \cdot b]}{P[e^m \mid h_j \cdot c^m \cdot b]} \\
&amp;~ = \frac{P[e_1 \mid h_i \cdot c_1 \cdot b]}{P[e_1 \mid h_j \cdot c_1 \cdot b]} \times \frac{P[e_2 \mid h_i \cdot c_2 \cdot b]}{P[e_2 \mid h_j \cdot c_2 \cdot b]} \times \ldots \times \frac{P[e_m \mid h_i \cdot c_m \cdot b]}{P[e_m \mid h_j \cdot c_m \cdot b]}.
\end{align}\]

<div class="indent">

<p>
<b>2. The <em>prior plausibilities</em> of hypotheses</b>: A <em>prior
probability</em> is a probabilistic argument for or against a
hypothesis of form \(P[h_k \mid b]\) or \(P[h_k \mid c \cdot b]\),
where the information carried by \(b\) or \((c \cdot b)\) does
<i>not</i> contain the kinds of evidential outcomes \(e\) for which
the \(h_k\) expresses likelihoods. These probabilistic arguments need
not be <i>a prior</i> arguments for hypothesis \(h_k\), as some have
suggested. Nor need they merely express the subjective opinions of
individual persons. Rather, the values for these arguments should
represent an assessment of the plausibility of hypotheses based on
a range of relevant considerations, including broadly empirical facts
not captured by evidential likelihoods. For instance, such
plausibility arguments may involve considerations of the
<i>simplicity</i> of the hypothesis, whether it is overly <i>ad
hoc</i>, whether it provides (or is at least consistent with) a
reasonable causal mechanism, etc. Such considerations may be
explicitly stated within statement \(b\). (This view on the nature of
Bayesian probabilities, and especially the prior probabilities, most
closely follows in the tradition of such Bayesians as Keynes,
Jeffreys, and Jaynes. Alternatively, many Bayesians, in the tradition
of Ramsey, de Finetti, and Savage, take all Bayesian probabilities,
including the priors, to express individual subjective degrees of
belief. However, the mathematical rules of the Bayesian logic itself
do not in any way depend on the resolution of this issue regarding
conceptual nature of Bayesian probabilities. So we can set this issue
aside here.)</p>

<p>
In many contexts such initial plausibility assessments will not be
well-represented by precise numerical values. However, it turns out
that the inductive inference rules presented below need only draw on
the values \(u\) for <i>ratios of priors</i>:</p> 
\[ u = P[h_i \mid c \cdot b] / P[h_j \mid c \cdot b].
\]

<p>
These ratios represent how much more (or less) plausible hypothesis
\(h_i\) is taken to be than alternative hypothesis \(h_j\), given
their comparative <i>simplicity</i>, <i>ad hocness</i>, <i>causal
viability</i>, etc., and including whatever broadly empirical factors
are relevant to the specific field of inquiry to which these
hypotheses are relevant. </p>

<p>
Furthermore, such comparative plausibility assessments may often be too
vague to be represented by precise numerical values. Rather, they will
often be best represented by numerical intervals: </p> 
\[ u \ge P[h_i \mid c \cdot b] / P[h_j \mid c \cdot b] \ge v,\]

<p>
for real numbers \(u\) and \(v\).</p>

<p>
One more point. Although the description of the
observational/experimental conditions, embodied by \(c\), will not
usually be relevant to the prior probability values (in the absence of
outcome \(e\)), the probabilistic logic itself doesn&rsquo;t
automatically permit the dismissal of information that may be
contained in \(c\). Rather, the logic requires that the relevance of
\(c\) be specifically addressed. However, if absent outcome \(e\),
conditions \(c\) are equally relevant to \(h_i\) and \(h_j\), then the
probabilistic logic permits \(c\) to be dropped, yielding comparative
plausibility ratios of the following form:</p> 
\[
u \ge P[h_i \mid b] / P[h_j \mid b] = P[h_i \mid c \cdot b] / P[h_j \mid c \cdot b] \ge v.
\]

<p>
So, although the rules for inductive inferences described below will
continue to include statements \(c\) within the prior probability
arguments, the reader should keep in mind that \(c\) is usually not
relevant to these arguments, and can be dropped from them.</p>
</div>

<p>
The logic of evidential support combines the numerical values of these
two kinds of factors to produce an assessment of the degree of
support, \(P[h_k \mid c \cdot e \cdot b]\), for hypotheses. To see how
this works, first return to following form of Bayes&rsquo; Theorem,
applied to each hypothesis \(h_k\): 
\[P[h_k \mid c \cdot e \cdot b] = \frac{P[e \mid h_k \cdot c \cdot b] \times P[h_k \mid c \cdot b]}{P[e \mid c \cdot b]}.\]
 The value of the term
\(P[e \mid c \cdot b]\), which occurs in the denominator of this form
of Bayes&rsquo; Theorem, is usually difficult (even impossible) to
assess. So it is generally more useful to consider the <i>comparative
support</i> of pairs of competing hypotheses by the evidence. Applying
Bayes&rsquo; Theorem to each of a pair of hypotheses, \(h_i\) and
\(h_j\), and then taking their ratio, produces the following formula
for assessing their comparative support, via the ratio of their
posterior probabilities: 
\[\frac{P[h_i \mid c \cdot e \cdot b]}{P[h_j \mid c \cdot e \cdot b]} = \frac{P[e \mid h_i \cdot c \cdot b] \times P[h_i \mid c \cdot b]}{P[e \mid h_j \cdot c \cdot b] \times P[h_j \mid c \cdot b]}.\]
 The following two sections
explicate this Ratio Form of Bayes&rsquo; Theorem, and show how it
captures the essential features of Bayesian inductive inference. </p>

<h3 id="Rule-RB">1.4 Inference Rule <em>RB</em>: the Ratio Form of Bayes&rsquo; Theorem</h3>

<p>
In this section and the next we look at two closely related versions
of Bayes&rsquo; Theorem as it applies to competing hypotheses. The
present section is devoted to the most elementary version, the
<em>Ratio Form of Bayes&rsquo; Theorem</em>. Here it is.</p>

<div class="indent" id="RB">

<p>
<strong>Rule RB: Ratio Form of Bayes&rsquo; Theorem</strong></p>

<p>
Let \(h_1\), \(h_2\), &hellip;, be a list of two or more alternative
hypotheses, <i>alternatives</i> in the sense that the conjunction of
any two of them, \((h_i \cdot h_j)\), is logically inconsistent (i.e.
no two of them can both be true): \(\vDash \neg (h_i \cdot h_j)\). Let
\(c\) be observational or experimental conditions for which \(e\) is
among the possible outcomes. And suppose \(b\) is a conjunction of
relevant auxiliary hypotheses and plausibility considerations.</p>

<p>
Let \(h_j\) be any hypothesis from the list for which both \(P[e \mid
h_j \cdot c \cdot b] > 0\) and \(P[h_j \mid c \cdot b] > 0\).</p>

<blockquote>
Then \(P[h_j \mid c \cdot e \cdot b] > 0\), <i>and</i> for each
\(h_i\) among the alternatives to \(h_j\),
</blockquote> 
\[ 
\frac{P[h_i \mid c \cdot e \cdot b]}{P[h_j \mid c \cdot e \cdot b]} 
  = 
   \frac{P[e \mid h_i \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} 
  \times
   \frac{P[h_i \mid c \cdot b]}{P[h_j \mid c \cdot b]}.
\]

<blockquote>
This ratio also provides an upper bound on \(P[h_i \mid c \cdot e
\cdot b]\), since
</blockquote> 
\[
P[h_i \mid c \cdot e \cdot b] \le \frac{P[h_i \mid c \cdot e \cdot b]}{P[h_j \mid c \cdot e \cdot b]}.
\]

</div>

<p>
This <i>Ratio Form of Bayes&rsquo; Theorem</i> is straightforwardly
derivable from the above axioms for conditional probability
functions.</p>

<p>
In any application of <em>Rule RB</em>, the <em>likelihood ratios</em>
carry the full import of the evidence \((c \cdot e)\). The evidence
influences the evaluation of hypotheses in no other way. In many
scientific contexts, each hypothesis (together with auxiliaries)
provides a precise value for the likelihoods of evidence claims. In
such cases the exact values for <i>likelihood ratios</i> can be
calculated. Indeed, in any given epistemic context, <em>RB</em> is
useful as a <i>rule of inference</i> for inductive logic only if, for
each pair of hypothesis \(h_i\) and \(h_j\) in the context, the values
of (or at least reasonable bounds on) their <i>likelihood ratios</i>
are determinable or calculable.</p>

<p>
In <em>Rule RB</em>, the only other factor that influences the value
of the <i>ratio of posterior probabilities</i> is the ratio of their
associated prior probabilities. And these <i>ratios of priors</i> play
a central role. So, for <em>Rule RB</em> to be useful as a rule of
inference for inductive logic, the values of these <i>ratios of
priors</i> must be estimable or calculable &mdash; or, at least
credible upper and lower bounds on them must be assessable.</p>

<p>
For some kinds of hypotheses, reasonably precise values for the
individual prior probabilities may be available, so the numerical
value for the <i>ratio of priors</i> may be calculated. However, in
many epistemic contexts the prior probability values for individual
hypotheses are vague and difficult to determine. In these contexts it
will often be easier to assess the <i>ratio of priors</i> directly,
since it represents an assessment of how much more (or less) plausible
one hypothesis is than another. Indeed, an assessment of credible
upper and lower bounds on <i>comparative plausibilities</i> suffices
for the kinds of inductive inferences supplied by <em>Rule RB</em>.
For, given a significant body of evidence, the associated
<i>likelihood ratios</i> applied to wide bounds on the <i>comparative
prior plausibilities</i> will often produce quite narrow bounds on the
resulting ratios of <i>posterior probabilities</i>. </p>

<p>
Notice that <em>Rule RB</em> implies that if either \(P[e \mid h_i
\cdot c \cdot b] = 0\) or \(P[h_i \mid c \cdot b] = 0\), then \(P[h_i
\mid c \cdot e \cdot b] = 0\).</p>

<p>
When \(P[h_i \mid c \cdot e \cdot b] = 0\) is due to \(P[e \mid h_i
\cdot c \cdot b] = 0\), we have an extended version of the notion of
the <em>falsification</em> of a hypothesis. <em>Falsification</em> is
usually associated with the deductive refutation of a hypothesis by
evidence. That is, when \((h_i \cdot c \cdot b) \vDash e^*\), but the
actual outcome \(e\) is logically incompatible with \(e^*\), it
follows that \((h_i \cdot c \cdot b) \vDash \neg e\). Then,
deductively, it also follows that \((c \cdot e \cdot b) \vDash \neg
h_i\), and \(h_i\) is said to be <em>falsified</em> by \((c \cdot
e)\), given \(b\).</p>

<p>
<em>Rule RB</em> captures this idea, since when \((h_i \cdot c \cdot
b) \vDash \neg e\), probability theory yields \(P[\neg e \mid h_i
\cdot c \cdot b] = 1\), so \(P[e \mid h_i \cdot c \cdot b] = 0\), in
which case rule <em>RB</em> yields \(P[h_i \mid c \cdot e \cdot b] =
0\). And, according to <em>RB</em>, \(P[e \mid h_i \cdot c \cdot b] =
0\) suffices for \(P[h_i \mid c \cdot e \cdot b] = 0\), from which it
follows that \(P[\neg h_i \mid c \cdot e \cdot b] = 1\).</p>

<p>
<em>Rule RB</em> goes further by showing how evidence may come to
<i>strongly refute</i> a hypothesis \(h_i\), without fully falsifying
it. Suppose now that both \(P[h_j \mid c \cdot b] > 0\) and \(P[h_i
\mid c \cdot b] > 0\). Then, regardless of how plausible or
implausible \(h_i\) is taken to be as compared to \(h_j\), provided
that \(h_j\) isn&rsquo;t <i>way too implausible</i>, if the body of
evidence \(e\) is sufficiently unlikely on \(h_i\) as compared to
\(h_j\), then <em>Rule RB</em> says that the posterior probability of
\(h_i\) on that evidence must also be extremely close to 0.</p>

<p>
More formally, suppose that \(P[h_i \mid c \cdot b] / P[h_j \mid c
\cdot b] \le K\), where \(K\) may be some very large number. This
represents the idea that \(h_i\) is initially considered to be up to
\(K\) times more plausible than \(h_j\). Let \(\epsilon\) be some
extremely small number, as close to 0 as you wish. Then, according to
<em>Rule RB</em>, to get the value of \(P[h_i \mid c \cdot e \cdot
b]\) within \(\epsilon\) of 0, it suffices for the body of evidence to
favor \(h_j\) over \(h_i\) strongly enough that \(P[e \mid h_i \cdot c
\cdot b] \lt (\epsilon / K) \times P[e \mid h_j \cdot c \cdot b]\).
That is, via <i>Rule RB</i>:</p> 
\[\begin{align}
&amp;\text{When }~ \frac{P[h_i \mid c \cdot b]}{P[h_j \mid c \cdot b]} \le K,
~\text{ if }~ \frac{P[e \mid h_i \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} \lt \frac{\epsilon}{K}, \\
&amp;\text{then }~ P[h_i \mid c \cdot e \cdot b] \lt \epsilon.
\end{align}\]

<p>
If all but the most extremely implausible alternatives to hypothesis
\(h_j\) become <i>strongly refuted</i> in this way by a body of
evidence \((c \cdot e)\), then the posterior probability of \(h_j\),
\(P[h_j \mid c \cdot e \cdot b]\), should approach 1. Thus, may
\(h_j\) become strongly supported by the evidence. The next rule will
endorse this idea more fully.</p>

<h3 id="Rule-OB">1.5 Inference Rule <em>OB</em>: the Odds Form of Bayes&rsquo; Theorem</h3>

<p>
<em>Rule RB</em> contributes to a more comprehensive inference rule,
one that applies to collections of competing hypotheses. This more
comprehensive rule employs the well-known probabilistic concept of
<i>odds</i>. By definition, the <i>odds of \(A\) given \(B\)</i>,
written \(\Omega[A \mid B]\), is related to the <i>probability of
\(A\) given \(B\)</i> by the formula: 
\[\Omega[A \mid B] = \frac{P[A \mid B]}{P[\neg A \mid B]}.\]
 However, for our
purposes it will be more useful to employ the inverse ratio of the
<i>odds</i>, the <i>odds against \(A\) given \(B\)</i>: 
\[\Omega[\neg A \mid B] = \frac{P[\neg A \mid B]}{P[A \mid B]} = \frac{1 - P[A \mid B]}{P[A \mid B]}.\]

From the definition of <i>odds against</i>, it follows that:

\[P[A \mid B] = \frac{1}{1 + \Omega[\neg A \mid B]}.\]
 </p>

<p>
Here is how odds comes into play in Bayesian inductive logic. Sum the
ratio versions of Bayes&rsquo; Theorem, as given by <em>Rule RB</em>,
over a range of alternatives to hypothesis \(h_j\). This yields the
<em>Odds Form of Bayes&rsquo; Theorem</em>. And from that we can
calculate the individual values of posterior probabilities.</p>

<div class="indent" id="OB">

<p>
<strong>Rule OB: Odds Form of Bayes&rsquo; Theorem</strong></p>

<p>
Let \(H\) = {\(h_1\), \(h_2\), &hellip;, \(h_n\)} be a collection of
two or more alternative hypotheses (i.e. \(n \ge 2\)), where the
conjunction of any two of them is logically inconsistent, \(\vDash
\neg (h_i \cdot h_j)\). Let \(c\) be observational or experimental
conditions for which \(e\) is among the possible outcomes. And suppose
\(b\) is a conjunction of relevant auxiliary hypotheses and
plausibility considerations.</p>

<p>
Let \(h_j\) be any hypothesis from the list for which both \(P[h_j
\mid c \cdot b] > 0\) and \(P[e \mid h_j \cdot c \cdot b] > 0\). </p>

<p class="indent">
Then \(P[h_j \mid c \cdot e \cdot b] > 0\) <i>and</i> for each \(h_i\)
an alternative to \(h_j\),</p> 
\[\begin{align}
\Omega[\neg h_j \mid  c \cdot e \cdot b \cdot (h_i \vee h_j)]  &amp;=
\frac{P[h_i \mid c \cdot e \cdot b]}{P[h_j \mid c \cdot e \cdot b]} \\
 &amp;= \frac{P[e \mid h_i \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} 
  \times \frac{P[h_i \mid c \cdot b]}{P[h_j \mid c \cdot b]}.
\end{align}\]

<p class="indent">
Furthermore,</p> 
\[\begin{align}
\Omega[\neg h_j \mid&amp;  c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)]  \\
 &amp;= \sum_{i = 1, i \ne j}^n \Omega[\neg h_j \mid  c \cdot e \cdot b \cdot (h_i \vee h_j)]  \\
 &amp;= \sum_{i = 1, i \ne j}^n  \frac{P[e \mid h_i \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} 
  \times \frac{P[h_i \mid c \cdot b]}{P[h_j \mid c \cdot b]}.
\end{align}\]

<p class="indent">
Finally, the associated posterior probability of \(h_j\), the degree
to which premise \((c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots
\vee h_n))\) supports conclusion \(h_j\), is given by the formula</p>

\[\begin{align}
&amp;P[h_j \mid c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)] \\
&amp;\quad = \frac{1}{1 + \Omega[\neg h_j \mid  c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)]}.
\end{align}\]

</div>

<p>
Thus, <em>Rule OB</em> shows that the <i>odds against a
hypothesis</i>, assessed against a finite collection of alternatives,
depends only on the values of <em>ratios of posterior
probabilities</em>, where each of these ratios entirely derives from
the <em>Ratio Form of Bayes&rsquo; Theorem</em>, stated by <em>Rule
RB</em>. The same goes for the <i>posterior probability of a
hypothesis</i>, since its value entirely derives from the odds against
it. Thus, the <i>Ratio Form of Bayes&rsquo; Theorem</i> captures the
essential features of the Bayesian evaluation of hypotheses. It shows
how the impact of evidence, captured by <i>likelihood ratios</i>,
combine with comparative plausibility assessments of hypotheses,
captured by <i>ratios of prior probabilities</i>, to provide a net
assessment of the extent to which hypotheses are refuted or supported
in a contest with their rivals.</p>

<p>
We conclude this section with a comment about why the posterior odds
and posterior probabilities provided by <em>Rule OB</em> usually need
to be relativised to finite disjunctions of alternative hypotheses,
\((h_1 \vee h_2 \vee \ldots \vee h_n)\).</p>

<p>
First notice that in any specific epistemic context where the
collection of \(n\) alternative hypotheses, \(\{h_1, h_2, \ldots,
h_n\},\) consists of <i>all possible</i> alternatives about the
subject matter at issue, and if background statement \(b\) says so
(i.e. if \(b \vDash (h_1 \vee h_2 \vee \ldots \vee h_n)\)), then the
explicit use of disjunctions of hypotheses can be dropped from the
equations in <em>Rule OB</em>. For, in that context,</p> 
\[\Omega[\neg h_j \mid c \cdot e \cdot b] = \Omega[\neg h_j \mid c
\cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)].
\]

<p>
However, in many epistemic contexts an investigator may not be aware
of <i>all possible</i> alternative hypotheses or theories about the
subject at issue. For instance, the medical community may not have
identified every possible disorder or disease that may afflict a
patient. Furthermore, in some contexts it may not even be possible to
formulate <i>all possible</i> alternative hypotheses or theories
&mdash; e.g. all possible alternative theories about the fundamental
nature of space-time and the origin of the universe. In such cases,
the best we can do is evaluate evidential support for (and against)
those hypotheses we&rsquo;ve formulated thus far, always keeping in
mind that the list of alternatives might well be expanded to
additional alternatives.</p>

<p>
Now, just one further point. Suppose that the list of \(n\)
alternatives contains all alternative hypotheses that the relevant
epistemic community has formulated so far, but other unidentified
alternatives remain possible. Can we not appeal to the following
Bayesian result to bypass the need to relativise to the disjunction of
presently formulated alternative hypotheses? After all, this result is
also a theorem of probability theory.</p>

<p>
For \(P[e \mid h_j \cdot c \cdot b] > 0\) and \(P[h_j \mid c \cdot
e\cdot b] > 0\), </p>

<div class="smaller">

\[\begin{align}
&amp;\Omega[\neg h_j \mid  c \cdot e \cdot b] \\
&amp;~ = \sum_{i = 1, i \ne j}^n 
 \frac{P[h_i \mid  c \cdot e \cdot b]}{P[h_j \mid  c \cdot e\cdot b]} + 
 \frac{P[(\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \mid  c \cdot e \cdot b]}{P[h_j \mid  c \cdot e\cdot b]} \\
&amp;~ = \Omega[\neg h_j \mid  c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)] 
 + \frac{P[(\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \mid  c \cdot e \cdot b]}
{P[h_j \mid  c \cdot e\cdot b]},
\end{align}\]

</div>

<p class="indent">
where the final term is given by the equation,</p>

<div class="smaller">

\[\begin{align}
&amp;\frac{P[(\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \mid  c \cdot e \cdot b]}{P[h_j \mid  c \cdot e\cdot b]} \\
&amp;\quad=
\frac{P[e \mid (\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \cdot c \cdot b]}{P[e \mid h_j \cdot c \cdot b]} 
  \times \frac{P[(\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \mid c \cdot b]}{P[h_j \mid c \cdot b]}.
\end{align}\]

</div>

<p>
The problem with this idea is that it draws on likelihoods of form
\(P[e \mid (\neg h_1 \cdot \neg h_2 \cdot \ldots \cdot \neg h_n) \cdot
c \cdot b]\). Such likelihoods will almost never have explicitly
determinable or calculable values. So, the values of \(\Omega[\neg h_j
\mid c \cdot e \cdot b]\) and \(P[h_j \mid c \cdot e \cdot b]\) that
derive from formulas that draw on this kind of likelihood must also
fail to be determinable or calculable. So, this approach to
sidestepping the relativization to \((h_1 \vee h_2 \vee \ldots \vee
h_n)\) is at cross-purposes with the idea that an inductive logic
should be couched in terms of <i>usable rules</i> of inductive
inference.</p>

<p>
Nevertheless, the calculable values of \(\Omega[\neg h_j \mid c \cdot
e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)]\) provided by
<em>Rule OB</em> do entail explicit <i>bounds</i> on the values for
the non-disjunctively-relativized posterior odds and posterior
probabilities. For, the probabilistic logic entails the following
relationships:</p> 
\[\Omega[\neg h_j \mid c \cdot e \cdot b] \ge \Omega[\neg h_j \mid c
\cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots \vee h_n)],\]

<p>
and so</p> 
\[P[h_j \mid c \cdot e \cdot b] \le P[h_j \mid c \cdot e \cdot b \cdot
(h_1 \vee h_2 \vee \ldots \vee h_n)].\]

<p>
Thus, if the evidence pushes \(P[h_j \mid c \cdot e \cdot b \cdot (h_1
\vee h_2 \vee \ldots \vee h_n)]\) close to 0, then it also must push
\(P[h_j \mid c \cdot e \cdot b]\) close to 0. However, although
pushing \(P[h_i \mid c \cdot e \cdot b \cdot (h_1 \vee h_2 \vee \ldots
\vee h_n)]\) close to 0 for all \((n-1)\) competitors of \(h_j\)
results in the approach of \(P[h_j \mid c \cdot e \cdot b \cdot (h_1
\vee h_2 \vee \ldots \vee h_n)]\) to 1, it need not result in the the
approach of the non-disjunctively-relativized posterior \(P[h_j \mid c
\cdot e \cdot b]\) to 1. For, some as yet unconsidered alternative
hypothesis may well be able to do better than \(h_j\) on the currently
available evidence \((c \cdot e \cdot b)\). The logic of Bayesian
inference does not rule out this possibility. </p>

<h3 id="Estimation">1.6 Inference Rules for Bayesian Interval Estimation</h3>

<p>
This section specifies two additional inference rules for Bayesian
inductive logic. They are specialized versions of Bayes&rsquo; Theorem
&mdash; basically extended versions of rule <i>OB</i>. These two rules
are especially useful in cases of interval estimation, where the
evidence bears on whether the true hypothesis lies within some
specific interval of alternative claims. The first of these two rules
will be stated in terms of evidential support for disjunctions of
hypotheses. The precise statement of this rule does not presuppose
that the hypotheses it addresses lie within some interval of values;
rather, it applies to the support for any finite disjunction of
hypotheses. However, one of its important applications is to the
evidential support of a <i>disjunctive interval</i> of alternative
hypotheses. An example application to a disjunctive interval of
alternative hypotheses is provided in Section 2.4.</p>

<p>
The second rule applies to the support of competing hypotheses that
range over continuous intervals of real numbers. For example, consider
each hypothesis of form, &ldquo;the chance of <i>heads</i> on tosses
of this particular (possibly biased) coin is \(r\)&rdquo;, where \(r\)
must have some real number value between 0 and 1. Perhaps the true
value of \(r\) for this particular coin is .72. However, the evidence
won&rsquo;t usually single out this exact chance hypothesis. Rather,
the best we can usually do is use evidence to narrow down the interval
within which the true value of \(r\) very probably resides (e.g. show
that the posterior probability that \(r\) lies between .67 and .77 is
.95, based on the evidence). The statement of this second interval
estimation rule will closely resemble the statement of the first rule,
but modifies it to apply to continuous intervals of values. An example
is provided in Section 2.5. </p>

<h4 id="Estimation-1">1.6.1 Inference Rule <em>BE-D</em>: Bayesian Estimation for Disjunctions of Hypotheses</h4>

<p>
The following rule provides lower bounds on the posterior probability
of disjunctions of alternative hypotheses. It derives from the above
axioms for conditional probabilities, with no additional suppositions
beyond those explicitly stated in the rule itself. Although the
statement of this rules is quite general, its most common application
is to disjunctions of hypotheses about closely spaced numerical
quantities. </p>

<div class="indent" id="BE-D">

<p>
<strong>Rule BE-D: Bayesian Estimation for Disjunctions of Alternative
Hypotheses</strong></p>

<p>
Let \(H\) be a collection of \(z\) alternative hypotheses, \(z \ge
2\), where the conjunction of any two of them is logically
inconsistent. Let \(c\) be observational or experimental conditions
for which \(e\) describes one of the possible outcomes. And suppose
\(b\) is a conjunction of relevant auxiliary hypotheses and
plausibility considerations. For each hypothesis \(h_i\) in \(H\), let
its prior probability be non-zero: \(P[h_i \mid c \cdot b] \gt
0\).</p>

<p>
Choose any \(k\) hypotheses from collection \(H\), where each one of
them, \(h_i\), has a likelihood value \(P[e \mid h_i \cdot c \cdot b]
> 0\). Label these \(k\) hypotheses (in whatever order you wish) as
\(\lsq h_1\rsq\), \(\lsq h_2\rsq\), \(\ldots\), \(\lsq h_k\rsq\). Then
label all the remaining hypotheses in \(H\) (in whatever order you
wish) as \(\lsq h_{k+1}\rsq\), \(\lsq h_{k+2}\rsq\), \(\ldots\),
\(\lsq h_z\rsq\).</p>

<p>
Given these labelings of hypotheses in \(H\), let \((h_1 \vee \ldots
\vee h_k)\) represent the disjunction of the first \(k\) hypotheses
chosen from \(H\), and \((h_{k+1} \vee \ldots \vee h_z)\) represent
the disjunction of the remaining hypotheses from \(H\). The expression
\((h_1 \vee \ldots \vee h_z)\) represents the disjunction of all
hypotheses in \(H\). Furthermore, let&rsquo;s take \(b\) to logically
entail that one of the hypotheses in \(H\) is true &mdash; i.e. \(b\)
logically entails the disjunction of all alternative hypotheses in
\(H\): \(b \vDash (h_1 \vee \ldots \vee h_z)\). So, both \(P[(h_1 \vee
\ldots \vee h_z) \mid c \cdot b] = 1\) and \(P[(h_1 \vee \ldots \vee
h_z) \mid c \cdot e \cdot b] = 1\). </p>

<p>
Then, the posterior probability of \((h_1 \vee \ldots \vee h_k)\)
satisfies the following form of Bayes&rsquo; Theorem: </p>
</div> 
\[
P[(h_1 \vee \ldots \vee h_k) \mid  c \cdot e \cdot b] \; \; = \; \;
\frac{\sum_{j = 1}^k P[e \mid h_j \cdot c \cdot b] \times P[h_j \mid c \cdot b]}{\sum_{i = 1}^z P[e \mid h_i \cdot c \cdot b] \times P[h_i \mid c \cdot b]}.
\]

<div class="indent">

<p>
In cases where the values of all the prior probabilities, \(P[h_i \mid
c \cdot b]\), are known, or can be closely approximated, this equation
suffices to provide values for the argument strengths \(r\) of the
posterior probabilities, \(P[(h_1 \vee \ldots \vee h_k) \mid c \cdot e
\cdot b] = r\). But when no precise values of the priors are
available, a useful estimate of bounds on the posterior probabilities
may be derived as follows. </p>

<p>
Let \(K\) be (your best estimate of) an upper bound on the ratios of
prior probabilities, \(P[h_i \mid c \cdot b] / P[h_j \mid c \cdot b]\)
for all \(h_j\) in \(\{h_1, h_2, \ldots, h_k\}\) and all \(h_i\) in
\(\{h_{k+1}, h_{k+2}, \ldots, h_z\}\). That is, for whichever \(h_j\)
in \(\{h_1, h_2, \ldots, h_k\}\) has the smallest value of \(P[h_j
\mid c \cdot b]\), and for whichever \(h_i\) in \(\{h_{k+1}, h_{k+2},
\ldots, h_z\}\) has the largest value of \(P[h_i \mid c \cdot b]\),
let \(K\) be a real number that is large enough that \(K \ge P[h_i
\mid c \cdot b] / P[h_j \mid c \cdot b]\). </p>

<p>
Then, 
\[
\Omega[\neg (h_1 \vee \ldots \vee h_k) \mid  c \cdot e \cdot b] \; \; \le \; \;
 
K \times \left[\frac{1}{\frac{\sum_{j = 1}^k P[e \; \mid \; h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \; \mid \; h_i \cdot c \cdot b]}} - 1 \right].
\]
 </p>

<p>
Thus, a lower bound on the associated posterior probability of \((h_1
\vee \ldots \vee h_k)\) is given by the formula 
\[
P[(h_1 \vee \ldots \vee h_k) \mid c \cdot e \cdot b] \; \; \ge \; \;

\frac{1}{1 + K \times \left[\frac{1}{\frac{\sum_{j = 1}^k P[e \; \mid \; h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \; \mid \; h_i \cdot c \cdot b]}} - 1 \right]}.
\]
 </p>
</div>

<p>
A few points about this rule are worth noting. First, notice that the
term \(\sum_{j = 1}^k P[e \mid h_j \cdot c \cdot b] / \sum_{i = 1}^z
P[e \mid h_i \cdot c \cdot b]\) is the ratio of the sum of the first
\(k\) likelihoods to the sum of all the likelihoods for hypotheses in
\(H\). So, although this rule applies to any collection \(H\)
consisting of \(z\) alternative hypotheses, it is most usefully
applied when each hypothesis \(h_j\) contained in the disjunction
\((h_1 \vee h_2 \vee \ldots \vee h_k)\) has a greater likelihood
value, \(P[e \mid h_j \cdot c \cdot b]\), than any of the other
hypotheses in \(H\). This is usually the most interesting case in
which a lower bound on the posterior probability, \(P[(h_1 \vee \ldots
\vee h_k) \mid c \cdot e \cdot b]\), is assessed. For, when these
\(k\) likelihoods yield a sum much greater than likelihoods for the
other hypotheses in \(H\), then this ratio term may approach 1, which
in turn drives the lower bound on the posterior probability, \(P[(h_1
\vee \ldots \vee h_k) \mid c \cdot e \cdot b]\), close to 1. We will
see how this can happen in an example in Section 2.4.</p>

<p>
Notice that when all the prior probabilities are equal, the value of
\(K\) will be 1. In that case the final formula can be replaced by the
equality, 
\[
P[(h_1 \vee \ldots \vee h_k) \mid c \cdot e \cdot b] \; \; = \; \;
\frac{\sum_{j = 1}^k P[e \mid h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \mid h_i \cdot c \cdot b]}.
\]
 </p>

<p>
When each of the prior probabilities for the first \(k\) hypotheses is
at least as large as any of the prior probabilities for the remaining
\(z-k\) hypotheses, the value of \(K\) must be less than or equal to
1. In that case, the following version of the final formula holds:

\[\begin{align}
P[(h_1 \vee \ldots \vee h_k) \mid c \cdot e \cdot b] &amp;\ge
\frac{1}{1 + K \times \left[\frac{1}{\frac{\sum_{j = 1}^k P[e \; \mid \; h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \; \mid \; h_i \cdot c \cdot b]}} - 1 \right]} \\
&amp;\ge 
\frac{\sum_{j = 1}^k P[e \mid h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \mid h_i \cdot c \cdot b]}.
\end{align}\]
 </p>

<p>
Derivations of the two Bayesian Estimation Rules, <i>Rule BE-D</i>,
and <i>Rule BE-C</i> (which will be described in the next subsection)
are provided in the following appendix:
 <a href="appendix3.html">Derivations of the Two Bayesian Estimation Rules, <i>Rule BE-D</i> and <i>Rule BE-C</i>.</a>
 </p>

<h4 id="Estimation-2">1.6.2 Inference Rule <em>BE-C</em>: Bayesian Estimation for a Continuous Range of Alternative Hypotheses</h4>

<p>
A rule similar to <i>BE-D</i> applies to a continuous range of
competing hypotheses. For example, the claim that &ldquo;the chance
<i>r</i> of <i>heads</i> on tosses of this coin lies between .63 and
point .81&rdquo; consists of a continuous (disjunctive) interval of
competing hypotheses. So,the statement of the following rule closely
parallels the statement of <i>Rule BE-D</i>. An example of its
application is provided in Section 2.5. </p>

<div class="indent" id="BE-C">

<p>
<strong>Rule BE-C: Bayesian Estimation for a Continuous Range of
Alternative Hypotheses</strong></p>

<p>
Let \(H\) be a continuous region of alternative hypotheses \(h_q\),
where \(q\) is a real number, and where the conjunction of any two of
these hypotheses is logically inconsistent. Let \(c\) be observational
or experimental conditions for which \(e\) describes one of the
possible outcomes. And suppose \(b\) is a conjunction of relevant
auxiliary hypotheses and plausibility considerations. For each point
hypothesis \(h_q\) in \(H\), we take \(p[e \mid h_q \cdot c \cdot b]\)
to be an appropriate likelihood.</p>

<p>
Let \(p[h_q \mid c \cdot b]\) and \(p[h_q \mid c \cdot e \cdot b]\) be
probability density functions on \(H\), where these two density
functions are related as follows: 
\[p[h_q \mid c \cdot e \cdot b] \times P[e \mid c \cdot b] \;=\; p[e \mid h_q \cdot c \cdot b] \times p[h_q \mid c \cdot b].\]
 </p>

<p>
We suppose throughout that prior probability density \(p[h_q \mid c
\cdot b] > 0\) for all values of \(q\). </p>

<p>
The prior probability that the true point hypothesis \(h_r\) lies
within measurable region \(R\) is given by</p>

<p class="center">
\(P[h_R \mid c \cdot b] \; = \; \int_R p[h_r \mid c \cdot b] \;
dr,\;\;\) where \(\; P[h_H \mid c \cdot b] \; = \; \int_H p[h_q \mid c
\cdot b] \; dq \: =\: 1\).</p>

<p>
The posterior probability that the true point hypothesis \(h_r\) lies
within measurable region \(R\) is given by</p>

<p class="center">
\(P[h_R \mid c \cdot e \cdot b] \; = \; \int_R p[h_r \mid c \cdot e
\cdot b] \; dr, \;\;\) where \(\;P[h_H \mid c \cdot e \cdot b] \; = \;
\int_H p[h_q \mid c \cdot e \cdot b] \; dq \: =\: 1\).</p>

<p>
Then, the posterior probability satisfies the following equation for
each measurable region \(R\): 
\[\begin{align}
P[h_R \mid c \cdot e \cdot b] &amp;= \frac{\int_R p[e \mid h_r \cdot c \cdot b] \times p[h_r \mid c \cdot b]  \; \; dr}{\int_H p[e \mid h_q \cdot c \cdot b] \times p[h_q \mid c \cdot b]  \; \; dq}.
\end{align}\]
 </p>

<p>
In cases where a precise model of the prior probability density,
\(p[h_q \mid c \cdot b]\), is available, this equation suffices to
provide values for the posterior probabilities, \(P[h_R \mid c \cdot e
\cdot b]\). However, when no precise model of the priors is available,
bounds on the values of posterior probabilities may be evaluated in
the following way. </p>

<p>
Let \(K\) be (your best estimate of) an upper bound on the ratios of
the probability density values, \(p[h_q \mid c \cdot b] / p[h_r \mid c
\cdot b]\), for each \(h_r\) in region \(R\) and \(h_q\) in \((H-R)\).
That is, for whichever \(h_r\) in \(R\) has the smallest value of
\(p[h_r \mid c \cdot b]\), and for whichever \(h_q\) in \((H-R)\) has
the largest value of \(p[h_q \mid c \cdot b]\), let \(K\) be a real
number such that \(K \ge p[h_q \mid c \cdot b] / p[h_r \mid c \cdot
b]\). </p>

<p>
Then, 
\[\begin{align}
\Omega[\neg h_R \mid  c \cdot e \cdot b] &amp; \; \le \;
 
K \times \left[\frac{1}{\frac{\int_{R} \; p[e \:\mid\; h_r \cdot c \cdot b]  \; \; dr}{\int_{H} \; p[e \;\mid\; h_q \cdot c \cdot b]  \; \; dq}} - 1 \right].
\end{align}\]
 Thus, a lower bound on the associated posterior
probability of \(h_R\) is given by the formula 
\[
P[h_R \mid c \cdot e \cdot b] \; \;  \ge \; \;
\frac{1}{1 + K \times \left[\frac{1}{\frac{\int_{R} \; p[e \;\mid\; h_r \cdot c \cdot b] \; \; dr}{\int_{H} \; p[e \;\mid\; h_q \cdot c \cdot b] \; \; dq}} - 1 \right]}.
\]
 </p>
</div>

<p>
In Bayesian statistics, interval hypotheses of this kind on which
posterior probabilities are assessed are called <i>credible
intervals</i>. The posterior probabilities of such intervals are
usually calculated from prior probability distributions governed by
explicitly known (or assumed) prior probability density functions.
Often the assumed density function is given by \(p[h_q \mid c \cdot b]
= 1\) over all \(h_q\) in \(H\), in which case the prior is said to
have a flat distribution. When the prior is flat, the value of
\(K=1\), and the precise value of the posterior probability for region
(interval) \(R\) is given by the formula, 
\[P[h_R \mid c \cdot e \cdot b] \; \; = \; \;
\frac{\int_R p[e \mid h_q \cdot c \cdot b] \; \; dr}{\int_H p[e \mid h_q \cdot c \cdot b]  \; \; dq}.\]
 </p>

<p>
<i>Rule BE-C</i> is closely related to the Bayesian <i>Principle of
Stable Estimation</i> (Edwards, Lindman, Savage, 1963), but somewhat
simpler and easier to apply. An example of its application is supplied
in Section 2.5. </p>

<h3 id="Auxiliaries">1.7 On the Epistemic Status of Auxiliary Hypotheses</h3>

<p>
As already noted, the logical connection between hypotheses and the
evidence expressed by the <i>likelihoods</i> often requires the
mediation of auxiliary hypotheses. When competing hypotheses, \(h_i\)
and \(h_j\) draw on distinct, incompatible auxiliary hypotheses,
\(a_i\) and \(a_j\), respectively, these auxiliaries cannot be
collected into a common background claim \(b\). Rather, they must be
evidentially evaluated along with (in conjunction with) the hypotheses
that draw on them. In that case <i>Rule RB</i> applies as follows:

\[ 
\frac{P[(h_i \cdot a_i) \mid c \cdot e \cdot b]}{P[(h_j \cdot a_j) \mid c \cdot e \cdot b]} 
  = 
   \frac{P[e \mid (h_i \cdot a_i) \cdot c \cdot b]}{P[e \mid (h_j \cdot a_j) \cdot c \cdot b]} 
  \times
   \frac{P[(h_i \cdot a_i) \mid c \cdot b]}{P[(h_j \cdot a_j) \mid c \cdot b]}.
\]
 </p>

<p>
But when two competing hypotheses draw on the same auxiliaries \(a\),
the logic treats them as &ldquo;given&rdquo; with regard to the
comparative support of those hypotheses. To see how the probabilistic
logic endorses this treatment, consider how <i>Rule RB</i> applies to
a pair of hypotheses when each is conjoined to the same auxiliary (or
conjunction of auxiliaries), \(a\). First notice that <i>Rule RB</i>
applies to the comparative support for \((h_i \cdot a)\) verses \((h_j
\cdot a)\) as expressed above. (Here we let \(d\) contain background
and auxiliaries other than \(a\), so that the previous background
claim \(b\) now consists of the conjunction (\(a \cdot d)\)):

\[ 
\frac{P[(h_i \cdot a) \mid c \cdot e \cdot d]}{P[(h_j \cdot a) \mid c \cdot e \cdot d]} 
  = 
   \frac{P[e \mid (h_i \cdot a) \cdot c \cdot d]}{P[e \mid (h_j \cdot a) \cdot c \cdot d]} 
  \times
   \frac{P[(h_i \cdot a) \mid c \cdot d]}{P[(h_j \cdot a) \mid c \cdot d]}.
\]
 </p>

<p>
Consider the following probabilistically valid rule &mdash; Axiom 5 of
the axioms for conditional probabilities: </p> 
\[P[(A \cdot B) \mid C] = P[A \mid B \cdot C] \times P[B \mid C].\]

<p>
Applying this rule to each posterior probability in the previous ratio
of posteriors yields </p> 
\[\begin{align}
\frac{P[(h_i \cdot a) \mid c \cdot e \cdot d]}{P[(h_j \cdot a) \mid c \cdot e \cdot d]} 
 &amp;= \frac{P[h_i \mid a \cdot c \cdot e \cdot d] \times P[a \mid c \cdot e \cdot d]}{P[h_j \mid a \cdot c \cdot e \cdot d] \times P[a \mid c \cdot e \cdot d]} \\
 &amp;= \frac{P[h_i \mid c \cdot e \cdot (a \cdot d)]}{P[h_j \mid c \cdot e \cdot (a \cdot d)]}
\end{align}\]

<p>
Similarly, applying this rule to each prior probability in the
previous ratio of priors yields </p> 
\[
\frac{P[(h_i \cdot a) \mid c \cdot d]}{P[(h_j \cdot a) \mid c \cdot d]} 
  = \frac{P[h_i \mid a \cdot c \cdot d] \times P[a \mid c \cdot d]}{P[h_j \mid a \cdot c \cdot d] \times P[a \mid c \cdot d]} =
\frac{P[h_i \mid c \cdot (a \cdot d)]}{P[h_j \mid c \cdot (a \cdot d)]}.\]

<p>
Now, substituting these equal posterior ratios and equal prior ratios
into the previous version of <i>RB</i> for \((h_i \cdot a)\) and
\((h_i \cdot a)\) yields </p> 
\[ 
\frac{P[h_i \mid c \cdot e \cdot (a \cdot d)]}{P[h_j \mid c \cdot e \cdot (a \cdot d)]} 
  = 
   \frac{P[e \mid h_i \cdot c \cdot (a \cdot d)]}{P[e \mid h_j \cdot c \cdot (a \cdot d)]} 
  \times
   \frac{P[h_i \mid c \cdot (a \cdot d)]}{P[h_j \mid c \cdot (a \cdot d)]}.
\]

<p>
Thus, when auxiliaries \(a\) are employed in common by competing
hypotheses, they may be swept into a common collection of background
claims \(b\) (i.e., becoming \((a \cdot d)\) in this example).</p>

<p>
As with any logic, the logic of inductive support only tells us what a
given collection of premises implies about various conclusions. It may
well happen that auxiliary \(a\) together the body of evidence \((c
\cdot e)\) implies, via likelihood ratios, that hypothesis \(h_j\) is
strongly supported over \(h_i\), 
\[ 
\frac{P[e \mid h_i \cdot c \cdot (a \cdot d)]}{P[e \mid h_j \cdot c \cdot (a \cdot d)]} \ll 1,
\]
 whereas, rival auxiliary
\(a_r\) together with the same body of evidence may tell us, via
likelihood ratios, that \(h_i\) is strongly supported over \(h_j\),

\[ 
\frac{P[e \mid h_i \cdot c \cdot (a_r \cdot d)]}{P[e \mid h_j \cdot c \cdot (a_r \cdot d)]} \gg 1.
\]
 </p>

<p>
This ability to switch between auxiliaries to the benefit of one
hypothesis over another seems epistemically dubious. Does the logic
permit epistemic agents to simply employ whatever auxiliaries may best
help support their own favorite hypotheses?</p>

<p>
No, not exactly. As with any logic, only arguments that have true
premises warrant their conclusions as true, or, for an inductive
logic, as more or less probably true. So, if we can determine which of
the alternative auxiliaries, \(a\) or \(a_r\), is true, then, provided
the body of evidence \((c \cdot e)\) is also true, the problem would
be solved. Our best assessment of which alternative hypothesis,
\(h_j\) or \(h_i\), is most probably true should draw on premises
(evidence and auxiliaries) that are themselves true. But how are we to
determine which auxiliaries are true? By assessing <i>their</i>
probable truth based on the body of evidence for and against
<i>them</i>.</p>

<p>
That is, the auxiliary hypotheses themselves are subject to evidence
that may strongly support (the truth of) one of them over its rivals.
Furthermore, this evidential support for the auxiliaries can, in turn,
impact the support of hypotheses that draw on them. To see how this
happens, consider again the two alternative auxiliaries (or
alternative conjunctions auxiliaries) \(a\) and \(a_r\). Suppose that
a large body of evidence, \((c^* \cdot e^*)\), bears on \(a\) and its
rivals, and that this body of evidence strongly supports \(a\) over
each of them. In particular, suppose that according to <i>Rule RB</i>
this body of evidence supplies very strong support for \(a\) over
rival \(a_r\):</p> 
\[ 
\frac{P[a_r \mid c^* \cdot e^* \cdot d]}{P[a \mid c^* \cdot e^* \cdot d]} 
  = 
   \frac{P[e^* \mid a_r \cdot c^* \cdot d]}{P[e^* \mid a \cdot c^* \cdot d]} 
  \times
   \frac{P[a_r \mid c^* \cdot d]}{P[a \mid c^* \cdot d]} = \epsilon,\]

<p>
for some extremely small value of \(\epsilon\).</p>

<p>
So, according to this body of evidence, \(a\) is much more likely to
be true than \(a_r\). Intuitively, this provides good epistemic reason
to employ \(a\) rather than \(a_r\) as premises in the evaluation of
hypotheses \(h_j\) verses \(h_i\). When the evidence strongly supports
one auxiliary hypothesis over an alternative, it makes good epistemic
sense to draw on the most strongly supported auxiliary. Indeed, the
Bayesian logic can be shown to reinforce this intuition in a sensible
way. The following appendix works through the technical details of a
theorem that establishes this claim.</p>

<p>

 <a href="appendix4.html">An Epistemic Advantage of Drawing on Well-Supported Auxiliary Hypotheses</a>
 </p>

<h2 id="Examples">2. Examples</h2>

<p>
Bayesian inductive logic captures the structure of evidential support
for all sorts of scientific hypotheses, ranging from simple diagnostic
claims (e.g., &ldquo;the patient is infected by the SARS-CoV-2
virus&rdquo;) to complex scientific theories about the fundamental
nature of the world, such as quantum theories and the theory of
relativity. As we&rsquo;ve seen, the logic is essentially comparative.
The evaluation of a hypothesis depends on how strongly evidence
supports it over rival hypotheses. In this section we consider several
applications of this logic to the evidential evaluation of scientific
hypotheses and theories. </p>

<p>
We have seen that comparisons among the <em>posterior
probabilities</em> of hypotheses depend on just two kinds of factors:
(1) the <em>likelihoods</em> of evidential outcomes \(e\) according to
each hypothesis \(h_k\), when conjoined with auxiliaries \(b\) and
evidential initial conditions \(c\), \(P[e \mid h_k\cdot c \cdot b]\);
and (2) the <em>prior probability</em> of each hypotheses, \(P[h_k
\mid c \cdot b]\). The likelihoods capture what a hypothesis
<i>says</i> about how evidential aspects of the world should turn out
(if the hypothesis is true). The prior probabilities represent
assessments of how plausible a hypothesis is assessed to be on grounds
not captured by evidential likelihoods.</p>

<p>
Plausibility assessments of hypotheses and theories always play an
important, legitimate role in the sciences. Plausibility assessments
are often backed by extensive arguments that may draw on forceful
conceptual considerations together with broadly empirical claims not
captured by the evidential likelihoods. Scientists often bring
plausibility arguments to bear in assessing competing views. Although
such arguments are usually far from decisive, they may bring the
scientific community into widely shared agreement with regard to the
<i>im</i>plausibility of some logically possible alternatives. This
seems to be the primary epistemic role of thought experiments.
Consider, for example, the kinds of plausibility arguments that have
been brought to bear on the various interpretations of quantum theory
(e.g., those related to the measurement problem). These arguments go
to the heart of conceptual issues that were central to the original
development of the theory. Many of these issues were first raised by
those scientists who made the greatest contributions to the
development of quantum theory, in their attempts to get a conceptual
hold on the theory and its implications.</p>

<p>
Furthermore, given any body of evidence, it is easy enough to cook up
<i>logically possible</i> alternative hypotheses that completely
account for the evidence. These cooked up, <i>ad hoc</i> hypotheses
may be constructed so as to logically entail all the known evidence,
providing likelihood values equal to 1 for the totality of the
available evidence. Although most of these cooked up hypotheses will
be laughably implausible, and no scientist would give them a moments
notice, the evidential likelihoods are unable to rule them out. Only
plausibility considerations, represented via prior probabilities,
provide a place for the inductive logic to bring such
<i>im</i>plausibility considerations to bear.</p>

<p>
Among those hypotheses that are not laughably implausible, the
contributions of prior plausibility assessments may be substantially
&ldquo;washed out&rdquo; as a sufficiently strong body of evidence
becomes available. Thus, provided the prior probability of a true
hypothesis isn&rsquo;t assessed to be too close to zero, the influence
of the values of the prior probabilities will <em>very probably</em>
fade away as evidence accumulates. Various Bayesian <i>convergence
results</i> establish reasonable conditions for this to occur. So, it
turns out that prior plausibility assessments play their most
important role when the distinguishing evidence represented by the
likelihoods remains weak. Some of the following examples illustrate
this idea.</p>

<h3 id="Stat-Evid">2.1. Testing Scientific Hypotheses with Statistical Evidence</h3>

<p>
Newtonian Gravitation Theory (NGT) accounts for the &ldquo;falling
together&rdquo; of massive bodies in terms of an attractive force
between them, the force of gravity produced by those massive bodies.
According to the General Theory of Relativity (GTR) there is no
gravitational force between bodies as such. Rather, in the vicinity of
massive bodies space-time is curved. That curvature in space-time
causes the distance between massive objects to decrease as they follow
these curved paths through space-time. One result of this difference
between GTR and NGT is that they entail different paths for beams of
light that pass near the surface of the Sun on their way to Earth.</p>

<p>
GTR entails that the light of distant stars that passes very close to
the surface of the Sun is deflected from a straight-line path. This
deflection will make the star, as viewed from Earth, appear to be in a
slightly different location than usual with respect to background
stars whose light does not pass so close to the Sun&rsquo;s surface.
According to GTR, the predicted angle of deflection for a beam passing
near the Sun&rsquo;s surface is 1.75 arcsec (where 1 arcsec is an
angle of 1/3600 of a degree).</p>

<p>
If light has gravitational mass, then Newtonian Gravitation Theory
also entails that the path of a light beam near the Sun&rsquo;s
surface will be deflected. But the predicted gravitational deflection
is only .875 arcsec, half as much as predicted by General Relativity.
On the other hand, if light has no gravitational mass, NGT entails
that it will not be deflected at all by gravity near the Sun&rsquo;s
surface.</p>

<p>
Einstein realized these differences in the predicted paths of light by
GTR vs. NGT. His publication of GTR in 1915 predicted this kind of empirical distinction between GTR and NGT. In order to test this prediction, Arthur Eddington and Andrew
Crommelin lead two separate expeditions to observe the positions of
stars near the edge of the Sun during a solar eclipse in 1919. Their
measurements involved taking photographs of stars that appear near the
Sun&rsquo;s surface during the eclipse, and then measuring their
apparent positions in those photographs as compared to other stars
that appear further away from the Sun&rsquo;s surface. The relative
positions of those same stars were also photographed and measured in
the night sky at another time of year, when the paths of their light
was not influenced by travel near the surface of the Sun.</p>

<p>
The hypotheses being tested by the evidence in this case are not
themselves statistical in nature. However, the evidential likelihoods
turn out to be probabilistic due to statistical error characteristics
of the measuring devices.</p>

<p>
The Eddington group measured a deflection of 1.61 arcsec, with an
error of plus or minus .31 arcsec. The Crommelin group measured a
deflection of 1.98 arcsec, with an error of plus or minus .12 arcsec.
These error terms are due to inaccuracies in the measuring devices,
such as irregularities in the photographic emulsions, and differences
in the cameras and telescopes during the eclipse measurements as
compared to the non-eclipse reference measurements of star positions
at other times (e.g. due to temperature and configuration
changes).</p>

<p>
Let&rsquo;s employ the following abbreviations:</p>

<dl class="sentag indent">
<dt>\(h_G\)</dt>
<dd>the General Theory of Relativity</dd>
<dt>\(h_N\)</dt>
<dd>Newtonian Gravitation Theory together with the hypothesis that
light has gravitational mass</dd>
<dt>\(h_{N_0}\)</dt>
<dd>Newtonian Gravitation Theory together with the hypothesis that
light has no gravitational mass</dd>
<dt>\(c_1\)</dt>
<dd> the conditions under which the Eddington group measurements are
made (type of telescope, camera, photographic plates, whether
conditions, etc.), both for the eclipse measurements and for the
non-eclipse reference measurements; this information includes the
inferred error intervals due to the measurement conditions and the
resulting states of the developed photographic plates: \(\pm .31\)
arcsec</dd>
<dt>\(e_1\)</dt>
<dd>the outcome of the Eddington group measurements; mean measured
deflection among all stars photographed near the Sun&rsquo;s rim =
1.61 arcsec</dd>
<dt>\(c_2\)</dt>
<dd>the conditions under which the Crommelin group measurements are
made (type of telescope, camera, photographic plates, whether
conditions, etc.), both for the eclipse measurements and for the
non-eclipse reference measurements; this information includes the
inferred error intervals due to the measurement conditions and the
resulting states of the developed photographic plates = \(\pm .12\)
arcsec</dd>
<dt>\(e_2\)</dt>
<dd>the outcome of the Crommelin group measurements: mean measured
deflection among all stars photographed near the Sun&rsquo;s rim =
1.98 arcsec</dd>
<dt>\(b\)</dt>
<dd>includes the supposition that measurement errors of the kind
involved in such measurements tend to be approximately <i>normally
distributed</i> about the true value, where the <i>inferred
measurement error</i> approximates the <i>standard deviation</i> of
this <i>normal distribution</i>.</dd>
</dl>

<p>
In cases like this, the statistical error in the measurement outcome
is taken to be normally distributed around the true value of the light
deflection, expressed by the hypothesis. That is, the likelihood of
the evidential outcome \(e\) for a hypothesis \(h_j\), given \(c \cdot
b\), is calculated in terms of how far away, in terms of <i>standard
deviations</i> for a normal distribution, the measured outcome lies
from the value predicted by that hypothesis.</p>

<p>
A well-know spreadsheet program can be used to calculate these values.
It uses the following syntax to calculate the probability value due to
a normal distribution for the region under the normal curve extending
from the left of the curve up to point <i>x</i>, given the <i>mean</i>
of the normal distribution and its standard deviation,
<i>standard_dev</i>:</p> 
\[\text{NORM.DIST}(x, mean, standard\_dev, \textit{TRUE})\]
 where the term \(\textit{TRUE}\)
tells the function to calculate the cumulative distribution up to
\(x\), instead of only calculating the value of the density function
at \(x\). Using this spreadsheet program, the probability of getting a
measured outcome value between \(m-v\) and \(m+v\) is calculated via
the following formula: 
\[\begin{align}
&amp;\text{NORM.DIST}(m+v, mean, standard\_dev, \textit{TRUE}) \\
&amp;\quad - \text{NORM.DIST}(m-v, mean, standard\_dev, \textit{TRUE}).
\end{align}\]

<p>
For the experiment conducted by the Eddington group, the evidence
consists of a measured deflection value of 1.61, accurate to no more
that two decimal places. Thus, the measurement result lies in the
interval between \((1.61-.005)\) and \((1.61+.005)\). This is the
evidential outcome \(e_1\). Thus, the relevant evidential likelihoods
may be calculated as follow: </p> 
\[\begin{align}
&amp;P[e_1 \mid h_G \cdot c_1 \cdot b]\ = \\
&amp;\qquad \text{NORM.DIST}(1.61 + 0.005, 1.75, .31, \textit{TRUE}) \\
&amp;\qquad\quad - \text{NORM.DIST}(1.61 - 0.005, 1.75, .31, \textit{TRUE}) \\
&amp;~=\ 1.16 \times 10^{-2}
\end{align}\]
 
\[\begin{align}
&amp;P[e_1 \mid h_N \cdot c_1 \cdot b] = \\
&amp;\qquad \text{NORM.DIST}(1.61 + 0.005, .875, .31, \textit{TRUE}) \\
&amp;\qquad\quad - \text{NORM.DIST}(1.61 - 0.005, .875, .31, \textit{TRUE}) \\
&amp;= 7.74 \times 10^{-4}
\end{align}\]

\[\begin{align}
&amp;P[e_1 \mid h_{N_0} \cdot c_1 \cdot b] = \\
&amp;\qquad \text{NORM.DIST}(1.61 + 0.005, 0, .31, \textit{TRUE}) \\
&amp;\qquad\quad - \text{NORM.DIST}(1.61 - 0.005, 0, .31, \textit{TRUE}) \\
&amp;= 1.79 \times 10^{-8}.
\end{align}\]

<p>
The likelihoods for the evidence from the Crommelin group, \((c_2
\cdot e_2)\), may be calculated in a similar way. </p>

<p>
The following table provides the likelihoods due to each hypothesis
for each experiment. And it provides the resulting values for the
corresponding likelihood ratios. </p>

<div class="figure">

<table class="centered cellpad-small-dense cell-left all-rules vert-top small avoid-break">
<tr>
<th>\(e_k\)</th>
<th>\(e_1\)</th>
<th>\(e_2\)</th> </tr>
<tr>
  <td> \(P[e_k \mid h_G \cdot c_k \cdot b]\)</td>
  <td>\(1.16 \times 10^{-2}\)</td>
  <td>\(5.30\times 10^{-3}\)</td> </tr>
<tr>
  <td> \(P[e_k \mid h_N \cdot c_k \cdot b]\)</td>
  <td>\(7.74 \times 10^{-4}\)</td>
  <td>\(1.29 \times 10^{-20}\)</td> </tr>
<tr>
  <td>\(P[e_k \mid h_{N_0} \cdot c_k \cdot b]\)</td>
  <td>\(1.79 \times 10^{-8}\)</td>
  <td>\(2.53 \times 10^{-61}\)</td> </tr>
<tr>
  <td> 
\[\frac{P[e_k \mid h_N \cdot c_k \cdot b]}{P[e_k \mid h_G \cdot c_k \cdot b]}\]
 </td>
  <td> 
\[6.67 \times 10^{-2}\]
 </td>
  <td> 
\[2.43 \times 10^{-18}\]
 </td> </tr>
<tr>
  <td> 
\[\frac{P[e_k \mid h_{N_0} \cdot c_k \cdot b]}{P[e_k \mid h_G \cdot c_k \cdot b]}\]
 </td>
  <td> 
\[1.54 \times 10^{-6}\]
 </td>
  <td> 
\[4.77 \times 10^{-59}\]
 </td> </tr>
<tr>
  <td></td> </tr>
<tr>
  <td> 
\[\frac{P[e_k \mid h_G \cdot c_k \cdot b]}{P[e_k \mid h_N \cdot c_k \cdot b]}\]
 </td>
  <td> 
\[1.50 \times 10^{1}\]
 </td>
  <td> 
\[4.11 \times 10^{17}\]
 </td> </tr>
<tr>
  <td> 
\[\frac{P[e_k \mid h_G \cdot c_k \cdot b]}{P[e_k \mid h_{N_0} \cdot c_k \cdot b]}\]
 </td>
  <td> 
\[6.48 \times 10^{5}\]
 </td>
  <td> 
\[2.09 \times 10^{58}\]
 </td> </tr>
</table>

<p class="center">
Table: Likelihoods and Likelihood Ratios</p>
</div>

<p>
Clearly, \((c_1 \cdot e_1)\) provides overwhelming evidence against
\(h_{N_0}\) as compared to \(h_G\), and strong evidence against
\(h_N\) as compared to \(h_G\). And, \((c_2 \cdot e_2)\) also provides
overwhelming evidence against both \(h_{N_0}\) and \(h_N\) as compared
to \(h_G\).</p>

<h3 id="Med-test">2.2. An Application to Medical Tests: Covid-19 Self-Tests</h3>

<p>
As an illustration of how evidential support works in a medical
setting, let&rsquo;s consider the kind of evidence supplied by
over-the-counter COVID-19 self-tests. Let \(h\) be the hypothesis that
the subject of the test <em>has COVID-19 on the day of testing</em>;
the alternative hypothesis, \(\neg h\), says that the subject does not
have COVID-19 on the day of testing. Background/auxiliary conditions
\(b\) state the <i>sensitivity</i> of the test (chance of a positive
test result when disease is present) and the <i>specificity</i> of the
test (chance of a negative test result when disease is not present).
Most home-tests report <i>sensitivity</i> and <i>specificity</i> for
test subjects who are already symptomatic &mdash; i.e. who already
show any of the following symptoms: fever, fatigue, chills, myalgia
(i.e. muscle pain), congestion, cough, loss of smell, shortness of
breath, sore throat, nausea, diarrhea. In addition, a home-test is
&ldquo;administered appropriately&rdquo; when the nasal swab is used
as the test instructions specify, and the result is deposited on the
supplied test strip as per instructions. For our purposes, all of this
information is included in the background/auxiliary information,
\(b\).</p>

<p>
Consider a home-test with the following characteristics for
<i>symptomatic</i> subjects: <i>sensitivity</i> = .94,
<i>specificity</i> = .98. The <i>sensitivity</i> is the <i>true
positive rate</i> (the chance of a positive test result when disease
is present); so the <i>false negative rate</i> (the chance of a
negative test result when disease is present) for this test is .06 =
(1 - <i>sensitivity</i>). The <i>specificity</i> is the <i>true
negative rate</i> (the chance of a negative test result when disease
is not present); so the <i>false positive rate</i> (the chance of a
positive test result when disease is not present) for this test is .02
= (1 - <i>specificity</i>).</p>

<p>
Now, let&rsquo;s suppose that an individual subject is tested.
Condition \(c\) says that this subject is <i>symptomatic</i> and that
the test is administered to the subject in the appropriate way (as
specified in the instructions for the test). Let \(e\) say that the
<em>test result is positive</em> (i.e. the test shows that a
significant amount of the target antigen of the SARS-CoV-2 virus is
detected); and let \(\neg e\) say that the <em>test result is
negative</em> (i.e. the test shows that no significant amount of the
target antigen of the SARS-CoV-2 virus is detected). What the test
subject wants to know is the value of the posterior probabilities,
\(P[h \mid c\cdot e \cdot b]\) and \(P[h \mid c \cdot \neg e\cdot
b]\), that the subject has COVID-19, given the evidence of the
positive result, \((c\cdot e)\), or the negative test result,
\((c\cdot \neg e)\), taken together with the error rates of these
tests as described in \(b\).</p>

<p>
The values of these posterior probabilities depend on the following
likelihoods, which come from applying the <i>sensitivity</i> and
<i>specificity</i> statistics for the test to this individual test
subject:</p> 
\[P[e \mid h \cdot c \cdot b] = .94, \text{ due to the }\textit{sensitivity},
\]
 
\[P[\neg e \mid \neg h \cdot c \cdot b] = .98, \text{ due to the }\textit{specificity}.\]

<p>
As a result, we also have the following values:</p> 
\[(P[\neg e \mid h \cdot c \cdot b] = .06, \text{ for the }\textit{false negative rate},
\]

\[P[e \mid \neg h \cdot c \cdot b] = .02, \text{ for the }\textit{false positive rate}.
\]

<p>
This provides the following likelihood ratios against disease (against
\(h\)) for this test subject when the test result is positive, or
negative, respectively: 
\[\frac{P[e \mid \neg h\cdot c\cdot b]}{P[e \mid h \cdot c\cdot b]} = .02/.94 = .0213\]
 
\[\frac{P[\neg e \mid \neg h\cdot c\cdot b]}{P[\neg e \mid h\cdot c\cdot b]} = .98/.06 = 16.34.\]
 </p>

<p>
The value of the posterior probability that the subject has COVID-19,
given the evidence, depends on how plausible it is that the patient
has COVID-19 on the day of the test prior to taking the test results
into account, \(P[h \mid c \cdot b]\). In the context of medical
diagnosis, this prior probability is usually assessed on the basis of
the <em>base rate</em> for the disease in the patient&rsquo;s risk
group. Such information may be stated within the background
information \(b\). <em>Rule OB</em> shows how to calculate the
posterior probabilities from these values.</p> 
\[\begin{align}
&amp;\Omega[\neg h \mid  c \cdot e \cdot b \cdot (h \vee \neg h)] = 
\frac{P[\neg h \mid c \cdot e \cdot b]}{P[h \mid c \cdot e \cdot b]} \\
&amp;\qquad =
    \frac{P[e \mid \neg h \cdot c \cdot b]}{P[e \mid h \cdot c \cdot b]} 
  \times
   \frac{P[\neg h \mid c \cdot b]}{P[h \mid c \cdot b]}.
\end{align}\]

\[\begin{align}
P[h \mid c \cdot e \cdot b] &amp;= P[h \mid c \cdot e \cdot b \cdot (h \vee \neg h)] \\
 &amp;= \frac{1}{1 + \Omega[\neg h \mid  c \cdot e \cdot b \cdot (h \vee \neg h)]}.
\end{align}\]

<p>
And similarly for \(P[h \mid c \cdot \neg e \cdot b]\). </p>

<p>
The table below shows how these posterior probabilities depend on the
values of prior probabilities. The columns under &ldquo;Test Brand
1&rdquo; shows the posterior probabilities for the test described
above, the test that has <i>sensitivity</i> = .94 and
<i>specificity</i> = .98. The columns under &ldquo;Test Brand 2&rdquo;
shows the posterior probabilities for a different, lower sensitivity
test, a test that has <i>sensitivity</i> = .84 and <i>specificity</i>
= .98.</p>

<div class="figure avoid-break">

<table class="centered cellpad-small-dense cell-left all-rules vert-top small">
<tr>
<th></th>
<th colspan="2">Test Brand 1
<br/>
Sensitivity = .94
<br/>
Specificity = .98 </th>
<th colspan="2">Test Brand 2
<br/>
Sensitivity = .84
<br/>
Specificity = .98 </th> </tr>
<tr>
<th><small>\(P[h \mid c \cdot b]\)</small></th>
<th><small>\(P[h \mid c \cdot e \cdot b]\)</small></th>
<th><small>\(P[h \mid c \cdot \neg e \cdot b]\)</small></th>
<th><small>\(P[h \mid c \cdot e \cdot b]\)</small></th>
<th><small>\(P[h \mid c \cdot \neg e \cdot b]\)</small></th> </tr>
<tr>
  <td> .01</td>
  <td> .322</td>
  <td> .001</td>
  <td> .298</td>
  <td> .002</td> </tr>
<tr>
  <td> .02</td>
  <td> .490</td>
  <td> .001</td>
  <td> .462</td>
  <td> .003</td> </tr>
<tr>
  <td> .03</td>
  <td> .592</td>
  <td> .002</td>
  <td> .565</td>
  <td> .005</td> </tr>
<tr>
  <td> .04</td>
  <td> .662</td>
  <td> .003</td>
  <td> .636</td>
  <td> .007</td> </tr>
<tr>
  <td> .05</td>
  <td> .712</td>
  <td> .003</td>
  <td> .689</td>
  <td> .009</td> </tr>
<tr>
  <td> .06</td>
  <td> .750</td>
  <td> .004</td>
  <td> .728</td>
  <td> .010</td> </tr>
<tr>
  <td> .07</td>
  <td> .780</td>
  <td> .005</td>
  <td> .760</td>
  <td> .012</td> </tr>
<tr>
  <td> .08</td>
  <td> .803</td>
  <td> .005</td>
  <td> .785</td>
  <td> .014</td> </tr>
<tr>
  <td> .09</td>
  <td> .823</td>
  <td> .006</td>
  <td> .806</td>
  <td> .016</td> </tr>
<tr>
  <td> .10</td>
  <td> .839</td>
  <td> .007</td>
  <td> .824</td>
  <td> .018</td> </tr>
<tr>
  <td> .20</td>
  <td> .922</td>
  <td> .015</td>
  <td> .913</td>
  <td> .039</td> </tr>
<tr>
  <td> .30</td>
  <td> .953</td>
  <td> .026</td>
  <td> .947</td>
  <td> .065</td> </tr>
<tr>
  <td> .40</td>
  <td> .969</td>
  <td> .039</td>
  <td> .966</td>
  <td> .098</td> </tr>
<tr>
  <td> .50</td>
  <td> .979</td>
  <td> .058</td>
  <td> .977</td>
  <td> .140</td> </tr>
<tr>
  <td> .60</td>
  <td> .986</td>
  <td> .084</td>
  <td> .984</td>
  <td> .197</td> </tr>
<tr>
  <td> .70</td>
  <td> .991</td>
  <td> .125</td>
  <td> .990</td>
  <td> .276</td> </tr>
<tr>
  <td> .80</td>
  <td> .995</td>
  <td> .197</td>
  <td> .994</td>
  <td> .395</td> </tr>
<tr>
  <td> .90</td>
  <td> .998</td>
  <td> .355</td>
  <td> .997</td>
  <td> .595</td> </tr>
</table>

<p class="center">
Table: Posterior Probabilities for COVID-19 Home Test Results
<br/>
\(h\) = <i>disease present</i> &nbsp; &nbsp;\(e\) = <i>test result
positive</i></p>
</div>

<p>
When the precise values of the prior probabilities are unknown, but a
reasonable range can be estimated, a resulting range of posterior
probabilities may be calculated. Suppose we can be confident that the
base-rate for COVID-19 among symptomatic members of the relevant
population for the test subject is between .05 and .09. Then, when the
subject is tested with Test Brand 1, the posterior probability that
the subject has COVID-19, given a positive result is, according to the
table, \(.713 \le P[h \mid c\cdot e \cdot b] \le .823\). And the
posterior probability that the subject has COVID-19, given a negative
result, is \(.003 \le P[h \mid c \cdot \neg e \cdot b] \le .006\).</p>

<h3 id="Vague-Likelihoods">2.3. When Likelihoods are Vague or Imprecise: Evidence for Continental Drift.</h3>

<p>
In many contexts the values of likelihoods may be vague or imprecise.
Nevertheless, the evidence may still be capable of strongly supporting
one hypothesis over another in a reasonably objective way. Here is an
example. </p>

<p>
Consider the following simple version of the continental drift
hypothesis. \(h_2\): The land masses of Africa and South America were
once joined, then split apart and have drifted to there current
positions on Earth over the eons. Let&rsquo;s compare this hypothesis
to the older <i>contractionist</i> theory: \(h_1\): The continents
have fixed positions on Earth, which they acquired when the Earth
first formed, cooled, and contracted into its present configuration.
</p>

<p>
The evidence available for the drift hypothesis over the
contractionist hypothesis during the first half of the 20<sup>th</sup>
century included the following observations: (1) Upon careful
examination, the east coast of South America fits the shape of the
west coast of Africa extremely well. (2) When the coasts of South
America and Africa are aligned as closely as possible, and the geology
of the two continents is carefully examined, a number of geologic
features align across the two continents (e.g. the Ghana mountain
ranges align with mountain ranges in Brazil; the rock strata of the
Karroo system of South Africa matches precisely with the Santa
Catarina system in Brazil; etc.). (3) When the fossil record on both
continents is carefully examined, a number fossils of identical
species have been discovered to have lived at the same time on both
continents (e.g. Mesosaurus (land reptile, 286-258 million yrs. ago),
Cynognathus (fresh water reptile 250-240 million yrs. ago),
Glossopteris (tree-sized fern, 299 million yrs. ago)); and none of
these species could have crossed the Atlantic Ocean under their own
power. </p>

<p>
Let \(c\) represent the conjunction of all the specific methods used
to collect the above evidence, and let \(e\) represent a detailed
description of the precise results of all these investigations. (Here
\(b\) expresses relevant scientific background knowledge, including
the relevant knowledge of geology and evolutionary biology.) Consider
the evidential likelihoods, \(P[e \mid h_1 \cdot c \cdot b]\) and
\(P[e \mid h_2 \cdot c \cdot b]\). Although experts may be unable to
specify anything like precise numerical values for these likelihoods,
experts may readily agree that each of the above cited evidential
observations is much more likely on the drift hypothesis than on the
contraction hypothesis, and that they jointly constitute extremely
strong evidence in favor of <i>drift</i> over <i>contraction</i>. On a
Bayesian analysis this is due to the fact that, although these
likelihoods do not have precise values, it is obvious to experts that
the ratio of the likelihoods is pretty extreme, strongly favoring
drift over contraction. That is,</p>

<blockquote>
\(P[e \mid h_2 \cdot c \cdot b] / P[e \mid h_1 \cdot c \cdot b]\) is
very large, and its inverse, \(P[e \mid h_1 \cdot c \cdot b] / P[e
\mid h_2 \cdot c \cdot b]\), is very nearly zero.
</blockquote>

<p>
Thus, according to the Ratio Form of Bayes&rsquo; Theorem,</p>

\[P[h_1 \mid c \cdot e \cdot b] \; \lt \; P[h_1 \mid c \cdot e \cdot b] / P[h_2 \mid c \cdot e \cdot b]\]

<p>
should be very close to 0, strongly supporting \(h_2\) over \(h_1\),
<i>unless</i> the <i>drift</i> hypothesis is taken to be extremely
implausible as compared to <i>contraction</i> on other grounds &mdash;
i.e. unless \(P[h_1 \mid c \cdot b] / P[h_2 \mid c \cdot b]\) is
extremely large due to other information (which may be listed within
\(b\)). </p>

<p>
Historically, the evidence described above was well-known during the
first half of the 20<sup>th</sup> century. Nevertheless, most
geologists largely dismissed the <i>drift</i> hypothesis until the
1960s. Apparently the strength of this evidence did not suffice to
overcome non-evidential (though broadly empirical) considerations that
made the drift hypothesis seem much less plausible than the
traditional <i>contractionist</i> view. The chief difficulty was the
apparent absence of a plausible mechanism for moving continents across
the ocean floor. This difficulty was overcome when a plausible enough
convection mechanism was articulated, and evidence favoring it was
acquired. </p>

<h3 id="Disjunction-Stat-Hypoth">2.4. Bayesian Estimation for Disjunctions of Discrete Statistical Hypotheses</h3>

<p>
We now turn to an example application of <i>Rule BE-D</i>. </p>

<p>
Let &lsquo;<i>B</i>&rsquo; represent the collection of all households
in the United States during July, 2020. Let &lsquo;<i>A</i>&rsquo;
represent those households among them in which one or more dogs
reside. What proportion of the <i>B</i>s are <i>A</i>s? Symbolically,
for real number \(r\) between 0 and 1, let \(F(A,B)= r\) say that the
frequency (i.e. proportion) of \(A\)s among \(B\)s is \(r\). So, we
want to know, for what value of \(r\) does \(F(A,B)= r\) hold. Given
that the number of households in the United States during July of 2020
was a little under \(z\) = 129 million (stated within the background
and auxiliaries, \(b\)), there are in principle that many alternative
hypotheses: \(F(A,B)=k/z\) for each integer \(k\) between 0 and 129
million. </p>

<p>
Suppose a sample <i>S</i> consisting of \(n = 400\) of these
households is randomly drawn from <i>B</i> (households present in the
United States during July 20, 2020) with respect to whether or not
they are <i>A</i> (households with dogs). This is the experimental
condition, \(c\). And suppose that within sample <i>S</i>, \(m = 248\)
households report being in <i>A</i> (having one or more dogs in
residence). So, \(F(A,S)= m/n = 248/400=.62\). This is the evidence
\(e\). </p>

<p>
The posterior probability of any specific hypothesis, \(P[F(A,B)=k/z
\mid c \cdot F[A,S]=248/400 \cdot b]\), will be extremely small, even
for \(F(A,B)=248/400=.62\). And in any case, we shouldn&rsquo;t expect
the value of \(F[A,B]\) to be exactly the value of \(F(A,S)\). Rather,
what we may reasonably hope to determine is that some interval of
values below and above the sample value .62 has a fairly high
probability: e.g. 
\[P[.57 \le F(A,B) \le .67 \mid c \cdot F(A,S)=248/400 \cdot b] \ge .95.\]
 We will see how to determine such
posterior probabilities via <i>Rule BE-D</i>. </p>

<p>
Before proceeding, let&rsquo;s settle on a few convenient notational
conventions. To facilitate the statement of rule <i>BE-D</i> we pulled
a particular list of hypotheses to the front of the queue, and listed
them as \(h_1\) through \(h_k\). In the present example we diverge
from this way of labeling hypotheses. Instead, we employ a notation
that is more natural for the present example. We let each hypothesis
in the set of alternatives \(H\) take the form \(F(A,B)=r_k\), where
\(k\) now ranges from 0 through \(z\), and where we now define each
\(r_k\) to abbreviate proportion \(k/z\) of the population \(B\).
Furthermore, the main disjunction of hypotheses of interest now
consists of those frequencies within some interval \([v,u]\) centered
around the sample frequency \(F(A,S)=m/n\). Thus, the expression \(v
\le F[A,B] \le u\) (for some specific values of \(v\) and \(u\))
represents the disjunction of hypotheses, \((F[A,B]=v \;\vee \ldots \)
\(\vee\; F[A,B]=m/n \;\vee \ldots \) \(\vee\; F[A,B]=u)\), whose
posterior probability we want to evaluate. </p>

<p>
When a hypothesis states that the proportion of \(A\)s among \(B\)s is
\(r_k\), the associated likelihood of drawing a sample proportion
\(F(A,S)=m/n\) is given by the binomial distribution formula:</p>

\[\begin{align}
&amp;P[F(A,S)=m/n \mid c \cdot F(A,B)=r_k \cdot b] \\ 
&amp;\qquad = \frac{n!}{m!(n-m)!}\; r_k^m\; (1-r_k)^{n-m}.
\end{align}\]

<p>
Now, we apply the Bayesian Estimation rule <i>BE-D</i> as follows:</p>

\[\begin{align}
&amp;P[v \le F[A,B] \le q \mid c \cdot F[A,S]=m/n \cdot b] \\
&amp;\qquad \ge \frac{1}{1 + K \times \left[\frac{1}{\frac{\sum_{j = v\cdot z}^{u\cdot z} P[e \; \mid \; h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \; \mid \; h_i \cdot c \cdot b]}} - 1 \right]},
\end{align}\]

<p>
where the ratio of sums in the denominator is given by the formula,

\[\frac{\sum_{j = v\cdot z}^{u\cdot z} P[e \mid h_j \cdot c \cdot b]}{\sum_{i = 1}^z P[e \mid h_i \cdot c \cdot b]} \; = \;

\frac{\sum_{j = v\cdot z}^{u\cdot z}\; r_j^m\; (1-r_j)^{n-m}}{\sum_{i = 1}^z\; r_i^m\; (1-r_i)^{n-m}},\]
 where \((v\cdot z)\) and \((u\cdot z)\) are the
appropriate integers for the endpoints of the interval \([v, u]\)
(i.e. \((v\cdot z) /z = v\) and \((u\cdot z)/z = u\)). </p>

<p>
These large sums of binomial factors are difficult to calculate
directly. Fortunately, they are closely approximated by a more easily
calculable formula, that for the normalized Beta distribution. That
is, </p> 
\[\begin{align}
\frac{\sum_{j = v\cdot z}^{u\cdot z}\; r_j^m\; (1-r_j)^{n-m}}{\sum_{i = 1}^z\; s_i^k\; (1-s_i)^{n-m}}  \; &amp;\approxeq \; Beta[v,u \;:\; m+1,\; (n-m)+1] \\
&amp;=\; \frac{\int_{v}^u r^{m} (1-r)^{n-m} \; dr}{\int_{0}^1 s^m (1-s)^{n-m} \; ds}.
\end{align}\]

<p>
The values of this normalized Beta-distribution function may easily be
computed using well-know mathematics and spreadsheet programs. For
example, the version of this function supplied by one such spreadsheet
program takes the form BETA.DIST(\(x\), \(\alpha\), \(\beta\), TRUE).
It computes the value of the normalized beta distribution from 0 up to
to \(x\), where for our purposes \(\alpha = m+1\), \(\beta = (n-m)
+1\). The input value TRUE tells the program to calculate the integral
from 0 to \(x\) (whereas FALSE would tell the program to calculate the
value of the density function at point \(x\)). Using this spreadsheet
version of the function, we calculate the value of the normalized
Beta-distribution between \(v\) and \(u\) by inputing the following
formula:</p> 
\[\begin{align}
\tag{$BD$} &amp;\text{BETA.DIST}[u,\; m+1,\; (n-m)+1,\; \textit{TRUE}] \\
&amp;\quad - \text{BETA.DIST}[v,\; m+1,\; (n-m)+1,\; \textit{TRUE}].
\end{align}\]

<p>
For simplicity, we refer to the above formula as \(BD(u,v,m,n)\). So,
to have the spreadsheet program compute a lower bound on the value of
\(P[v\le F[A,B]\le u \mid c \cdot F[A,S]=m/n \cdot b]\) for specific
values of \(m\), \(n\), \(v\), and \(u\), we need only input this
formula with those values, together with a value for the upper bound
\(K\) on ratios of prior probabities: </p> 
\[
\frac{1}{1 + K\times\left(\frac{1}{
BD(u,v,m,n)} - 1\right)}
\]

<p>
In many real cases it will be at least as initially plausible that the
true frequency value lies within of the <em>region of interest</em>
between <i>v</i> and <i>u</i> as that it lies outside that that
region. In such cases the value of <i>K</i> must be less than or equal
to 1. However, even when the upper bound <i>K</i> on the ratio of
these priors is quite large, any moderately large sample size <i>n</i>
will drive the posterior probability \(P[v \le F[A,B] \le q \mid c
\cdot F[A,S]=m/n \cdot b]\) close to 1, for fairly narrow bounds
<i>v</i> and <i>u</i>. The following table, calculated via the
Beta-distribution, illustrates this for both</p> 
\[P[F(A,B)=.62\pm .05\mid c \cdot F(A,S)=m/n=.62 \cdot b]\]

<p>
and</p> 
\[P[F(A,B)=.62\pm .025\mid c \cdot F(A,S)=m/n=.62 \cdot b]\]

<p>
over a range of different samples sizes \(n\), and over a wide range
of values of \(K\).</p>

<div class="figure wide">

<table class="centered cellpad-small-dense cell-left all-rules vert-top smallest avoid-break">
<tr>
  <td colspan="2">Size of sample <i>S</i> from <i>B</i> \(= n\),
<br />
Number of <i>A</i>s in sample <i>S</i> \(= m\):
<br />
\(m/n = .62\) throughout table </td>
  <td></td>
  <td colspan="6">Where \(\frac{P[F(A,B)=s \mid c \cdot b]}{P[F(A,B)=r
\mid c \cdot b]} \: \le \: K\) for all \(r\), \(s\) such that
<br />
\(.62-q \le r \le .62+q\) <i>and</i> either \(s \lt .62-q\) or \(s \gt
.62+q\),
<br />
\(P[F(A,B)=.62\pm q\mid c \cdot F(A,S)=m/n \cdot b] \;\; \ge\)</td>
</tr>
<tr>
  <td>Prior
<br />
Ratio <i>K</i>
<br />
\(\downarrow\)</td>
  <td><i>n</i> \(\rightarrow\)
<br />
(<i>m</i>) \(\rightarrow\) </td>
  <td> </td>
  <td>400
<br />
(248)</td>
  <td>800
<br />
(496)</td>
  <td>1600
<br />
(992)</td>
  <td>3200
<br />
(1984)</td>
  <td>6400
<br />
(3968)</td>
  <td>12800
<br />
(7936)</td> </tr>
<tr>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td> </tr>
<tr>
  <td>1</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.9614
<br />
0.6982</td>
  <td>0.9965
<br />
0.8554</td>
  <td>1.0000
<br />
0.9608</td>
  <td>1.0000
<br />
0.9964</td>
  <td>1.0000
<br />
1.0000</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>2</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.9256
<br />
0.5364</td>
  <td>0.9930
<br />
0.7474</td>
  <td>0.9999
<br />
0.9246</td>
  <td>1.0000
<br />
0.9929</td>
  <td>1.0000
<br />
0.9999</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>5</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.8327
<br />
0.3163</td>
  <td>0.9827
<br />
0.5420</td>
  <td>0.9998
<br />
0.8306</td>
  <td>1.0000
<br />
0.9825</td>
  <td>1.0000
<br />
0.9998</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>10</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> =.025 \(\rightarrow\)</td>
  <td></td>
  <td>0.7133
<br />
0.1879</td>
  <td>0.9661
<br />
0.3717</td>
  <td>0.9996
<br />
0.7103</td>
  <td>1.0000
<br />
0.9656</td>
  <td>1.0000
<br />
0.9996</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>100</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.1992
<br />
0.0226</td>
  <td>0.7402
<br />
0.0559</td>
  <td>0.9963
<br />
0.1969</td>
  <td>1.0000
<br />
0.7371</td>
  <td>1.0000
<br />
0.9962</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>1,000</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.0243
<br />
0.0023</td>
  <td>0.2217
<br />
0.0059</td>
  <td>0.9639
<br />
0.0239</td>
  <td>1.0000
<br />
0.2190</td>
  <td>1.0000
<br />
0.9637</td>
  <td>1.0000
<br />
1.0000</td> </tr>
<tr>
  <td>10,000</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.0025
<br />
0.0002</td>
  <td>0.0277
<br />
0.0006</td>
  <td>0.7277
<br />
0.0024</td>
  <td>0.9999
<br />
0.0273</td>
  <td>1.0000
<br />
0.7261</td>
  <td>1.0000
<br />
0.9999</td> </tr>
<tr>
  <td>100,000</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.0002
<br />
0.0000</td>
  <td>0.0028
<br />
0.0001</td>
  <td>0.2109
<br />
0.0002</td>
  <td>0.9994
<br />
0.0028</td>
  <td>1.0000
<br />
0.2096</td>
  <td>1.0000
<br />
0.9994</td> </tr>
<tr>
  <td>1,000,000</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.0000
<br />
0.0000</td>
  <td>0.0003
<br />
0.0000</td>
  <td>0.0260
<br />
0.0000</td>
  <td>0.9940
<br />
0.0003</td>
  <td>1.0000
<br />
0.0258</td>
  <td>1.0000
<br />
0.9943</td> </tr>
<tr>
  <td>10,000,000</td>
  <td><i>q</i> = .05 \(\rightarrow\)
<br />
<i>q</i> = .025 \(\rightarrow\)</td>
  <td></td>
  <td>0.0000
<br />
0.0000</td>
  <td>0.0000
<br />
0.0000</td>
  <td>0.0027
<br />
0.0000</td>
  <td>0.9433
<br />
0.0000</td>
  <td>1.0000
<br />
0.0026</td>
  <td>1.0000
<br />
0.9457</td> </tr>
</table>

<p class="center">
Table: Lower Bounds on Posterior Probability
<br />
\(P[F(A,B)=.62\pm q\mid c \cdot F(A,S)=m/n=.62 \cdot b]\),
<br />
for Sample <i>S</i> of Size <i>n</i> Randomly Drawn from <i>B</i>.</p>
</div>

<p>
All probability entries in this table are accurate to four decimal
places. Those entries of form &lsquo;1.0000&rsquo; actually represent
probability values that are a tiny bit less than 1.0000. </p>

<p>
Notice that even when the bound of ratios of prior probabilities,
\(K\), is extremely large, a sufficiently large sample size overcomes
this disparity between prior probabilities. To illustrate the point,
let&rsquo;s focus on those hypotheses that lie in the interval
\(F(A,B)=.62\pm .025\) (i.e. the interval \(.595 \le F(A,B) \le
.645\)). In this context K is an an upper bound on the ratios of all
the prior probabilities, 
\[K \;\ge\; P[F(A,B)=r_i \mid c \cdot b] / P[F(A,B)=r_j \mid c \cdot b],\]
 such that \(r_j\) lies within
the interval \(.62\pm .025\) and \(r_i\) lies outside the interval
\(.62\pm .025\). For \(K = 1,000\) this means that some of the
specific frequency hypotheses \(F(A,B)=k/z\) outside this interval
(i.e. some hypotheses that either have \(k/z \lt .62-.025\) or have
\(k/z \gt .62+.025\)) may have prior probabilities <i>up to</i> 1000
times larger than the priors of specific hypotheses within this
interval. But no specific hypotheses outside the interval has a prior
<i>more than</i> 1000 times larger than any hypothesis inside the
interval. The table shows that even when the upper bound on these
ratios of priors is this extreme, a large enough sample size, \(n =
6400\), results in a reasonably good lower bound on the posterior
probability: 
\[P[F(A,B)=.62\pm .025\mid c \cdot F(A,S)=3968/6400 \cdot b] \; \ge \; .9637.\]
 And even for a really extreme value of this
ratio of priors, \(K = 10,000,000\), a sample size of \(n = 12800\)
results in a decent lower bound on the posterior: 
\[P[F(A,B)=.62\pm .025\mid c \cdot F(A,S)=7936/12800 \cdot b] \; \ge \; .9457.\]
 </p>

<h3 id="Continuous-Stat-Hypoth">2.5. Bayesian Estimation for a Continuous Range of Alternative Hypotheses</h3>

<p>
Let&rsquo;s consider a simple example of a statistical hypothesis
about a collection of independent evidential outcomes. Suppose we
possess a warped coin and want to determine its propensity for turning
up <em>heads</em> when tossed in a standard unbiased way. Consider two
hypotheses, \(h_{q}\) and \(h_{r}\), which say that the chances (or
propensities) for the coin to come up <em>heads</em> when tossed are
\(q\) and \(r\), respectively. Let \(c\) report that the coin is
tossed \(n\) times in the normal way, and let \(e\) say that precisely
\(m\) occurrences of <em>heads</em> result. Supposing that the
outcomes of such tosses are probabilistically independent (asserted by
\(b\)). So, the respective likelihoods take the usually binomial form

\[ P[e \mid h_{r}\cdot c \cdot b] = \frac{n!}{m! \times(n-m)!} \times r^m (1-r)^{n-m}, \]
 </p>

<p>
Then, <i>Rule RB</i> yields the following formula, where the
likelihood ratio is the ratio of the respective binomial terms:</p>

\[ \frac{P[h_{q} \mid c\cdot e \cdot b]} {P[h_{r} \mid c\cdot e \cdot b]} = \frac{q^m (1-q)^{n-m}} {r^m (1-r)^{n-m}} \times \frac{P[h_{q} \mid c \cdot b]} {P[h_{r} \mid c \cdot b]} \]

<p>
When, for instance, the coin is tossed \(n = 100\) times and comes up
<em>heads</em> \(m = 72\) times, the evidence for hypothesis
\(h_{1/2}\) as compared to \(h_{3/4}\) is given by the likelihood
ratio</p> 
\[\frac{P [e \mid h_{1/2}\cdot c \cdot b]} {P [e \mid h_{3/4}\cdot c \cdot b]} = \frac{[(1/2)^{72}(1/2)^{28}]}{[(3/4)^{72}(1/4)^{28}]} = .000056269. \]

<p>
Such evidence <em>strongly refutes</em> the \(h_{1/2}\)
(<i>fair-coin</i>) hypothesis with respect to the \(h_{3/4}\)
(<i>bias-coin</i> towards 3/4-<i>heads</i>) hypothesis, provided that
the assessment of prior plausibilities for these two hypotheses
doesn&rsquo;t make the latter hypothesis <em>too extremely
implausible</em> to begin with. In this case, provided that
\(h_{1/2}\) is initially no more that 100 times more plausible than
the \(h_{3/4}\) &mdash; i.e. provided that \(P[h_{1/2} \mid b] /
P[h_{3/4} \mid b] \le 100\) &mdash; the resulting ratio of posterior
probabilities must be less than or equal to .0056269: 
\[ \frac{P[h_{1/2} \mid c^{n}\cdot e^{n} \cdot b]} {P[h_{3/4} \mid c^{n}\cdot e^{n} \cdot b]} \le .000056269 \times 100 = .0056269 \]

Notice, however, that this <em>strong refutation</em> of \(h_{1/2}\)
is not <em>absolute refutation</em>. Additional evidence could reverse
the total proportion of <i>heads</i> outcomes that favor it.</p>

<p>
In cases like this, where all the competing hypotheses lie within a
continuous region, the Bayesian Estimation Rule <i>BE-C</i> provides
another useful way to assess the evidential support for hypotheses. In
the coin-tossing case, the relevant region of alternative hypotheses
\(H\) is the class of all hypotheses of form \(h_{r}\), where each
such hypothesis says that the chance of <i>heads</i> on each coin-toss
is \(r\). So, when \(c\) says the coin is tossed \(n\) times, and e
says these tosses produce precisely \(m\) occurrences of <i>heads</i>
(and \(b\) says the tosses are independent and identically
distributed), the individual likelihoods continue to take the binomial
form: 
\[P[e \mid h_{r} \cdot c \cdot b] = \frac{n!}{m! \times(n-m)!} \times r^m (1-r)^{n-m}.\]
 </p>

<p>
Let \(h[v,u]\) express the hypothesis that the propensity for tosses
to land <i>heads</i> is some real number in the interval between \(v\)
and \(u\). Then, applying <i>Rule BE-C</i> to this problem, our goal
is to evaluate posterior probabilities of form 
\[\begin{align}
P[h[v,u]  \mid c \cdot e \cdot b] &amp;= \int_v^u p[h_q \mid c \cdot e \cdot b] \; \; dq \\
&amp;\ge \frac{1}{1 + K \times \left[\frac{1}{\frac{\int_v^u r^m (1-r)^{n-m} \; \; dr}{\int_0^1 q^m (1-q)^{n-m} \; \; dq}} - 1 \right]},
\end{align}\]
 where K is
an an upper bound on the ratios of values of the prior probability
density functions, 
\[K \;\ge\; p[h_q \mid c \cdot b] / p[h_r \mid c \cdot b],\]
 when \(r\) lies within the interval
between \(v\) and \(u\), and \(q\) lies outside this interval.</p>

<p>
It turns out that the ratio \(\frac{\int_v^u r^m (1-r)^{n-m} \; \;
dr}{\int_0^1 q^m (1-q)^{n-m} \; \; dq}\) in this equation is the very
definition of the normalized Beta-distribution function (discussed
earlier) applied to \(m\) positive outcomes in \(n\) trials. We can
employ a well-known spreadsheet application to calculate values of the
normalized Beta-distribution between specific values of <i>v</i> and
<i>u</i>, using the previously-defined formula \(BD(u,v,m,n)\).</p>

<p>
Thus, we have the following formula for the lower bound on the
posterior probability that the propensity for <i>heads</i> lies within
an interval between bounds \(v\) and \(u\).</p> 
\[P[h[v,u]  \mid c \cdot e \cdot b] \; \; \ge
\frac{1}{1 + K\times\left(\frac{1}{BD(u,v,m,n)}\right)}.
\]

<p>
Here are a few examples calculated via this formula. In each case, the
values of \(v\) and \(u\) have been chosen to lie equal distances
below and above .72, which we assume to be the proportion found in the
sample, \(m/n = .72\). Each of the following posterior probabilities
draws on specified values of m and n, and a specified value for \(K\).
</p>

<table class="centered cellpad-med-dense cell-left all-rules vert-top avoid-break">
<tr>
<th>\(K\)</th>
<th>\(n\)</th>
<th>\(m\)</th>
<th><em>posterior probabilities</em></th> </tr>
<tr>
  <td>1</td>
  <td>100</td>
  <td>72</td>
  <td>\(P[h[.63,.81] \mid c \cdot e \cdot b] \; \; \gt .956\)
<br />
\(P[h[.60,.84] \mid c \cdot e \cdot b] \; \; \gt .992\)</td></tr>
<tr>
  <td>10</td>
  <td>100</td>
  <td>72</td>
  <td>\(P[h[.59,.85] \mid c \cdot e \cdot b] \; \; \gt .959\)
<br />
\(P[h[.56,.88] \mid c \cdot e \cdot b] \; \; \gt .994\)</td></tr>
<tr>
  <td>100</td>
  <td>100</td>
  <td>72</td>
  <td>\(P[h[.56,.88] \mid c \cdot e \cdot b] \; \; \gt .946\)
<br/>
\(P[h[.53,.91] \mid c \cdot e \cdot b] \; \; \gt .994\)</td></tr>
<tr>
  <td>1</td>
  <td>1000</td>
  <td>720</td>
  <td>\(P[h[.69,.75] \mid c \cdot e \cdot b] \; \; \gt .965\)
<br/>
\(P[h[.68,.76] \mid c \cdot e \cdot b] \; \; \gt .995\)</td></tr>
<tr>
  <td>10</td>
  <td>1000</td>
  <td>720</td>
  <td>\(P[h[.68,.76] \mid c \cdot e \cdot b] \; \; \gt .953\)
<br />
\(P[h[.67,.77] \mid c \cdot e \cdot b] \; \; \gt .995\)</td></tr>
<tr>
  <td>100</td>
  <td>1000</td>
  <td>720</td>
  <td>\(P[h[.67,.77] \mid c \cdot e \cdot b] \; \; \gt .956\)
<br />
\(P[h[.66,.78] \mid c \cdot e \cdot b] \; \; \gt .997\)</td></tr>
</table>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian
Epistemology</em>, Oxford: Oxford University Press.
doi:10.1093/0199269750.001.0001</li>

<li>Carnap, Rudolf, 1950, <em>Logical Foundations of Probability</em>,
Chicago: University of Chicago Press.</li>

<li>&ndash;&ndash;&ndash;, 1952, <em>The Continuum of Inductive
Methods</em>, Chicago: University of Chicago Press.</li>

<li>&ndash;&ndash;&ndash;, 1963, &ldquo;Replies and Systematic
Expositions&rdquo;, in <em>The Philosophy of Rudolf Carnap</em>, Paul
Arthur Schilpp (ed.),La Salle, IL: Open Court.</li>

<li>Chihara, Charles S., 1987, &ldquo;Some Problems for Bayesian
Confirmation Theory&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 38(4): 551&ndash;560. doi:10.1093/bjps/38.4.551</li>

<li>Christensen, David, 1999, &ldquo;Measuring Confirmation&rdquo;,
<em>Journal of Philosophy</em>, 96(9): 437&ndash;61.
doi:10.2307/2564707</li>

<li>&ndash;&ndash;&ndash;, 2004, <em>Putting Logic in its Place:
Formal Constraints on Rational Belief</em>, Oxford: Oxford University
Press. doi:10.1093/0199263256.001.0001</li>

<li>De Finetti, Bruno, 1937, &ldquo;La Pr&eacute;vision: Ses Lois
Logiques, Ses Sources Subjectives&rdquo;, <em>Annales de
l&rsquo;Institut Henri Poincar&eacute;</em>, 7: 1&ndash;68; translated
by Henry E. Kyburg, Jr. as &ldquo;Foresight. Its Logical Laws, Its
Subjective Sources&rdquo;, in <em>Studies in Subjective
Probability</em>, Henry E. Kyburg, Jr. and H.E. Smokler (eds.), Robert
E. Krieger Publishing Company, 1980.</li>

<li>Dowe, David L., Steve Gardner, and Graham Oppy, 2007,
&ldquo;Bayes, Not Bust! Why Simplicity is No Problem for
Bayesians&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 58(4): 709&ndash;754. doi:10.1093/bjps/axm033</li>

<li>Dubois, Didier J. and Henri Prade, 1980, <em>Fuzzy Sets and
Systems</em>, (Mathematics in Science and Engineering, 144), New York:
Academic Press.</li>

<li>&ndash;&ndash;&ndash;, 1990, &ldquo;An Introduction to
Possibilistic and Fuzzy Logics&rdquo;, in Glenn Shafer and Judea Pearl
(eds.), <em>Readings in Uncertain Reasoning</em>, San Mateo, CA:
Morgan Kaufmann, 742&ndash;761.</li>

<li>Duhem, P., 1906, <em>La theorie physique. Son objet et sa
structure</em>, Paris: Chevalier et Riviere; translated by P.P.
Wiener, <em>The Aim and Structure of Physical Theory</em>, Princeton,
NJ: Princeton University Press, 1954.</li>

<li>Earman, John, 1992, <em>Bayes or Bust? A Critical Examination of
Bayesian Confirmation Theory</em>, Cambridge, MA: MIT Press.</li>

<li>Edwards, A.W.F., 1972, <em>Likelihood: an account of the
statistical concept of likelihood and its application to scientific
inference</em>, Cambridge: Cambridge University Press.</li>

<li>Edwards, Ward, Harold Lindman, and Leonard J. Savage, 1963,
&ldquo;Bayesian Statistical Inference for Psychological
Research&rdquo;, <em>Psychological Review</em>, 70(3): 193&ndash;242.
doi:10.1037/h0044139</li>

<li>Eells, Ellery, 1985, &ldquo;Problems of Old Evidence&rdquo;,
<em>Pacific Philosophical Quarterly</em>, 66(3&ndash;4):
283&ndash;302. doi:10.1111/j.1468-0114.1985.tb00254.x</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Confirmation Theory&rdquo;,
Sarkar and Pfeifer 2006..</li>

<li>Eells, Ellery and Branden Fitelson, 2000, &ldquo;Measuring
Confirmation and Evidence&rdquo;, <em>Journal of Philosophy</em>,
97(12): 663&ndash;672. doi:10.2307/2678462</li>

<li>Field, Hartry H., 1977, &ldquo;Logic, Meaning, and Conceptual
Role&rdquo;, <em>Journal of Philosophy</em>, 74(7): 379&ndash;409.
doi:10.2307/2025580</li>

<li>Fisher, R.A., 1922, &ldquo;On the Mathematical Foundations of
Theoretical Statistics&rdquo;, <em>Philosophical Transactions of the
Royal Society, series A </em>, 222(594&ndash;604): 309&ndash;368.
doi:10.1098/rsta.1922.0009</li>

<li>Fitelson, Branden, 1999, &ldquo;The Plurality of Bayesian Measures
of Confirmation and the Problem of Measure Sensitivity&rdquo;,
<em>Philosophy of Science</em>, 66: S362&ndash;S378.
doi:10.1086/392738</li>

<li>&ndash;&ndash;&ndash;, 2001, &ldquo;A Bayesian Account of
Independent Evidence with Applications&rdquo;, <em>Philosophy of
Science</em>, 68(S3): S123&ndash;S140. doi:10.1086/392903</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Putting the Irrelevance Back
Into the Problem of Irrelevant Conjunction&rdquo;, <em>Philosophy of
Science</em>, 69(4): 611&ndash;622. doi:10.1086/344624</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Inductive Logic&rdquo;, Sarkar
and Pfeifer 2006..</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Logical Foundations of
Evidential Support&rdquo;, <em>Philosophy of Science</em>, 73(5):
500&ndash;512. doi:10.1086/518320</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Likelihoodism, Bayesianism,
and Relational Confirmation&rdquo;, <em>Synthese</em>, 156(3):
473&ndash;489. doi:10.1007/s11229-006-9134-9</li>

<li>Fitelson, Branden and James Hawthorne, 2010, &ldquo;How Bayesian
Confirmation Theory Handles the Paradox of the Ravens&rdquo;, in Eells
and Fetzer (eds.), <em>The Place of Probability in Science</em>, Open
Court.
 [<a href="https://fitelson.org/ravens.pdf" target="other">Fitelson &amp; Hawthorne 2010 preprint available from the author (PDF)</a>]</li>
 
<li>Forster, Malcolm and Elliott Sober, 2004, &ldquo;Why
Likelihood&rdquo;, in Mark L. Taper and Subhash R. Lele (eds.),
<em>The Nature of Scientific Evidence</em>, Chicago: University of
Chicago Press.</li>

<li>Friedman, Nir and Joseph Y. Halpern, 1995, &ldquo;Plausibility
Measures: A User&rsquo;s Guide&rdquo;, in <em>UAI 95: Proceedings of
the Eleventh Conference on Uncertainty in Artificial
Intelligence</em>, 175&ndash;184.</li>

<li>Gaifman, Haim and Marc Snir, 1982, &ldquo;Probabilities Over Rich
Languages, Testing and Randomness&rdquo;, <em>Journal of Symbolic
Logic</em>, 47(3): 495&ndash;548. doi:10.2307/2273587</li>

<li>Gillies, Donald, 2000, <em>Philosophical Theories of
Probability</em>, London: Routledge.</li>

<li>Glymour, Clark N., 1980, <em>Theory and Evidence</em>, Princeton,
NJ: Princeton University Press.</li>

<li>Goodman, Nelson, 1983, <em>Fact, Fiction, and Forecast</em>,
4<sup>th</sup> edition, Cambridge, MA: Harvard University Press.</li>

<li>Hacking, Ian, 1965, <em>Logic of Statistical Inference</em>,
Cambridge: Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 1975, <em>The Emergence of Probability: a
Philosophical Study of Early Ideas about Probability, Induction and
Statistical Inference</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511817557</li>

<li>&ndash;&ndash;&ndash;, 2001, <em>An Introduction to Probability
and Inductive Logic</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511801297</li>

<li>H&aacute;jek, Alan, 2003a, &ldquo;What Conditional Probability
Could Not Be&rdquo;, <em>Synthese</em>, 137(3):, 273&ndash;323.
doi:10.1023/B:SYNT.0000004904.91112.16</li>

<li>&ndash;&ndash;&ndash;, 2003b, &ldquo;Interpretations of the
Probability Calculus&rdquo;, in the <em>Stanford Encyclopedia of
Philosophy</em>, (Summer 2003 Edition), Edward N. Zalta (ed.), URL =
 &lt;<a href="https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/" target="other">https://plato.stanford.edu/archives/sum2003/entries/probability-interpret/</a>&gt;</li>
 
<li>&ndash;&ndash;&ndash;, 2005, &ldquo;Scotching Dutch Books?&rdquo;
<em>Philosophical Perspectives</em>, 19 (Epistemology): 139&ndash;151.
doi:10.1111/j.1520-8583.2005.00057.x</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;The Reference Class Problem is
Your Problem Too&rdquo;, <em>Synthese</em>, 156(3): 563&ndash;585.
doi:10.1007/s11229-006-9138-5</li>

<li>Halpern, Joseph Y., 2003, <em>Reasoning About Uncertainty</em>,
Cambridge, MA: MIT Press.</li>

<li>Harper, William L., 1976, &ldquo;Rational Belief Change, Popper
Functions and Counterfactuals&rdquo;, in Harper and Hooker 1976:
73&ndash;115. doi:10.1007/978-94-010-1853-1_5</li>

<li>Harper, William L. and Clifford Alan Hooker (eds.), 1976,
<em>Foundations of Probability Theory, Statistical Inference, and
Statistical Theories of Science, volume I Foundations and Philosophy
of Epistemic Applications of Probability Theory</em>, (The Western
Ontario Series in Philosophy of Science, 6a), Dordrecht: Reidel.
doi:10.1007/978-94-010-1853-1</li>

<li>Hawthorne, James, 1993, &ldquo;Bayesian Induction <em>is</em>
Eliminative Induction&rdquo;, <em>Philosophical Topics</em>, 21(1):
99&ndash;138. doi:10.5840/philtopics19932117</li>

<li>&ndash;&ndash;&ndash;, 1994,&ldquo;On the Nature of Bayesian
Convergence&rdquo;, <em>PSA: Proceedings of the Biennial Meeting of
the Philosophy of Science Association 1994</em>, 1: 241&ndash;249.
doi:10.1086/psaprocbienmeetp.1994.1.193029</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;<em>Degree-of-Belief</em> and
<em>Degree-of-Support</em>: Why Bayesians Need Both Notions&rdquo;,
<em>Mind</em>, 114(454): 277&ndash;320. doi:10.1093/mind/fzi277</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;The Lockean Thesis and the
Logic of Belief&rdquo;, in Franz Huber and Christoph Schmidt-Petri
(eds.), <em>Degrees of Belief</em>, (Synthese Library, 342),
Dordrecht: Springer, pp. 49&ndash;74.
doi:10.1007/978-1-4020-9198-8_3</li>

<li>Hawthorne, James and Luc Bovens, 1999, &ldquo;The Preface, the
Lottery, and the Logic of Belief&rdquo;, <em>Mind</em>, 108(430):
241&ndash;264. doi:10.1093/mind/108.430.241</li>

<li>Hawthorne, James and Branden Fitelson, 2004, &ldquo;Discussion:
Re-solving Irrelevant Conjunction With Probabilistic
Independence&rdquo;, <em>Philosophy of Science</em>, 71(4):
505&ndash;514. doi:10.1086/423626</li>

<li>Hellman, Geoffrey, 1997, &ldquo;Bayes and Beyond&rdquo;,
<em>Philosophy of Science</em>, 64(2): 191&ndash;221.
doi:10.1086/392548</li>

<li>Hempel, Carl G., 1945, &ldquo;Studies in the Logic of
Confirmation&rdquo;, <em>Mind</em>, 54(213): 1&ndash;26,
54(214):97&ndash;121. doi:10.1093/mind/LIV.213.1
doi:10.1093/mind/LIV.214.97</li>

<li>Horwich, Paul, 1982, <em>Probability and Evidence</em>, Cambridge:
Cambridge University Press. doi:10.1017/CBO9781316494219</li>

<li>Howson, Colin, 1997, &ldquo;A Logic of Induction&rdquo;,
<em>Philosophy of Science</em>, 64(2): 268&ndash;290.
doi:10.1086/392551</li>

<li>&ndash;&ndash;&ndash;, 2000, <em>Hume&rsquo;s Problem: Induction
and the Justification of Belief</em>, Oxford: Oxford University Press.
doi:10.1093/0198250371.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Bayesianism in
Statistics&ldquo;, in Swinburne 2002: 39&ndash;71.
doi:10.5871/bacad/9780197263419.003.0003</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Logic With Numbers&rdquo;,
<em>Synthese</em>, 156(3): 491&ndash;512.
doi:10.1007/s11229-006-9135-8</li>

<li>Howson, Colin and Peter Urbach, 1993, <em>Scientific Reasoning:
The Bayesian Approach</em>, La Salle, IL: Open Court. [3rd edition,
2005.]</li>

<li>Huber, Franz, 2005a, &ldquo;Subjective Probabilities as Basis for
Scientific Reasoning?&rdquo; <em>British Journal for the Philosophy of
Science</em>, 56(1): 101&ndash;116. doi:10.1093/phisci/axi105</li>

<li>&ndash;&ndash;&ndash;, 2005b, &ldquo;What Is the Point of
Confirmation?&rdquo; <em>Philosophy of Science</em>, 72(5):
1146&ndash;1159. doi:10.1086/508961</li>

<li>Jaynes, Edwin T., 2003, <em>Probability Theory: the Logic of
Science</em>, Cambridge: Cambridge University Press.</li>

<li>Jeffrey, Richard C., 1983, <em>The Logic of Decision</em>, 2nd
edition, Chicago: University of Chicago Press.</li>

<li>&ndash;&ndash;&ndash;, 1987, &ldquo;Alias Smith and Jones: The
Testimony of the Senses&rdquo;, <em>Erkenntnis</em>, 26(3):
391&ndash;399. doi:10.1007/BF00167725</li>

<li>&ndash;&ndash;&ndash;, 1992, <em>Probability and the Art of
Judgment</em>, New York: Cambridge University Press.
doi:10.1017/CBO9781139172394</li>

<li>&ndash;&ndash;&ndash;, 2004, <em>Subjective Probability: The Real
Thing</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511816161</li>

<li>Jeffreys, Harold, 1939, <em>Theory of Probability</em>, Oxford:
Oxford University Press.</li>

<li>Joyce, James M., 1998, &ldquo;A Nonpragmatic Vindication of
Probabilism&rdquo;, <em>Philosophy of Science</em>, 65(4):

575&ndash;603. doi:10.1086/392661</li>

<li>&ndash;&ndash;&ndash;, 1999, <em>The Foundations of Causal
Decision Theory</em>, New York: Cambridge University Press.
doi:10.1017/CBO9780511498497</li>

<li>&ndash;&ndash;&ndash;, 2003, &ldquo;Bayes&rsquo; Theorem&rdquo;,
in the <em>Stanford Encyclopedia of Philosophy</em>, (Summer 2003
Edition), Edward N. Zalta (ed.), URL =
 &lt;<a href="https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/" target="other">https://plato.stanford.edu/archives/win2003/entries/bayes-theorem/</a>&gt;</li>
 
<li>&ndash;&ndash;&ndash;, 2004, &ldquo;Bayesianism&rdquo;, in Alfred
R. Mele and Piers Rawling (eds.), <em>The Oxford Handbook of
Rationality</em>, Oxford: Oxford University Press, pp. 132&ndash;153.
doi:10.1093/0195145399.003.0008</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;How Probabilities Reflect
Evidence&rdquo;, <em>Philosophical Perspectives</em>, 19:
153&ndash;179. doi:10.1111/j.1520-8583.2005.00058.x</li>

<li>Kaplan, Mark, 1996, <em>Decision Theory as Philosophy</em>,
Cambridge: Cambridge University Press.</li>

<li>Kelly, Kevin T., Oliver Schulte, and Cory Juhl, 1997,
&ldquo;Learning Theory and the Philosophy of Science&rdquo;,
<em>Philosophy of Science</em>, 64(2): 245&ndash;267.
doi:10.1086/392550</li>

<li>Keynes, John Maynard, 1921, <em>A Treatise on Probability</em>,
London: Macmillan and Co.</li>

<li>Kolmogorov, A.N., 1956, <em>Foundations of the Theory of
Probability</em> (<em>Grundbegriffe der
Wahrscheinlichkeitsrechnung</em>, 2<sup>nd</sup> edition, New York:
Chelsea Publishing Company.</li>

<li>Koopman, B.O., 1940, &ldquo;The Bases of Probability&rdquo;,
<em>Bulletin of the American Mathematical Society</em>, 46(10):
763&ndash;774. Reprinted in H. Kyburg and H. Smokler (eds.), 1980,
<em>Studies in Subjective Probability</em>, 2nd edition, Huntington,
NY: Krieger Publ. Co.
 [<a href="https://projecteuclid.org/euclid.bams/1183503229" target="other">Koopman 1940 available online</a>]</li>
 
<li>Kyburg, Henry E., Jr., 1974, <em>The Logical Foundations of
Statistical Inference</em>, Dordrecht: Reidel.
doi:10.1007/978-94-010-2175-3</li>

<li>&ndash;&ndash;&ndash;, 1977, &ldquo;Randomness and the Right
Reference Class&rdquo;, <em>Journal of Philosophy</em>, 74(9):
501&ndash;520. doi:10.2307/2025794</li>

<li>&ndash;&ndash;&ndash;, 1978, &ldquo;An Interpolation Theorem for
Inductive Relations&rdquo;, <em>Journal of Philosophy</em>,
75:93&ndash;98.</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Belief, Evidence, and
Conditioning&rdquo;, <em>Philosophy of Science</em>, 73(1):
42&ndash;65. doi:10.1086/510174</li>

<li>Lange, Marc, 1999, &ldquo;Calibration and the Epistemological Role
of Bayesian Conditionalization&rdquo;, <em>Journal of Philosophy</em>,
96(6): 294&ndash;324. doi:10.2307/2564680</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Okasha on Inductive
Scepticism&rdquo;, <em>The Philosophical Quarterly</em>, 52(207):
226&ndash;232. doi:10.1111/1467-9213.00264</li>

<li>Laudan, Larry, 1997, &ldquo;How About Bust? Factoring Explanatory
Power Back into Theory Evaluation&rdquo;, <em>Philosophy of
Science</em>, 64(2): 206&ndash;216. doi:10.1086/392553</li>

<li>Lenhard Johannes, 2006, &ldquo;Models and Statistical Inference:
The Controversy Between Fisher and Neyman-Pearson&rdquo;, <em>British
Journal for the Philosophy of Science</em>, 57(1): 69&ndash;91.
doi:10.1093/bjps/axi152</li>

<li>Levi, Isaac, 1967, <em>Gambling with Truth: An Essay on Induction
and the Aims of Science</em>, New York: Knopf.</li>

<li>&ndash;&ndash;&ndash;, 1977, &ldquo;Direct Inference&rdquo;,
<em>Journal of Philosophy</em>, 74(1): 5&ndash;29.
doi:10.2307/2025732</li>

<li>&ndash;&ndash;&ndash;, 1978, &ldquo;Confirmational
Conditionalization&rdquo;, <em>Journal of Philosophy</em>, 75(12):
730&ndash;737. doi:10.2307/2025516</li>

<li>&ndash;&ndash;&ndash;, 1980, <em>The Enterprise of Knowledge: An
Essay on Knowledge, Credal Probability, and Chance</em>, Cambridge,
MA: MIT Press.</li>

<li>Lewis, David, 1980, &ldquo;A Subjectivist&rsquo;s Guide to
Objective Chance&rdquo;, in Richard C. Jeffrey, (ed.), <em>Studies in
Inductive Logic and Probability</em>, vol. 2, Berkeley: University of
California Press, 263&ndash;293.</li>

<li>Maher, Patrick, 1993, <em>Betting on Theories</em>, Cambridge:
Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 1996, &ldquo;Subjective and Objective
Confirmation&rdquo;, <em>Philosophy of Science</em>, 63(2):
149&ndash;174. doi:10.1086/289906</li>

<li>&ndash;&ndash;&ndash;, 1997, &ldquo;Depragmatized Dutch Book
Arguments&rdquo;, <em>Philosophy of Science</em>, 64(2):
291&ndash;305. doi:10.1086/392552</li>

<li>&ndash;&ndash;&ndash;, 1999, &ldquo;Inductive Logic and the Ravens
Paradox&rdquo;, <em>Philosophy of Science</em>, 66(1): 50&ndash;70.
doi:10.1086/392676</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;Probability Captures the Logic
of Scientific Confirmation&rdquo;, in Christopher Hitchcock (ed.),
<em>Contemporary Debates in Philosophy of Science</em>, Oxford:
Blackwell, 69&ndash;93.</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;Confirmation Theory&rdquo;,
<em>The Encyclopedia of Philosophy</em>, 2nd edition, Donald M.
Borchert (ed.), Detroit: Macmillan.</li>

<li>&ndash;&ndash;&ndash;, 2006a, &ldquo;The Concept of Inductive
Probability&rdquo;, <em>Erkenntnis</em>, 65(2): 185&ndash;206.
doi:10.1007/s10670-005-5087-5</li>

<li>&ndash;&ndash;&ndash;, 2006b, &ldquo;A Conception of Inductive
Logic&rdquo;, <em>Philosophy of Science</em>, 73(5): 513&ndash;523.
doi:10.1086/518321</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;Bayesian Probability&rdquo;,
<em>Synthese</em>, 172(1): 119&ndash;127.
doi:10.1007/s11229-009-9471-6</li>

<li>Mayo, Deborah G., 1996, <em>Error and the Growth of Experimental
Knowledge</em>, Chicago: University of Chicago Press.</li>

<li>&ndash;&ndash;&ndash;, 1997, &ldquo;Duhem&rsquo;s Problem, the
Bayesian Way, and Error Statistics, or &lsquo;What&rsquo;s Belief Got
to do with It?&rsquo;&rdquo;, <em>Philosophy of Science</em>, 64(2):
222&ndash;244. doi:10.1086/392549</li>

<li>Mayo Deborah and Aris Spanos, 2006, &ldquo;Severe Testing as a
Basic Concept in a Neyman-Pearson Philosophy of Induction&ldquo;,
<em>British Journal for the Philosophy of Science</em>, 57(2):
323&ndash;357. doi:10.1093/bjps/axl003</li>

<li>McGee, Vann, 1994, &ldquo;Learning the Impossible&rdquo;, in E.
Eells and B. Skyrms (eds.), <em>Probability and Conditionals: Belief
Revision and Rational Decision</em>, New York: Cambridge University
Press, 179&ndash;200.</li>

<li>McGrew, Timothy J., 2003, &ldquo;Confirmation, Heuristics, and
Explanatory Reasoning&rdquo;, <em>British Journal for the Philosophy
of Science</em>, 54: 553&ndash;567.</li>

<li>McGrew, Lydia and Timothy McGrew, 2008, &ldquo;Foundationalism,
Probability, and Mutual Support&rdquo;, <em>Erkenntnis</em>, 68(1):
55&ndash;77. doi:10.1007/s10670-007-9062-1</li>

<li>Neyman, Jerzy and E.S. Pearson, 1967, <em>Joint Statistical
Papers</em>, Cambridge: Cambridge University Press.</li>

<li>Norton, John D., 2003, &ldquo;A Material Theory of
Induction&rdquo;, <em>Philosophy of Science</em>, 70(4):
647&ndash;670. doi:10.1086/378858</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Probability
Disassembled&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 58(2): 141&ndash;171. doi:10.1093/bjps/axm009</li>

<li>Okasha, Samir, 2001, &ldquo;What Did Hume Really Show About
Induction?&rdquo;, <em>The Philosophical Quarterly</em>, 51(204):
307&ndash;327. doi:10.1111/1467-9213.00231</li>

<li>Popper, Karl, 1968, <em>The Logic of Scientific Discovery</em>,
3<sup>rd</sup> edition, London: Hutchinson.</li>

<li>Quine, W.V., 1953, &ldquo;Two Dogmas of Empiricism&rdquo;, in
<em>From a Logical Point of View</em>, New York: Harper Torchbooks.
Routledge Encyclopedia of Philosophy, Version 1.0, London:
Routledge</li>

<li>Ramsey, F.P., 1926, &ldquo;Truth and Probability&rdquo;, in
<em>Foundations of Mathematics and other Essays</em>, R.B. Braithwaite
(ed.), Routledge &amp; P. Kegan,1931, 156&ndash;198. Reprinted in
<em>Studies in Subjective Probability</em>, H. Kyburg and H. Smokler
(eds.), 2<sup>nd</sup> ed., R.E. Krieger Publishing Company, 1980,
23&ndash;52. Reprinted in <em>Philosophical Papers</em>, D.H. Mellor
(ed.), Cambridge: University Press, Cambridge, 1990,</li>

<li>Reichenbach, Hans, 1938, <em>Experience and Prediction: An
Analysis of the Foundations and the Structure of Knowledge</em>,
Chicago: University of Chicago Press.</li>

<li>R&eacute;nyi, Alfred, 1970, <em>Foundations of Probability</em>,
San Francisco, CA: Holden-Day.</li>

<li>Rosenkrantz, R.D., 1981, <em>Foundations and Applications of
Inductive Probability</em>, Atascadero, CA: Ridgeview Publishing.</li>

<li>Roush, Sherrilyn , 2004, &ldquo;Discussion Note: Positive
Relevance Defended&rdquo;, <em>Philosophy of Science</em>, 71(1):
110&ndash;116. doi:10.1086/381416</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Induction, Problem of&rdquo;,
Sarkar and Pfeifer 2006..</li>

<li>&ndash;&ndash;&ndash;, 2006, <em>Tracking Truth: Knowledge,
Evidence, and Science</em>, Oxford: Oxford University Press.</li>

<li>Royall, Richard M., 1997, <em>Statistical Evidence: A Likelihood
Paradigm</em>, New York: Chapman &amp; Hall/CRC.</li>

<li>Salmon, Wesley C., 1966, <em>The Foundations of Scientific
Inference</em>, Pittsburgh, PA: University of Pittsburgh Press.</li>

<li>&ndash;&ndash;&ndash;, 1975, &ldquo;Confirmation and
Relevance&rdquo;, in H. Feigl and G. Maxwell (eds.), <em>Induction,
Probability, and Confirmation</em>, (Minnesota Studies in the
Philosophy of Science, 6), Minneapolis: University of Minnesota Press,
3&ndash;36.</li>

<li>Sarkar, Sahotra and Jessica Pfeifer (eds.), 2006, <em>The
Philosophy of Science: An Encyclopedia</em>, 2 volumes, New York:
Routledge.</li>

<li>Savage, Leonard J., 1954, <em>The Foundations of Statistics</em>,
John Wiley (2nd ed., New York: Dover 1972).</li>

<li>Savage, Leonard J., et al., 1962, <em>The Foundations of
Statistical Inference</em>, London: Methuen.</li>

<li>Schlesinger, George N., 1991, <em>The Sweep of Probability</em>,
Notre Dame, IN: Notre Dame University Press.</li>

<li>Seidenfeld, Teddy, 1978, &ldquo;Direct Inference and Inverse
Inference&rdquo;, <em>Journal of Philosophy</em>, 75(12):
709&ndash;730. doi:10.2307/2025515</li>

<li>&ndash;&ndash;&ndash;, 1992, &ldquo;R.A. Fisher&rsquo;s Fiducial
Argument and Bayes&rsquo; Theorem&rdquo;, <em>Statistical
Science</em>, 7(3): 358&ndash;368. doi:10.1214/ss/1177011232</li>

<li>Shafer, Glenn, 1976, <em>A Mathematical Theory of Evidence</em>,
Princeton, NJ: Princeton University Press.</li>

<li>&ndash;&ndash;&ndash;, 1990, &ldquo;Perspectives on the Theory and
Practice of Belief Functions&rdquo;, <em>International Journal of
Approximate Reasoning</em>, 4(5&ndash;6): 323&ndash;362.
doi:10.1016/0888-613X(90)90012-Q</li>

<li>Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New
Haven, CT: Yale University Press.</li>

<li>&ndash;&ndash;&ndash;, 1990, <em>The Dynamics of Rational
Deliberation</em>, Cambridge, MA: Harvard University Press.</li>

<li>&ndash;&ndash;&ndash;, 2000, <em>Choice and Chance: An
Introduction to Inductive Logic</em>, 4<sup>th</sup> edition, Belmont,
CA: Wadsworth, Inc.</li>

<li>Sober, Elliott, 2002, &ldquo;Bayesianism&mdash;Its Scope and
Limits&rdquo;, in Swinburne 2002: 21&ndash;38.
doi:10.5871/bacad/9780197263419.003.0002</li>

<li>Spohn, Wolfgang, 1988, &ldquo;Ordinal Conditional Functions: A
Dynamic Theory of Epistemic States&rdquo;, in William L. Harper and
Brian Skyrms (eds.), <em>Causation in Decision, Belief Change, and
Statistics</em>, vol. 2, Dordrecht: Reidel, 105&ndash;134.
doi:10.1007/978-94-009-2865-7_6</li>

<li>Strevens, Michael, 2004, &ldquo;Bayesian Confirmation Theory:
Inductive Logic, or Mere Inductive Framework?&rdquo;
<em>Synthese</em>, 141(3): 365&ndash;379.
doi:10.1023/B:SYNT.0000044991.73791.f7</li>

<li>Suppes, Patrick, 2007, &ldquo;Where do Bayesian Priors Come
From?&rdquo; <em>Synthese</em>, 156(3): 441&ndash;471.
doi:10.1007/s11229-006-9133-x</li>

<li>Swinburne, Richard, 2002, <em>Bayes&rsquo; Theorem</em>, Oxford:
Oxford University Press. doi:10.5871/bacad/9780197263419.001.0001</li>

<li>Talbot, W., 2001, &ldquo;Bayesian Epistemology&rdquo;, in the
<em>Stanford Encyclopedia of Philosophy</em>, (Fall 2001 Edition),
Edward N. Zalta (ed.), URL =
 &lt;<a href="https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/" target="other">https://plato.stanford.edu/archives/fall2001/entries/epistemology-bayesian/</a>&gt;</li>
 
<li>Teller, Paul, 1976, &ldquo;Conditionalization, Observation, and
Change of Preference&rdquo;, in Harper and Hooker 1976: 205&ndash;259.
doi:10.1007/978-94-010-1853-1_9</li>

<li>Van Fraassen, Bas C., 1983, &ldquo;Calibration: A Frequency
Justification for Personal Probability &rdquo;, in R.S. Cohen and L.
Laudan (eds.), <em>Physics, Philosophy, and Psychoanalysis: Essays in
Honor of Adolf Grunbaum</em>, Dordrecht: Reidel.
doi:10.1007/978-94-009-7055-7_15</li>

<li>Venn, John, 1876, <em>The Logic of Chance</em>, 2<sup>nd</sup>
ed., Macmillan and co; reprinted, New York, 1962.</li>

<li>Vineberg, Susan, 2006, &ldquo;Dutch Book Argument&rdquo;, Sarkar
and Pfeifer 2006..</li>

<li>Vranas, Peter B.M., 2004, &ldquo;Hempel&rsquo;s Raven Paradox: A
Lacuna in the Standard Bayesian Solution&rdquo;, <em>British Journal
for the Philosophy of Science</em>, 55(3): 545&ndash;560.
doi:10.1093/bjps/55.3.545</li>

<li>Weatherson, Brian, 1999, &ldquo;Begging the Question and
Bayesianism&rdquo;, <em>Studies in History and Philosophy of Science
[Part A]</em>, 30(4): 687&ndash;697.
doi:10.1016/S0039-3681(99)00020-5</li>

<li>Williamson, Jon, 2007, &ldquo;Inductive Influence&rdquo;,
<em>British Journal for Philosophy of Science</em>, 58(4):
689&ndash;708. doi:10.1093/bjps/axm032</li>
</ul>

</div> 

<div id="academic-tools">
<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=logic-inductive" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/logic-inductive/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=logic-inductive&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/logic-inductive/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>
</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<ul>

 <li><a href="http://www.iep.utm.edu/c/conf-ind.htm" target="other">Confirmation and Induction</a>.
 Really nice overview by Franz Huber in the <em>Internet Encyclopedia
of Philosophy</em>.</li>

 <li><a href="http://fitelson.org/il.pdf" target="other">Inductive Logic</a>,
 (in PDF), by Branden Fitelson, <em>Philosophy of Science: An
Encyclopedia</em>, (J. Pfeifer and S. Sarkar, eds.), Routledge. An
extensive encyclopedia article on inductive logic.</li>

 <li><a href="http://www.ditext.com/clay/armendt2.html" target="other">Teaching Theory of Knowledge: Probability and Induction</a>.
 A very extensive outline of issues in Probability and Induction, each
topic accompanied by a list of relevant books and articles (without
links), compiled by Brad Armendt and Martin Curd.</li>

 <li><a href="http://homepages.wmich.edu/~mcgrew/confirm.htm" target="other">Probabilistic Confirmation Theory and Bayesian Reasoning</a>.
 An annotated bibliography of influential works compiled by Timothy
McGrew.</li>

 <li><a href="http://www.cs.ubc.ca/~murphyk/Bayes/Charniak_91.pdf" target="other">Bayesian Networks Without Tears</a>,
 (in PDF), by Eugene Charniak (Computer Science and Cognitive Science,
Brown University). An introductory article on Bayesian inference.</li>

 <li><a href="http://www.princeton.edu/~bayesway/" target="other">Miscellany of Works on Probabilistic Thinking</a>.
 A collection of on-line articles on Subjective Probability and
probabilistic reasoning by Richard Jeffrey and by several other
philosophers writing on related issues.</li>

 <li><a href="http://fitelson.org/confirmation/" target="other">Fitelson&rsquo;s course on Confirmation Theory</a>.
 Main page of Branden Fitelson&rsquo;s course on Confirmation Theory.
The
 <a href="http://fitelson.org/confirmation/syllabus.html" target="other">Syllabus</a>
 provides an extensive list of links to readings. The
 <a href="http://fitelson.org/confirmation/notes.html" target="other">Notes, Handouts, &amp; Links</a>
 page has Fitelson&rsquo;s weekly course notes and some links to
useful internet resources on confirmation theory.</li>

 <li><a href="http://fitelson.org/probability/" target="other">Fitelson&rsquo;s course on Probability and Induction</a>.
 Main page of Branden Fitelson&rsquo;s course on Probability and
Induction. The
 <a href="http://fitelson.org/probability/syllabus.html" target="other">Syllabus</a>
 provides an extensive list of links to readings on the subject. The
 <a href="http://fitelson.org/probability/notes.html" target="other">Notes &amp; Handouts</a>
 page has Fitelson&rsquo;s powerpoint slides for each of his lectures
and some links to handouts for the course. The
 <a href="http://fitelson.org/probability/links.html" target="other">Links</a>
 page contains links to some useful internet resources.</li>
</ul>

</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../bayes-theorem/">Bayes&rsquo; Theorem</a> |
 <a href="../formal-belief/">belief, formal representations of</a> |
 <a href="../carnap/">Carnap, Rudolf</a> |
 <a href="../confirmation/">confirmation</a> |
 <a href="../epistemology-bayesian/">epistemology: Bayesian</a> |
 <a href="../probability-interpret/">probability, interpretations of</a> |
 <a href="../statistics/">statistics, philosophy of</a>

</p>
</div>

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>
Thanks to Alan H&aacute;jek, Jim Joyce, and Edward Zalta for many
valuable comments and suggestions. The editors and author also thank
Greg Stokley and Philippe van Basshuysen for carefully reading an
earlier version of the entry and identifying a number of typographical
errors.</p>
</div>
<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2025</a> by

<br />
<a href="http://james-hawthorne.oucreate.com/" target="other">James Hawthorne</a>
&lt;<a href="m&#97;ilto:hawthorne&#37;40ou&#37;2eedu"><em>hawthorne<abbr title=" at ">&#64;</abbr>ou<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
            <li role="menuitem"><a href="../../new.html">What's New</a></li>
            <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li role="menuitem"><a href="../../published.html">Chronological</a></li>
            <li role="menuitem"><a href="../../archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
            <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
            <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
            <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
            <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
            <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
            <li role="menuitem"><a href="../../accessibility.html">Accessibility</a></li>
            <li role="menuitem"><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
            <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
            <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2025</a> by <a href="https://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>
</html>
