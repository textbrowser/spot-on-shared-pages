<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html lang="en"> <!-- <![endif]-->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Internet Research Ethics (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Internet Research Ethics" />
<meta property="citation_author" content="Buchanan, Elizabeth A." />
<meta property="citation_author" content="Zimmer, Michael" />
<meta property="citation_publication_date" content="2012/06/22" />
<meta name="DC.title" content="Internet Research Ethics" />
<meta name="DC.creator" content="Buchanan, Elizabeth A." />
<meta name="DC.creator" content="Zimmer, Michael" />
<meta name="DCTERMS.issued" content="2012-06-22" />
<meta name="DCTERMS.modified" content="2025-07-13" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
                    <li role="menuitem"><a href="../../new.html">What's New</a></li>
                    <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li role="menuitem"><a href="../../published.html">Chronological</a></li>
                    <li role="menuitem"><a href="../../archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
                    <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
                    <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
                    <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
                    <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
                    <li role="menuitem"><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
                    <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
                    <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit" aria-label="search"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/ethics-internet-research/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-internet-research">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Internet Research Ethics</h1><div id="pubinfo"><em>First published Fri Jun 22, 2012; substantive revision Sun Jul 13, 2025</em></div>

<div id="preamble">

<p>
There is little research that is not impacted in some way on or
through the Internet. The Internet, as a field, a tool, and a venue,
has specific and far-reaching ethical issues. Internet research ethics
is a subdiscipline that fits across many disciplines, ranging from
social sciences, arts and humanities, medical/biomedical, and natural
sciences. Extant ethical frameworks, including
 <a href="../consequentialism/">consequentialism</a>,
 <a href="../ethics-deontological/">deontology</a>,
 <a href="../ethics-virtue/">virtue ethics</a>,
 and
 <a href="../feminism-ethics/">feminist ethics</a>,
 have contributed to the ways in which ethical issues in Internet
research are considered and evaluated.</p>

<p>
Conceptually and historically, Internet research ethics is most
related to computer and information ethics and includes such ethical
issues as participant knowledge and consent, data privacy, security,
anonymity and confidentiality, and integrity of data, intellectual
property issues, and community, disciplinary, and professional
standards or norms. Throughout the Internet&rsquo;s evolution, there
has been continued debate whether there are new ethical dilemmas
emerging, or if the existing dilemmas are similar to dilemmas in other
research realms (Elgesem 2002; Walther 2002; Ess &amp; AoIR 2002;
Markham &amp; Buchanan 2012). These debates are similar to
philosophical debates in computer and information ethics. For example,
many years ago, James Moor (1985) asked &ldquo;what is special about
computers&rdquo; in order to understand what, if anything, is unique
ethically. Reminding us, however, that research itself must be guided
by ethical principles, regardless of technological intervention, van
Heerden et al. (2020) and Sloan et al. (2020) stress that the
&ldquo;fundamental principles of conducting ethical social research
remain the same&rdquo; (Ess &amp; AoIR 2002; King 1996; Samuel and
Buchanan, 2020).</p>

<p>
Yet, as the Internet has evolved into a more social and communicative
tool and venue, the ethical issues have shifted from purely
data-driven to more human-centered. &ldquo;On-ground&rdquo; or
face-to-face analogies, however, may not be applicable to online
research. For example, physical spaces like a public park have
traditionally served as settings where researchers could observe
public behavior with minimal ethical concern. In online environments,
however, distinctions between public and private are far more blurred:
visibility does not necessarily imply consent, and users may have
differing expectations of privacy even in open digital spaces (SACHRP
2013). Thus, some scholars suggest that the specificity of Internet
research ethics calls for new regulatory and/or professional and
disciplinary guidance. For these reasons, the concept of human
subjects research policies and regulation, informs this entry, which
will continue discussions around ethical and methodological
complexity, including personal identifiability, reputational risk and
harm, notions of public space and public text, ownership, and
longevity of data as they relate to Internet research. Specifically,
the emergence of the social web raises issues around subject or
participant recruitment practices, imperfect informed consent models,
and protection of various expectations and forms of privacy in an
ever-increasing world of diffused and ubiquitous technologies.
Additional ethical concerns center on issues of anonymity and
confidentiality of data in spaces where researchers and their subjects
may not fully understand the terms and conditions of those venues or
tools, challenges to data integrity as research projects can be
outsourced or crowdsourced to online labor marketplaces, and
jurisdictional issues as more research is processed, stored, and
disseminated via cloud computing or in remote server locales,
presenting myriad legal complexities given jurisdictional differences
in data laws. Further, the dominance of big data research has
continued across research spaces, with the notions of
&ldquo;real-world data&rdquo; and pervasive computing readily accepted
and used in all disciplines. The ease of access and availability to
use big data sets in myriad ways has enabled AI (artificial
intelligence) and ML (machine learning) to grow as standard tools for
researchers.</p>

<p>
As a result, researchers using the Internet as a tool for and/or a
space of research&mdash;and their research ethics boards (REBs), also
known as institutional review boards (IRBs) in the United States or
human research ethics committees (HRECs) in other countries such as
Australia&mdash;have been confronted with a series of novel ethical
questions: What ethical obligations do researchers have to protect the
privacy of research subjects engaging in activities in
&ldquo;public&rdquo; Internet spaces? What are such public spaces? Is
there any reasonable expectation of privacy in an era of pervasive and
ubiquitous surveillance and data tracking? How is confidentiality or
anonymity assured online? How is and should informed consent be
obtained online? Is deception (pretending to be someone you are not,
withholding identifiable information, etc.) an acceptable online norm
or a harm? How is &ldquo;harm&rdquo; possible to someone existing in
an online space? How identifiable are individuals in large data sets?
Do human subjects protections apply to big data and social media
datasets? As more industry-sponsored research takes place, what
ethical protections exist outside of current regulatory structures? As
new global regulatory frameworks, such as the EU&rsquo;s General Data
Protection Regulation (GDPR 2016), Digital Services Act (DSA 2022),
and Digital Markets Act (DMA 2022) are implemented, what are the
implications for balancing data collection, sharing, and access with
subject privacy and individual rights?</p>

<p>
A growing number of scholars have explored these and related questions
(see, for example, Bromseth 2002; Bruckman 2006; Buchanan 2004;
Buchanan &amp; Ess 2008; Johns, Chen &amp; Hall 2003; Kitchin 2003,
2008; King 1996; Mann 2003; Markham &amp; Baym 2008; McKee &amp;
Porter 2009; Thorseth 2003; Ess 2016; Zimmer &amp; Kinder-Kurlanda
(eds.) 2017; Samuel &amp; Buchanan, 2020), scholarly associations have
drafted ethical guidelines for Internet research (Ess &amp;
Association of Internet Researchers 2002; Markham, Buchanan, and AoIR
2012; Franzke et al., 2020; Kraut et al. 2004), and scientific
professional organizations such as AAAS (Frankel &amp; Siang 1999) and
NASEM (2022) are confronting the myriad of ethical concerns that
Internet and big data research poses to researchers and research
ethics boards (REBs). Emerging controversies around the increased use
of internet data for training machine learning and artificial
intelligence platforms, coupled with increased restrictions on access
to data by internet platforms, has prompted renewed attention to
internet research ethics by a broad range of communities and
stakeholders (see, for example, CITR 2022, NTIA 2024).</p>
</div> 

<div id="toc"><!--Entry Contents-->
<ul>
<li><a href="#Defi">1. Definitions</a></li>
<li><a href="#HumaSubjRese">2. Human Subjects Research</a></li>
<li><a href="#HistDeveIREDisc">3. History and Development of IRE as a Discipline</a></li>
<li><a href="#KeyEthiIssuInteRese">4. Key Ethical Issues in Internet Research</a>
   <ul>
   <li><a href="#Priv">4.1 Privacy</a></li>
   <li><a href="#Recr">4.2 Recruitment</a></li>
   <li><a href="#InfoCons">4.3 Informed Consent</a></li>
   <li><a href="#ClouBasePlatReseEthi">4.4 Cloud-Based Platforms and Research Ethics</a></li>
   <li><a href="#BigDataCons">4.5 Big Data Considerations</a></li>
   <li><a href="#InteReseInduEthi">4.6 Internet Research and Industry Ethics</a></li>
   </ul></li>
<li><a href="#Conc">5. Conclusion</a></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a>
   <ul>
   <li><a href="#CiteEntr">Cited in Entry</a></li>
   <li><a href="#LawsGoveDocu">Laws and Government Documents</a></li>
   <li><a href="#ProfStan">Professional Standards</a></li>
   <li><a href="#Jour">Journals</a></li>
   <li><a href="#OtheReso">Other Resources</a></li>
   </ul></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr /></div>

<div id="main-text">

<h2 id="Defi">1. Definitions</h2>

<p>
The commonly accepted definition of Internet research ethics (IRE) has
been used by Buchanan and Ess (2008, 2009), Buchanan (2011), and Ess
&amp; Association of Internet Researchers (AoIR) (2002):</p>

<blockquote>

<p>
<em>IRE</em> is defined as the analysis of ethical issues and
application of research ethics principles as they pertain to research
conducted on and in the Internet. Internet-based research, broadly
defined, is research which utilizes the Internet to collect
information through an online tool, such as an online survey; studies
about how people use the Internet, e.g., through collecting data
and/or examining activities in or on any online environments; and/or,
uses of online datasets, databases, or repositories.</p>
</blockquote>

<p>
These examples were broadened in 2013 by the United States
Secretary&rsquo;s Advisory Committee to the Office for Human Research
Protections (SACHRP 2013), and included under the umbrella term
Internet Research:</p>

<ul class="jfy">

<li>Research studying information that is already available on or via
the Internet without direct interaction with human subjects
(harvesting, mining, profiling, scraping, observation or recording of
otherwise-existing data sets, chat room interactions, blogs, social
media postings, etc.)</li>

<li>Research that uses the Internet as a vehicle for recruiting or
interacting, directly or indirectly, with subjects (Self-testing
websites, survey tools, Amazon Mechanical Turk, etc.)</li>

<li>Research about the Internet itself and its effects (use patterns
or effects of social media, search engines, email, etc.; evolution of
privacy issues; information contagion; etc.)</li>

<li>Research about Internet users: what they do, and how the Internet
affects individuals and their behaviors Research that utilizes the
Internet as an interventional tool, for example, interventions that
influence subjects&rsquo; behavior</li>

<li>Others (emerging and cross-platform types of research and methods,
including m-research (mobile))</li>

<li>Recruitment in or through Internet locales or tools, for example
social media, push technologies</li>
</ul>

<p>
A critical distinction in the definition of Internet research ethics
is that between the Internet as a research tool versus a research
venue. The distinction between tool and venue plays out across
disciplinary and methodological orientations. As a tool, Internet
research is enabled by search engines, data aggregators, digital
archives, application programming interfaces (APIs), online survey
platforms, and crowdsourcing platforms. Internet-based research venues
include such spaces as conversation applications (instant messaging
and discussion forums, for example), online multiplayer games, blogs
and interactive websites, and social networking platforms.</p>

<p>
Another way of conceptualizing the distinction between tool and venue
comes from Kitchin (2008), who has referred to a distinction in
Internet research using the concepts of &ldquo;engaged web-based
research&rdquo; versus &ldquo;non-intrusive web-based
research:&rdquo;</p>

<blockquote>

<p>
Non-intrusive analyses refer to techniques of data collection that do
not interrupt the naturally occurring state of the site or
cybercommunity, or interfere with premanufactured text. Conversely,
engaged analyses reach into the site or community and thus engage the
participants of the web source (2008: 15).</p>
</blockquote>

<p>
These two constructs provide researchers with a way of recognizing
when considering of human subject protections might need to occur.
McKee and Porter (2009), as well as Banks and Eble (2007) provide
guidance on the continuum of human-subjects research, noting a
distinction between person-based versus text-based. For example, McKee
and Porter provide a range of research variables (public/private,
topic sensitivity, degree of interaction, and subject vulnerability)
which are useful in determining where on the continuum of text-based
versus how person-based the research is, and whether or not subjects
would need to consent to the research (2009: 87&ndash;88).</p>

<p>
While conceptually useful for determining human subjects
participation, the distinction between tool and venue or engaged
versus non-intrusive web-based research is increasingly blurring in
the face of social media and their third-party applications. Buchanan
(2016) has conceptualized three phases of Internet research, starting
in the late 1990s when the Internet was used as a tool for research,
and later the emergence of social media characterizes the second
phase, circa 2006&ndash;2014. The concept of social media entails</p>

<blockquote>

<p>
A group of Internet-based applications that build on the ideological
and technological foundations of Web 2.0, and that allow the creation
and exchange of user-generated content (Kaplan &amp; Haenlein 2010:
61).</p>
</blockquote>

<p>
A &ldquo;social network site&rdquo; is a category of websites with
profiles, semi-persistent public commentary on the profile, and a
traversable publicly articulated social network displayed in relation
to the profile.</p>

<p>
This collapse of tool and venue can be traced primarily to the
increasing use of third-party sites and applications such as Facebook,
X/Twitter, or any of the myriad online platforms where subject or
participant recruitment, data collection, data analysis, and data
dissemination can all occur in the same space. With these collapsing
boundaries, the terms of &ldquo;inter-jurisdictional
coordination&rdquo; (Gilbert 2009: 3) are inherently challenging;
Gilbert has specifically argued against the terms of use or end-user
license agreement stipulations in virtual worlds, noting that such
agreements are often &ldquo;flawed&rdquo;, as they rely on laws and
regulations from a specific locale and attempt to enforce them in a
non place-based environment. Nonetheless, researchers now make
frequent use of data aggregation tools, scraping data from user
profiles or transaction logs, harvesting data from social media
streams, or storing data on cloud servers only after agreeing to the
terms of service that go along with those sites. The use of such
third-party applications or tools changes fundamental aspects of
research, oftentimes displacing the researcher or research team as the
sole owner of their data. These unique characteristics implicate
concepts and practicalities of privacy, consent, ownership, and
jurisdictional boundaries.</p>

<p>
The definition of Internet research, thus, has expanded significantly
beyond studying online communities or content. It now includes the
collection and analysis of data generated through internet-connected
devices, such as smartphones, wearables, and IoT technologies, which
produce continuous streams of behavioral, biometric, and location
data. Researchers increasingly access social media and platform data
via APIs, enabling large-scale, automated studies of online
interactions and trends. Internet research also encompasses the
analysis of digital infrastructures&mdash;like algorithms, recommender
systems, and moderation tools&mdash;as objects of study. And internet
research might also include the widespread collection and analysis of
large-scale datasets used to train machine learning and artificial
intelligence models. These shifts reflect a broader turn toward big
data and computational methods, raising new ethical questions about
consent, privacy, and data governance in increasingly pervasive and
often invisible digital environments.</p>

<h2 id="HumaSubjRese">2. Human Subjects Research</h2>

<p>
The practical, professional, and theoretical implications of human
subjects protections has been covered extensively in scholarly
literature, ranging from medical/biomedical to social sciences to
computing and technical disciplines (see Beauchamp &amp; Childress
2008; Emanual et al. 2003; PRIM&amp;R et al. 2021; Sieber 1992; Wright
2006). Relevant protections and regulations continue to receive much
attention in the face of research ethics violations (see, for example,
Skloot 2010, on Henrietta Lacks; the U.S. Government&rsquo;s admission
and apology to the Guatemalan Government for STD testing in the 1940s
(BBC 2011); and Gaw &amp; Burns 2011, on how lessons from the past
might inform current research ethics and conduct).</p>

<p>
The history of human subjects protections (Resnik &amp; Hofweber 2025
 [<a href="#Oth">Other Internet Resources</a>])
 grew out of atrocities such as Nazi human experimentation during
World War II, which resulted in the Nuremberg Code (1947);
subsequently followed by the Declaration of Helsinki on Ethical
Principles for Medical Research Involving Human Subjects (World
Medical Association 1964/2008). Partially in response to the Tuskegee
syphilis experiment, an infamous clinical study conducted between 1932
and 1972 by the U.S. Public Health Service studying the natural
progression of untreated syphilis in rural African-American men in
Alabama under the guise of receiving free health care from the
government, the U.S. Department of Health and Human Services put forth
a set of basic regulations governing the protection of human subjects
(45 C.F.R. &sect; 46) (see the links in the Other Internet Resources
section, under Laws and Government Documents). This was later followed
by the publication of the &ldquo;Ethical Principles and Guidelines for
the Protection of Human Subjects of Research&rdquo; by the National
Commission for the Protection of Human Subjects of Biomedical and
Behavioral Research, known as the Belmont Report (NCPHSBBR 1979). The
Belmont Report identifies three fundamental ethical principles for all
human subjects research: Respect for Persons, Beneficence, and
Justice.</p>

<p>
To ensure consistency across federal agencies in the United States
context in human subjects protections, in 1991, the Federal Policy for
the Protection of Human Subjects, also known as the &ldquo;Common
Rule&rdquo; was codified; the Revised Common Rule was released in the
Federal Register on 19 January 2017, and went into effect 19 July
2018. Similar regulatory frameworks for the protection of human
subjects exist across the world, and include, for example, the
Canadian Tri-Council, the Australian Research Council, The European
Commission, The Research Council of Norway and its National Committee
for Research Ethics in the Social Sciences and Humanities (NESH 2006;
NESH 2019), and the U.K.&rsquo;s NHS National Research Ethics Service
and the Research Ethics Framework (REF) of the ESRC (Economic and
Social Research Council) General Guidelines, and the Forum for Ethical
Review Committees in Asia and the Western Pacific (FERCAP).</p>

<p>
In the United States, the various regulatory agencies who have signed
on to the Common Rule (45 C.F.R. 46 Subpart A) have not issued formal
guidance on Internet research (see the links in the Other Internet
Resources section, under Laws and Government Documents). The Preamble
to the Revised Rule referenced significant changes in the research
environment, recognizing a need to broaden the scope of the Rule.
However, substantial changes to the actual Rule in regards to Internet
research in its broadest context, were minimal.</p>

<p>
For example, the Preamble states:</p>

<blockquote>

<p>
This final rule recognizes that in the past two decades a paradigm
shift has occurred in how research is conducted. Evolving
technologies&mdash;including imaging, mobile technologies, and the
growth in computing power&mdash;have changed the scale and nature of
information collected in many disciplines. Computer scientists,
engineers, and social scientists are developing techniques to
integrate different types of data so they can be combined, mined,
analyzed, and shared. The advent of sophisticated computer software
programs, the Internet, and mobile technology has created new areas of
research activity, particularly within the social and behavioral
sciences (Federal Register 2017 and HHS 2017).</p>
</blockquote>

<p>
Modest changes to the definition of human subjects included changing
&ldquo;data&rdquo; to &ldquo;information&rdquo; and
&ldquo;biospecimens;&rdquo; the definition now reads:</p>

<div class="indent">
(e)

<dl class="sentag tag2em">
<dt>(1)</dt>
<dd><strong>Human subject</strong> means a living individual about
whom an investigator (whether professional or student) conducting
research:

<dl class="sentag tag2em">
<dt>(i)</dt>
<dd>Obtains information or biospecimens through intervention or
interaction with the individual, and uses, studies, or analyzes the
information or biospecimens; or</dd>
<dt>(ii)</dt>
<dd>Obtains, uses, studies, analyzes, or generates identifiable
private information or identifiable biospecimens.</dd>
</dl> </dd>
<dt>(2)</dt>
<dd><strong>Intervention</strong> includes both physical procedures by
which information or biospecimens are gathered (e.g., venipuncture)
and manipulations of the subject or the subject&rsquo;s environment
that are performed for research purposes.</dd>
<dt>(3)</dt>
<dd><strong>Interaction</strong> includes communication or
interpersonal contact between investigator and subject.</dd>
<dt>(4)</dt>
<dd><strong>Private information</strong> includes information about
behavior that occurs in a context in which an individual can
reasonably expect that no observation or recording is taking place,
and information that has been provided for specific purposes by an
individual and that the individual can reasonably expect will not be
made public (e.g., a medical record).</dd>
<dt>(5)</dt>
<dd><strong>Identifiable private information</strong> is private
information for which the identity of the subject is or may readily be
ascertained by the investigator or associated with the
information.</dd>
<dt>(6)</dt>
<dd><strong>An identifiable biospecimen</strong> is a biospecimen for
which the identity of the subject is or may readily be ascertained by
the investigator or associated with the biospecimen (45 C.F.R. &sect;
46.102 (2018)).</dd>
</dl>
</div>

<p>
However, the Revised Rule does have a provision that stands to be of
import in regards to Internet research; the Rule calls for
implementing departments or agencies to,</p>

<div class="indent">
[(e)(7)]

<dl class="sentag tag2em">
<dt>(i)</dt>
<dd>Upon consultation with appropriate experts (including experts in
data matching and re-identification), reexamine the meaning of
&ldquo;identifiable private information&rdquo;, as defined in
paragraph (e)(5) of this section, and &ldquo;identifiable
biospecimen&rdquo;, as defined in paragraph (e)(6) of this section.
This reexamination shall take place within 1 year and regularly
thereafter (at least every 4 years). This process will be conducted by
collaboration among the Federal departments and agencies implementing
this policy. If appropriate and permitted by law, such Federal
departments and agencies may alter the interpretation of these terms,
including through the use of guidance.</dd>
<dt>(ii)</dt>
<dd>Upon consultation with appropriate experts, assess whether there
are analytic technologies or techniques that should be considered by
investigators to generate &ldquo;identifiable private
information&rdquo;, as defined in paragraph (e)(5) of this section, or
an &ldquo;identifiable biospecimen&rdquo;, as defined in paragraph
(e)(6) of this section. This assessment shall take place within 1 year
and regularly thereafter (at least every 4 years). This process will
be conducted by collaboration among the Federal departments and
agencies implementing this policy. Any such technologies or techniques
will be included on a list of technologies or techniques that produce
identifiable private information or identifiable biospecimens. This
list will be published in the Federal Register after notice and an
opportunity for public comment. The Secretary, HHS, shall maintain the
list on a publicly accessible Web site (45 C.F.R. &sect; 46.102
(2018)).</dd>
</dl>
</div>

<p>
As of this writing, there has not yet been a reexamination of the
concepts of &ldquo;identifiable private information&rdquo; or
&ldquo;identifiable biospecimens&rdquo;. However, as data analytics,
AI, and machine learning continue to expose ethical issues in human
subjects research, we expect to see engaged discussion at the federal
level and amongst research communities (PRIM&amp;R 2021). Those
discussions may refer to previous conceptual work by Carpenter and
Dittrich (2012) and Aycock et al. (2012) that is concerned with risk
and identifiability. Secondary uses of identifiable, private data, for
example, may pose downstream harms, or unintentional risks, causing
reputational or informational harms. Reexaminations of
&ldquo;identifiable private information&rdquo; can not occur without
serious consideration of risk and &ldquo;human harming
research&rdquo;. Carpenter and Dittrich (2012) encourage</p>

<blockquote>

<p>
&ldquo;Review boards should transition from an informed consent driven
review to a risk analysis review that addresses potential harms
stemming from research in which a researcher does not directly
interact with the at-risk individuals&rdquo; (p. 4) as &ldquo;[T]his
distance between researcher and affected individual indicates that a
paradigm shift is necessary in the research arena. We must transition
our idea of research protection from &lsquo;human subjects
research&rsquo; to &lsquo;human harming research&rsquo;&rdquo; (p.
 14).<sup>[<a href="notes.html#note-1" id="ref-1">1</a>]</sup></p>
 </blockquote>

<p>
Similarly, Aycock et al. (2012) assert that</p>

<blockquote>

<p>
Researchers and boards must balance presenting risks related to the
specific research with risks related to the technologies in use. With
computer security research, major issues around risk arise, for
society at large especially. The risk may not seem evident to an
individual but in the scope of security research, larger populations
may be vulnerable. There is a significant difficulty in quantifying
risks and benefits, in the traditional sense of research
ethics&hellip;.An aggregation of surfing behaviors collected by a bot
presents greater distance between researcher and respondent than an
interview done in a virtual world between avatars. This distance leads
us to suggest that computer security research focus less concern
around <em>human subjects research</em> in the traditional sense and
more concern with <em>human harming research</em> (p. 3, italics
original).</p>
</blockquote>

<p>
These two conceptual notions are relevant for considering emergent
forms of identities or personally identifiable information (PII) such
as avatars, virtual beings, bots, textual and graphical information.
Within the Code of Federal Regulations (45 C.F.R. &sect; 46.102(f)
2009): New forms of representations are considered human subjects if
PII about living individuals is obtained. PII can be obtained by
researchers through scraping data sources, profiles or avatars, or
other pieces of data made available by the platform. Fairfield agrees:
&ldquo;An avatar, for example, does not merely represent a collection
of pixels&mdash;it represents the identity of the user&rdquo; (2012:
701).</p>

<p>
The multiple academic disciplines already long engaged in human
subjects research (medicine, sociology, anthropology, psychology,
communication) have established ethical guidelines intended to assist
researchers and those charged with ensuring that research on human
subjects follows both legal requirements and ethical practices. But
with research involving the Internet&mdash;where individuals
increasingly share personal information on platforms with porous and
shifting boundaries, where both the spread and aggregation of data
from disparate sources has become the norm, and where web-based
services, and their privacy policies and terms of service statements,
morph and evolve rapidly&mdash;the ethical frameworks and assumptions
traditionally used by researchers and REBs are frequently
challenged.</p>

<p>
Research ethics boards themselves are increasingly challenged with the
unique ethical dimensions of internet-based research protocols. In a
2008 survey of U.S. IRBs, less than half of the ethical review boards
identified internet-based research was &ldquo;an area of concern or
importance&rdquo; at that time, and only 6% had guidelines or
checklists in place for reviewing internet-based research protocols
(Buchanan &amp; Ess 2009). By 2015, 93% of IRBs surveyed acknowledged
that are ethical issues unique to research using &ldquo;online
data&rdquo;, yet only 55% said they felt their IRBs are well versed in
the technical aspects of online data collection, and only 57% agreed
that their IRB has the expertise to stay abreast of changes in online
technology. IRBs are now further challenged with the growth of big
data research (see
 <a href="#BigDataCons">&sect;4.5 below</a>),
 which increasingly relies on large datasets of personal information
generated via social media, digital devices, or other means often
hidden from users. A 2019 study of IRBs at 77 U.S. institutions
revealed only 25% felt prepared to evaluate protocols relying on big
data, and only 6% had tools sufficient for considering this emerging
area of internet research (Zimmer &amp; Chapman 2020). Further, after
being presented various hypothetical research scenarios utilizing big
data and asked how their IRB would likely review such a protocol,
numerous viewpoints different strongly in many cases. Consider the
following scenario:</p>

<blockquote>

<p>
Researchers plan to scrape public comments from online newspaper pages
to predict election outcomes. They will aggregate their analysis to
determine public sentiment. The researchers don&rsquo;t plan to inform
commenters, and they plan to collect potentially-identifiable user
names. Scraping comments violates the newspaper&rsquo;s terms of
service.</p>
</blockquote>

<p>
18% of respondents indicated their IRB would view this as exempt, 21%
indicated expedited review, 33% suggested it would need full board
review, while 28% did not think this was even human subjects research
that would fall under their IRB&rsquo;s purview (Zimmer &amp; Chapman
2020). This points to potential gaps and inconsistencies in how IRBs
review the ethical implications of big data research protocols.</p>

<p>
As research protocols increasingly include data collected through
individuals&rsquo; digital interactions&mdash;such as social media
activity, wearable device data, and online behaviors&mdash;without
direct engagement with the individuals themselves, a more nuanced
understanding of what constitutes human subjects research is emerging
that recognizes how individuals can be impacted by studies even
without direct participation or awareness (Shilton et al. 2021;
Fiesler et al. 2024).</p>

<h2 id="HistDeveIREDisc">3. History and Development of IRE as a Discipline</h2>

<p>
An extensive body of literature has developed since the 1990s around
the use of the Internet for research (S. Jones 1999; Hunsinger,
Klastrup, &amp; Allen (eds.) 2010; Consalvo &amp; Ess (eds.) 2011;
Zimmer &amp; Kinder-Kurlanda (eds.) 2017), with a growing emphasis on
the ethical dimensions of Internet research.</p>

<p>
A flurry of Internet research, and explicit concern for the ethical
issues concurrently at play in it, began in the mid-1990s. In 1996,
Storm King recognized the growing use of the Internet as a venue for
research. His work explored the American Psychological
Association&rsquo;s guidelines for human subjects research with
emergent forms of email, chat, listservs, and virtual communities.
With careful attention to risk and benefit to Internet subjects, King
offered a cautionary note:</p>

<blockquote>

<p>
When a field of study is new, the fine points of ethical
considerations involved are undefined. As the field matures and
results are compiled, researchers often review earlier studies and
become concerned because of the apparent disregard for the human
subjects involved (1996: 119).</p>
</blockquote>

<p>
The 1996 issue of <em>Information Society</em> dedicated to Internet
research is considered a watershed moment, and included much seminal
research still of impact and relevance today (Allen 1996; Boehlefeld
1996; Reid 1996).</p>

<p>
Sherry Turkle&rsquo;s 1997 <em>Life on the Screen: Identity in the Age
of the Internet</em> called direct attention to the human element of
online game environments. Moving squarely towards person-based versus
text-based research, Turkle pushed researchers to consider human
subjects implications of Internet research. Similarly, Markham&rsquo;s
<em>Life Online: Researching Real Experience in Virtual Space</em>
(1998) highlighted the methodological complexities of online
ethnographic studies, as did Jacobson&rsquo;s 1999 methodological
treatment of Internet research. The &ldquo;field&rdquo; of study
changed the dynamics of researcher-researched roles, identity, and
representation of participants from virtual spaces. Markham&rsquo;s
work in qualitative online research has been influential across
disciplines, as research in nursing, psychology, and medicine has
found the potential of this paradigm for online research (Flicker et
al. 2004; Eysenbach &amp; Till 2001; Seaboldt &amp; Kupier 1997; Sharf
1997).</p>

<p>
Then, in 1999, the American Association for the Advancement of Science
(AAAS), with a contract from the U.S. Office for Protection from
Research Risks (now known as the Office for Human Research
Protections), convened a workshop, with the goal of assessing the
alignment of traditional research ethics concepts to Internet
research. The workshop acknowledged</p>

<blockquote>

<p>
The vast amount of social and behavioral information potentially
available on the Internet has made it a prime target for researchers
wishing to study the dynamics of human interactions and their
consequences in this virtual medium. Researchers can potentially
collect data from widely dispersed population sat relatively low cost
and in less time than similar efforts in the physical world. As a
result, there has been an increase in the number of Internet studies,
ranging from surveys to naturalistic observation (Frankel &amp; Siang
1999: 1).</p>
</blockquote>

<p>
In the medical/biomedical contexts, Internet research has grown
rapidly. Also in 1999, Gunther Eysenbach wrote the first editorial to
the newly formed <em>Journal of Medical Internet Research</em>. There
were three driving forces behind the inception of this journal, and
Eysenbach called attention to the growing social and interpersonal
aspects of the Internet:</p>

<blockquote>

<p>
First, Internet protocols are used for clinical information and
communication. In the future, Internet technology will be the platform
for many telemedical applications. Second, the Internet revolutionizes
the gathering, access and dissemination of non-clinical information in
medicine: Bibliographic and factual databases are now world-wide
accessible via graphical user interfaces, epidemiological and public
health information can be gathered using the Internet, and
increasingly the Internet is used for interactive medical education
applications. Third, the Internet plays an important role for consumer
health education, health promotion and teleprevention. (As an aside,
it should be emphasized that &ldquo;health education&rdquo; on the
Internet goes beyond the traditional model of health education, where
a medical professional teaches the patient: On the Internet, much
&ldquo;health education&rdquo; is done
&ldquo;consumer-to-consumer&rdquo; by means of patient self support
groups organizing in cyberspace. These patient-to-patient interchanges
are becoming an important part of healthcare and are redefining the
traditional model of preventive medicine and health promotion).</p>
</blockquote>

<p>
With scholarly attention growing and with the 1999 AAAS report
(Frankel &amp; Siang 1999) calling for action, other professional
associations took notice and began drafting statements or guidelines,
or addendum to their extant professional standards. For example, The
Board of Scientific Affairs (BSA) of the American Psychological
Association established an Advisory Group on Conducting Research on
the Internet in 2001; the American Counseling Association&rsquo;s 2005
revision to its Code of Ethics; the Association of Internet
Researchers (AoIR) Ethics Working Group Guidelines, the National
Committee for Research Ethics in the Social Sciences and the
Humanities (NESH Norway), among others, have directed researchers and
review boards to the ethics of Internet research, with attention to
the most common areas of ethical concern (see
 <a href="#Oth">Other Internet Resources</a>
 for links).</p>

<p>
While many researchers focus on traditional research ethics
principles, conceptualizations of Internet research ethics depend on
disciplinary perspectives. Some disciplines, notably from the arts and
humanities, posit that Internet research is more about context and
representation than about &ldquo;human subjects&rdquo;, suggesting
there is no intent, and thus minimal or no harm, to engage in research
about actual persons. The debate has continued since the early 2000s.
White (2002) argued against extant regulations that favored or
privileged specific ideological, disciplinary and cultural
prerogatives, which limit the freedoms and creativity of arts and
humanities research. For example, she notes that the AAAS report
&ldquo;confuses physical individuals with constructed materials and
human subjects with composite cultural works&rdquo;, again calling
attention to the person versus text divide that has permeated Internet
research ethics debates. Another example of disciplinary differences
comes from the Oral History Association, which acknowledged the
growing use of the Internet as a site for research:</p>

<blockquote>

<p>
Simply put, oral history collects memories and personal commentaries
of historical significance through recorded interviews. An oral
history interview generally consists of a well-prepared interviewer
questioning an interviewee and recording their exchange in audio or
video format. Recordings of the interview are transcribed, summarized,
or indexed and then placed in a library or archives. These interviews
may be used for research or excerpted in a publication, radio or video
documentary, museum exhibition, dramatization or other form of public
presentation. Recordings, transcripts, catalogs, photographs and
related documentary materials can also be posted on the Internet
(Ritchie 2003: 19).</p>
</blockquote>

<p>
While the American Historical Association (A. Jones 2008) has argued
that such research be &ldquo;explicitly exempted&rdquo; from ethical
review board oversight, the use of the Internet could complicate such
a stance if such data became available in public settings or available
&ldquo;downstream&rdquo; with potential, unforeseeable risks to
reputation, economic standing, or psychological harm, should
identification occur.</p>

<p>
Under the concept of text rather than human subjects, Internet
research rests on arguments of publication and copyright; consider the
venue of a blog, which does not meet the definition of human subject
as in 45 C.F.R. &sect; 46.102f (2009), as interpreted by most ethical
review boards. A researcher need not obtain consent to use text from
an open blog, as it is generally considered publicly available,
textual, published material. This argument of the &ldquo;public
park&rdquo; analogy that has been generally accepted by researchers is
appropriate for some Internet venues and tools, but not all: Context,
intent, sensitivity of data, and expectations of Internet participants
were identified in 2004 by Sveninngsson as crucial markers in Internet
research ethics considerations.</p>

<p>
By the mid-2000s, with three major anthologies published, and a
growing literature base, there was ample scholarly literature
documenting IRE across disciplines and methodologies, and
subsequently, there was anecdotal data emerging from the review boards
evaluating such research. In search of empirical data regarding the
actual review board processes of Internet research from a human
subjects perspective, Buchanan and Ess surveyed over 700 United States
ethics review boards, and found that boards were primarily concerned
with privacy, data security and confidentiality, and ensuring
appropriate informed consent and recruitment procedures (Buchanan
&amp; Ess 2009; Buchanan &amp; Hvizdak 2009).</p>

<p>
In 2008, the Canadian Tri-Council&rsquo;s Social Sciences and
Humanities Research Ethics Special Working Committee: A Working
Committee of the Interagency Advisory Panel on Research Ethics was
convened (Blackstone et al. 2008); and in 2010, a meeting at the
Secretary&rsquo;s Advisory Committee to the Office for Human Research
Protections highlighted Internet research (SACHRP 2010). Such
prominent professional organizations as the Public Responsibility in
Medicine and Research (PRIM&amp;R) and the American Educational
Research Association (AERA) have begun featuring Internet research
ethics regularly at their conferences and related publications.</p>

<p>
Increasingly, disciplines not traditionally involved in human subjects
research have begun their own explorations of IRE. For example,
researchers in computer security have actively examined the tenets of
research ethics in CS and ICT (Aycock et al. 2012; Dittrich, Bailey,
&amp; Dietrich 2011; Carpenter &amp; Dittrich 2012; Buchanan et al.
2011). The U.S. Federal Register requested comments on &ldquo;The
Menlo Report&rdquo; in December 2011, calling for a commitment by
computer science researchers to the three principles of respect for
persons, beneficence, and justice, while also adding a fourth
principle on respect for law and public interest (Homeland Security
2011). SIGCHI, an international society for professionals, academics,
and students interested in human-technology and human-computer
interaction (HCI), has increasingly focused on how IRE applies to work
in their domain (Frauenberger et al. 2017; Fiesler et al. 2022).
Further, SIGCHI, along with other computational-oriented venues such
as the Conference on Neural Information Processing Systems (NeurIPS),
have started incorporating the consideration of research ethics into
their submission requirements and peer review processes (Ashurst et
al. 2022). Reflecting this increased focus within computational
domains, the National Science Foundation (NSF) launched a
cross-directorate program supporting research and training efforts on
&ldquo;Ethical and Responsible Research&rdquo; (ER2) in 2019
(Bauchspies et al. 2023).</p>

<h2 id="KeyEthiIssuInteRese">4. Key Ethical Issues in Internet Research</h2>

<h3 id="Priv">4.1 Privacy</h3>

<p>
Principles of research ethics dictate that researchers must ensure
there are adequate provisions to protect the privacy of research
subjects and to maintain the confidentiality of any data collected. A
violation of privacy or breach of confidentiality presents a risk of
serious harm to participants, ranging from the exposure of personal or
sensitive information, the divulgence of embarrassing or illegal
conduct, or the release of data otherwise protected under law.</p>

<p>
Research ethics concerns around individual privacy is often expressed
in terms of the level of linkability of data to individuals, and the
potential harms from disclosure of information. As Internet research
has grown in complexity and computational sophistication, ethics
concerns have focused on current and future uses of data, and the
potential downstream harms that could occur. Protecting research
participants&rsquo; privacy and confidentiality is typically achieved
through a combination of research tactics and practices, including
engaging in data collection under controlled or anonymous
environments, the scrubbing of data to remove personally identifiable
information (PII), or the use of access restrictions and related data
security methods. And, the specificity and characteristics of the data
will often dictate if there are regulatory considerations, in addition
to the methodological considerations around privacy and
confidentiality. For example, personally identifiable information
(PII) typically demands the most stringent protections. The National
Institutes of Health (NIH), for example, defines PII as:</p>

<blockquote>

<p>
any information about an individual maintained by an agency,
including, but not limited to, education, financial transactions,
medical history, and criminal or employment history and information
which can be used to distinguish or trace an individual&rsquo;s
identity, such as their name, SSN, date and place of birth,
mother&rsquo;s maiden name, biometric records, etc., including any
other personal information that is linked or linkable to an individual
(NIH 2010).</p>
</blockquote>

<p>
Typically, examples of identifying pieces of information have included
personal characteristics (such as date of birth, place of birth,
mother&rsquo;s maiden name, gender, sexual orientation, and other
distinguishing features and biometrics information, such as height,
weight, physical appearance, fingerprints, DNA and retinal scans),
unique numbers or identifiers assigned to an individual (such as a
name, address, phone number, social security number, driver&rsquo;s
license number, financial account numbers), and descriptions of
physical location (GIS/GPS log data, electronic bracelet monitoring
information).</p>

<p>
The 2018 EU General Data Protection Regulation lays out the legal and
regulatory requirements for data use across the EU. Mondschein &amp;
Monda (2018) provides a thorough discussion on the different types of
data that are considered in the GDPR: Personal data, such as names,
identification numbers, location data, and so on; Special categories
of personal data, such as race or ethic origin, political opinions, or
religious beliefs; Pseudonymous data, referring to data that has been
altered so the subject cannot be directly identified without having
further information; Anonymous data, information which does not relate
to an identifiable natural person or to personal data rendered
anonymous in such a manner that the data subject is not or no longer
identifiable. They also advise researchers to consider</p>

<blockquote>

<p>
data protection issues at an early stage of a research project is of
great importance specifically in the context of large-scale research
endeavours that make use of personal data (2018: 56).</p>
</blockquote>

<p>
Internet research introduces new complications to these longstanding
definitions and regulatory frameworks intended to protect subject
privacy. For example, researchers increasingly are able to collect
detailed data about individuals from sources such as Facebook,
X/Twitter, blogs or public email archives, and these rich data sets
can more easily be processed, compared, and combined with other data
(and datasets) available online. In numerous cases, both researchers
and members of the general public have been able to re-identify
individuals by analyzing and comparing such datasets, using
data-fields as benign as one&rsquo;s zip code (Sweeney 2002), random
Web search queries (Barbaro &amp; Zeller 2006), or movie ratings
(Narayanan &amp; Shmatikov 2008) as the vital key for reidentification
of a presumed anonymous user. Prior to widespread Internet-based data
collection and processing, few would have considered one&rsquo;s movie
ratings or zipcode as personally-identifiable. Yet, these cases reveal
that merely stripping traditional &ldquo;identifiable&rdquo;
information such as a subject&rsquo;s name, address, or social
security number is no longer sufficient to ensure data remains
anonymous (Ohm 2010), and requires the reconsideration of what is
considered &ldquo;personally identifiable information&rdquo; (Schwartz
&amp; Solove 2011). This points to the critical distinction between
data that is kept confidential versus data that is truly anonymous.
Increasingly, data are rarely completely anonymous, as researchers
have routinely demonstrated they can often reidentify individuals
hidden in &ldquo;anonymized&rdquo; datasets with ease (Ohm 2010). This
reality places new pressure on ensuring datasets are kept, at the
least, suitably confidential through both physical and computational
security measures. These measures may also include requirements to
store data in &ldquo;clean rooms&rdquo;, or in non-networked
environments in an effort to control data transmission.</p>

<p>
Similarly, new types of data often collected in Internet research
might also be used to identify a subject within a previously-assumed
anonymous dataset. For example, Internet researchers might collect
Internet Protocol (IP) addresses when conducting online surveys or
analyzing transaction logs. An IP address is a unique identifier that
is assigned to every device connected to the Internet; in most cases,
individual computers are assigned a unique IP address, while in some
cases the address is assigned to a larger node or Internet gateway for
a collection of computers. Many websites and Internet service
providers store activity logs linking IP addresses to online activity,
which can often be connected to specific devices or users (Mayer &amp;
Mitchell 2012). U.S. privacy legislation remains fragmented and only
some laws treat IP addresses as personally identifiable information
(PII) in limited contexts. For example, the Children&rsquo;s Online
Privacy Protection Act (COPPA) defines IP addresses as personal
information (16 C.F.R. &sect; 312.2), and the California Consumer
Privacy Act (CCPA) also considers IP addresses to be personal
information (Cal. Civ. Code &sect; 1798.140). In contrast, under the
European Union&rsquo;s General Data Protection Regulation (GDPR), IP
addresses are explicitly considered personal data when they can be
linked to an identifiable individual. There could potentially be a
reconsideration by other federal regulatory agencies over IP addresses
as PII, and researchers and boards will need to be attentive should
such change occur.</p>

<p>
A similar complication emerges when we consider the meaning of
&ldquo;private information&rdquo; within the context of Internet-based
research. U.S. federal regulations define &ldquo;private
information&rdquo; as:</p>

<blockquote>

<p>
[A]ny information about behavior that occurs in a context in which an
individual can reasonably expect that no observation or recording is
taking place, and information that has been provided for specific
purposes by an individual and that the individual can reasonably
expect will not be made public (for example, a medical record) (45
C.F.R. &sect; 46.102(f) 2009).</p>
</blockquote>

<p>
This standard definition of &ldquo;private information&rdquo; has two
key components. First, private information is that which subjects
reasonably expect is not normally monitored or collected. Second,
private information is that which subjects reasonably expect is not
typically publicly available. Conversely, the definition also suggests
the opposite is true: if users cannot reasonably expect data
isn&rsquo;t being observed or recorded, or they cannot expect data
isn&rsquo;t publicly available, then the data does not rise to the
level of &ldquo;private information&rdquo; requiring particular
privacy protections. Researchers and REBs have routinely worked with
this definition of &ldquo;private information&rdquo; to ensure the
protection of individuals&rsquo; privacy.</p>

<p>
These distinctions take on greater weight, however, when considering
the data environments and collection practices common with
Internet-based research. Researchers interested in collecting or
analyzing online actions of subjects&mdash;perhaps through the mining
of online server logs, the use of tracking cookies, or the scraping of
social media profiles and feeds&mdash;could argue that subjects do not
have a reasonable expectation that such online activities are not
routinely monitored since nearly all online transactions and
interactions are routinely logged by websites and service providers.
Thus, online data trails might not rise to the level of &ldquo;private
information&rdquo;. However, numerous studies have indicated that
average Internet users have incomplete understandings of how their
activities are routinely tracked, and the related privacy practices
and policies of the sites they visit (Hoofnagle &amp; King 2008 [Other
Internet Resources]; Milne &amp; Culnan 2004; Tsai et al. 2006).
Hudson and Bruckman (2005) conducted empirical research on
users&rsquo; expectations and understandings of privacy, finding that
participants&rsquo; expectations of privacy within public chatrooms
conflicted with what was actually a very public online space.
Rosenberg (2010) examined the public/private distinction in the realm
of virtual worlds, suggesting researchers must determine what kind of
social norms and relations predominate an online space before making
assumptions about the &ldquo;publicness&rdquo; of information shared
within. Thus, it remains unclear whether Internet users truly
understand if and when their online activity is regularly monitored
and tracked, and what kind of reasonable expectations truly exist.
This ambiguity creates new challenges for researchers and REBs when
trying to apply the definition of &ldquo;private information&rdquo; to
ensure subject privacy is properly addressed (Zimmer 2010).</p>

<p>
This complexity in addressing subject privacy in Internet research is
further compounded with the rise of social networking as a place for
the sharing of information, and a site for research. Users
increasingly share more and more personal information on platforms
like Facebook, Instagram, and TikTok. For researchers, social media
platforms provide a rich resource for study, and much of the content
is available to be viewed and downloaded with minimal effort. Since
much of the information posted to social media sites is publicly
viewable, it thus fails to meet the standard regulatory definition of
&ldquo;private information&rdquo;. Therefore, researchers attempting
to collect and analyze social media postings might not treat the data
as requiring any particular privacy considerations. Yet, social media
platforms represent a complex environment of social interaction where
users are often required to place friends, lovers, colleagues, and
minor acquaintances within the same singular category of
&ldquo;friends&rdquo;, where privacy policies and terms of service are
not fully understood (Madejski et al. 2011), and where the technical
infrastructures fail to truly support privacy projections (Bonneau
&amp; Preibush 2010) and regularly change with little notice (Stone
2009 [Other Internet Resources]; Zimmer 2009 [Other Internet
Resources]). As a result, it is difficult to understand with any
certainty what a user&rsquo;s intention was when posting an item onto
a social media platform (Acquisti &amp; Gross 2006). The user may have
intended the post for a private group but failed to completely
understand how to adjust the privacy settings accordingly. Or, the
information might have previously been restricted to only certain
friends, but a change in the technical platform suddenly made the data
more visible to all.</p>

<p>
Ohm (2010) warns that</p>

<blockquote>

<p>
the utility and privacy of data are linked, and so long as data is
useful, even in the slightest, then it is also potentially
reidentifiable (2010: 1751).</p>
</blockquote>

<p>
With the rapid growth of Internet-based research, Ohm&rsquo;s concern
becomes even more dire. The traditional definitions and approaches to
understanding the nature of privacy, anonymity, and precisely what
kind of information deserves protection becomes strained, forcing
researchers and REBs to consider more nuanced theories of privacy and
approaches to protecting subject privacy (Markham 2012; Zimmer 2010).
Zimmer (2018), for example, employs Nissenbaum&rsquo;s (2009) theory
of privacy as &ldquo;contextual integrity&rdquo; when assessing the
ethics of using publicly-available data in research, urging a move
beyond binary classifications of data and towards a context-aware
ethical framework. Similarly, Fiesler et al. (2024) argue that
researchers who justify large-scale data collection from Reddit based
on its public availability must reconsider privacy as a contextual
concept&mdash;one shaped by user expectations, community norms, and
the potential harms of removing information from its original
environment.</p>

<h3 id="Recr">4.2 Recruitment</h3>

<p>
Depending on the type of Internet research being carried out,
recruitment of participants may be done in a number of ways. As with
any form of research, the study population or participants are
selected for specific purposes (i.e., an ethnographic study of a
particular group on online game players), or, can be selected from a
range of sampling techniques (i.e., a convenience sample gleaned from
the users of Amazon&rsquo;s Mechanical Turk crowdsourcing
 platform<sup>[<a href="notes.html#note-2" id="ref-2">2</a>]</sup>).
 In the U.S. context, a recruitment plan is considered part of the
informed consent process, and as such, any recruitment script or
posting must be reviewed and approved by an IRB prior to posting or
beginning solicitation (if the project is human subjects research).
Further, the selection of participants must be impartial and unbiased,
and any risks and benefits must be justly distributed. This concept is
challenging to apply in Internet contexts, in which populations are
often self-selected and can be exclusive, depending on membership and
access status, as well as the common disparities of online access
based on economic and social variables. Researchers also face
recruitment challenges due to online subjects&rsquo; potential
anonymity, especially as it relates to the frequent use of pseudonyms
online, having multiple or alternative identities online, and the
general challenges of verifying a subject&rsquo;s age and demographic
information. Moreover, basic ethical principles for approaching and
recruiting participants involve protecting their privacy and
confidentiality. Internet research can both maximize these
protections, as an individual may never be known beyond a screen name
or avatar existence; or, conversely, the use of IP addresses,
placement of cookies, availability and access to more information than
necessary for the research purposes, may minimize the protections of
privacy and confidentiality.</p>

<p>
Much recruitment is taking place via social media; examples include
push technologies, a synchronous approach in which a text or tweet is
sent from a researcher to potential participants based on profile
data, platform activity, or geolocation. Other methods of pull
technologies recruitment include direct email, dedicated web pages,
YouTube videos, direct solicitation via &ldquo;stickies&rdquo; posted
on fora or web sites directing participants to a study site, or data
aggregation or scraping data for potential recruitment. Just as
researchers must first comply with their institution&rsquo;s research
ethics policies and review procedures, they must also respect the
terms and conditions of the platform, including both the specific
norms and community expectations of a given site or locale, as well as
the legal obligations imposed by terms of service agreements. For
example, early pro-anorexia web sites (see Overbeke 2008) were often
treated as sensitive spaces deserving special consideration, and
researchers were asked to respect the privacy of the participants and
not engage in research (Walstrom 2004). In the gaming context,
Reynolds and de Zwart (2010) ask:</p>

<blockquote>

<p>
Has the researcher disclosed the fact that he or she is engaged in
research and is observing/interacting with other players for the
purposes of gathering research data? How does the research project
impact upon the community and general game play? Is the research
project permitted under the Terms of Service?</p>
</blockquote>

<p>
Colvin and Lanigan (2005: 38) suggest researchers</p>

<blockquote>

<p>
Seek permission from Web site owners and group moderators before
posting recruitment announcements, Then, preface the recruitment
announcement with a statement that delineates the permission that has
been granted, including the contact person and date received. Identify
a concluding date (deadline) for the research study and make every
effort to remove recruitment postings, which often become embedded
within Web site postings.</p>
</blockquote>

<p>
Barratt and Lenton, among others, agree:</p>

<blockquote>

<p>
It is critical, therefore, to form partnerships with online community
moderators by not only asking their permission to post the request,
but eliciting their feedback and support as well (2010: 71).</p>
</blockquote>

<p>
Mendelson (2007) and Smith and Leigh (1997) note that recruitment
notices need to contain more information than the typical flyers or
advertisements used for newspaper advertisements. Mentioning the
approval of moderators is important for establishing authenticity, and
so is providing detailed information about the study and how to
contact both the researchers and the appropriate research ethics
board.</p>

<p>
Given the array of techniques possible for recruitment, the concept of
&ldquo;research spam&rdquo; requires attention. The Council of
American Survey Research warns</p>

<blockquote>

<p>
Research Organizations should take steps to limit the number of survey
invitations sent to targeted respondents by email solicitations or
other methods over the Internet so as to avoid harassment and response
bias caused by the repeated recruitment and participation by a given
pool (or panel) of data subjects (CASRO 2011: I.B.3).</p>
</blockquote>

<p>
Ultimately, internet researchers must take care to ensure that online
recruitment practices provide prospective participants with clear,
accessible, and sufficient information--both in the initial
recruitment message and in any subsequent consent processes.
Transparency is essential, particularly when recruitment occurs in
public or semi-public digital spaces where individuals may not expect
to be targeted for research. As Fiesler et al. (2024) point out in the
context of studying participants in online communities such as Reddit,
researchers must assess whether their recruitment methods could
inadvertently expose an individual&rsquo;s identity without their
explicit consent, especially when usernames, comments, or other
digital traces can be linked back to a person. Ethical recruitment in
online spaces, they argue, requires a context-sensitive approach that
balances the visibility of digital data with respect for
individuals&rsquo; autonomy, anonymity, and safety.</p>

<h3 id="InfoCons">4.3 Informed Consent</h3>

<p>
As the cornerstone of human subjects protections, informed consent
means that participants are voluntarily participating in the research
with adequate knowledge of relevant risks and benefits. Providing
informed consent typically includes the researcher explaining the
purpose of the research, the methods being used, the possible outcomes
of the research, as well as associated risks or harms that the
participants might face. The process involves providing the recipient
clear and understandable explanations of these issues in a concise
way, providing sufficient opportunity to consider them and enquire
about any aspect of the research prior to granting consent, and
ensuring the subject has not been coerced into participating. Gaining
consent in traditional research is typically done verbally or in
writing, either in a face-to-face meeting where the researcher reviews
the document, through telephone scripts, through mailed documents,
fax, or video, and can be obtained with the assistance of an advocate
in the case of vulnerable populations. Most importantly, informed
consent was built on the ideal of &ldquo;process&rdquo; and the
verification of understanding, and thus, requires an ongoing
communicative relationship between and among researchers and their
participants. The emergence of the Internet as both a tool and a venue
for research has introduced challenges to this traditional approach to
informed consent.</p>

<p>
In most regulatory frameworks, there are instances when informed
consent might be waived, or the standard processes of obtaining
informed consent might be modified, if approved by a research ethics
 board.<sup>[<a href="notes.html#note-3" id="ref-3">3</a>]</sup>
 Various forms of Internet research require different approaches to
the consent process. Some standards have emerged, depending on venue
(i.e., an online survey platform versus a private Facebook group).
However, researchers are encouraged to consider waiver of consent
and/or documentation, if appropriate, by using the flexibilities of
their extant regulations.</p>

<p>
Where consent is required but documentation has been waived by an
ethical review board, a &ldquo;portal&rdquo; can be used to provide
consent information. For example, a researcher may send an email to
the participant with a link a separate portal or site information page
where information on the project is contained. The participant can
read the documentation and click on an &ldquo;I agree&rdquo;
submission. Rosser et al. (2010) recommend using a
&ldquo;chunked&rdquo; consent document, whereby individuals can read
specific sections, agree, and then continue onwards to completion of
the consent form, until reaching the study site.</p>

<p>
In addition to portals, researchers will often make use of consent
cards or tokens; this alleviates concerns that unannounced researcher
presence is unacceptable, or, that a researcher&rsquo;s presence is
intrusive to the natural flow and movement of a given locale. Hudson
and Bruckman (2004, 2005) highlighted the unique challenges in gaining
consent in chat rooms, while Lawson (2004) offers an array of consent
possibilities for synchronous computer-mediated communication. There
are different practical challenges in the consent process in Internet
research, given the fluidity and temporal nature of Internet
spaces.</p>

<p>
If documentation of consent is required, some researchers have
utilized alternatives such as electronic signatures, which can range
from a simple electronic check box to acknowledge acceptance of the
terms to more robust means of validation using encrypted digital
signatures, although the validity of electronic signatures vary by
jurisdiction.</p>

<p>
Regardless of venue, informed consent documents are undergoing changes
in the information provided to research participants. While the basic
elements of consent remain intact, researchers must now acknowledge
with less certainty specific aspects of their data longevity, risks to
privacy, confidentiality and anonymity (see
 <a href="#Priv">&sect;4.1 Privacy, above</a>),
 and access to or ownership of data. Research participants must
understand that their terms of service or end user license agreement
consent is distinct from their consent to participate in research.
And, researchers must address and inform participants/subjects about
potential risk of data intrusion or misappropriation of data if
subsequently made public or available outside of the confines of the
original research. Statements should be revised to reflect such
realities as cloud storage (see
 <a href="#ClouBasePlatReseEthi">&sect;4.4 below</a>)
 and data sharing.</p>

<p>
For example, Aycock et al. (2012: 141) describe a continuum of
security and access statements used in informed consent documents:</p>

<ul class="jfy">

<li>&ldquo;No others will have access to the data&rdquo;</li>

<li>&ldquo;Anonymous identifiers will be used during all data
collection and analysis and the link to the subject identifiers will
be stored in a secure manner&rdquo;</li>

<li>&ldquo;Data files that contain summaries of chart reviews and
surveys will only have study numbers but no data to identify the
subject. The key [linking] subject names and these study identifiers
will be kept in a locked file&rdquo;</li>

<li>&ldquo;Electronic data will be stored on a password protected and
secure computer that will be kept in a locked office. The software
&lsquo;File Vault&rsquo; will be used to protect all study data loaded
to portable laptops, flash drives or other storage media. This will
encode all data&hellip; using Advanced Encryption Standard with
128-bit keys (AES-128)&rdquo;</li>
</ul>

<p>
This use of encryption in the last statement may be necessary in
research including sensitive data, such as medical, sexual, health,
financial, and so on. Barratt and Lenton (2010), in their research on
illicit drug use and online forum behaviors, also provide guidance
about use of secure transmission and encryption as part of the consent
process.</p>

<p>
In addition to informing participants about potential risks and
employing technological protections, NIH-funded researchers whose work
includes projects with identifiable, sensitive information will
automatically be issued a Certificate of Confidentiality:</p>

<blockquote>

<p>
CoCs protect the privacy of research subjects by prohibiting
disclosure of identifiable, sensitive research information to anyone
not connected to the research except when the subject consents or in a
few other specific situations (NIH 2021 [Other Internet
Resources]).</p>
</blockquote>

<p>
However, these do not protect against release of data outside of the
U.S. Given that Internet research inherently spans national and
cultural boundaries, new models of informed consent and data
protection may be necessary to ensure meaningful confidentiality and
respect for participants. In traditional international research,
models of informed consent already face fundamental challenges due to
cultural norms and local expectations (Annas 2009; Boga et al. 2011;
Krogstad et al. 2010). In the context of Internet research&ndash;where
researchers may not even know the geographic or cultural context of an
individual participant&ndash;obtaining valid, culturally appropriate
consent becomes even more complex and demanding. While current
standards of practice show that consent models stem from the
jurisdiction of the researcher and sponsoring research institution,
complications arise in the face of age verification, age of
majority/consent, reporting of adverse effects or complaints with the
research process, and authentication of identity. Various
jurisdictional laws around privacy are relevant for the consent
process; a useful tool is DLA Piper&rsquo;s &ldquo;Data Protection
Laws of the World&rdquo; resource, which relies on in-depth analyses
of the data privacy-related laws and cultures of countries around the
world, helping researchers design appropriate approaches to privacy
and data protection given the particular context (see
 <a href="#Oth">Other Internet Resources</a>).</p>
 
<p>
In addition, as more federal agencies and funding bodies around the
world require researchers to make their data publicly available (i.e.,
NSF, NIH, Wellcome Trust, Research Councils U.K.), the language used
in consent documents will change accordingly to represent this
intended longevity of data and opportunities for future, unanticipated
use. Given the ease with which Internet data can flow between and
among Internet venues, changes in the overall accessibility of data
might occur (early &ldquo;private&rdquo; newsgroup conversations were
made &ldquo;publicly searchable&rdquo; when Google bought DejaNews),
and reuse and access by others is increasingly possible with shared
datasets. Current data sharing mandates must be considered in the
consent process. Alignment between a data sharing policy and an
informed consent document is imperative. Both should include
provisions for appropriate protection of privacy, confidentiality,
security, and intellectual property.</p>

<p>
There is general agreement in the U.S. that individual consent is not
necessary for researchers to use publicly available data, such as
public X/Twitter feeds. Recommendations were made by The National
Human Subjects Protection Advisory Committee (NHRPAC) in 2002
regarding publicly available data sets (see
 <a href="#Oth">Other Internet Resources</a>).
 Data use or data restriction agreements are commonly used and set the
parameters of use for researchers.</p>

<p>
The U.K. Data Archive (2011
 [<a href="#Oth">Other Internet Resources</a>])
 provides guidance on consent and data sharing:</p>

<blockquote>

<p>
When research involves obtaining data from people, researchers are
expected to maintain high ethical standards such as those recommended
by professional bodies, institutions and funding organisations, both
during research and when sharing data. Research data &mdash; even
sensitive and confidential data &mdash; can be shared ethically and
legally if researchers pay attention, from the beginning of research,
to three important aspects:

<ul>

<li>when gaining informed consent, include provision for data
sharing</li>

<li>where needed, protect people&rsquo;s identities by anonymising
data</li>

<li>consider controlling access to data</li>
</ul>

<p>
These measures should be considered jointly. The same measures form
part of good research practice and data management, even if data
sharing is not envisioned. Data collected from and about people may
hold personal, sensitive or confidential information. This does not
mean that all data obtained by research with participants are personal
or confidential. (p. 23)</p>
</blockquote>

<p>
The ethical complexities of consent and data sharing made public
headlines in 2016 when a Danish researcher released a data set
comprised of scraped data from nearly 70,000 users of the OkCupid
online dating site. The data set was highly reidentifiable and
included potentially sensitive information, including usernames, age,
gender, geographic location, what kind of relationship (or sex)
they&rsquo;re interested in, personality traits, and answers to
thousands of profiling questions used by the site. The researcher
claimed the data were public and thus, such sharing and use was
unproblematic and no consent was necessary. Zimmer (2016) was among
many privacy and ethics scholars who critiqued this stance.</p>

<p>
Shilton et al. (2021) further explore the ethical challenges of
applying traditional informed consent models to internet and social
data-driven research. They argue that in contexts where data is
passively collected or repurposed--such as through social media
platforms or mobile devices--conventional informed consent mechanisms
often fall short, highlighting that users may not fully comprehend how
their data is utilized, and the dynamic nature of data collection
makes it difficult to obtain meaningful consent. They suggest that
relying solely on informed consent is insufficient for ethical data
practices in internet research, and advocate for a more holistic
approach that includes transparency, accountability, and user
empowerment to ensure ethical standards are upheld in the evolving
landscape of big data data research.</p>

<h3 id="ClouBasePlatReseEthi">4.4 Cloud-Based Platforms and Research Ethics</h3>

<p>
The rise of cloud-based platforms and distributed computing
environments has created new opportunities&mdash;and ethical
challenges&mdash;for researchers working with internet-based tools,
data, and collaborations. While &ldquo;cloud computing&rdquo; once
referred narrowly to the remote delivery of storage and computing
power via services like Amazon Web Services or Microsoft Azure, the
term now broadly encompasses a range of tools and platforms that
enable the collection, processing, and sharing of data online.</p>

<p>
Researchers today rely on a variety of cloud-enabled services,
including collaborative workspaces (e.g., Google Drive, Microsoft
365), social platforms (e.g., Reddit, Facebook, X/Twitter),
API-enabled data collection tools, and crowdsourcing platforms (e.g.,
Amazon Mechanical Turk, Prolific). These tools are used for tasks such
as subject recruitment, data scraping, analysis, storage, remote
collaboration, and even real-time AI-powered interventions. As these
tools grow in sophistication and reach, so do the ethical risks
associated with their use.</p>

<p>
A central ethical concern in cloud-based research is ensuring the
protection of personal data. Researchers must verify that datasets
stored in the cloud are secured through appropriate access controls,
encryption, and data minimization practices. It is critical to assess
whether third-party platforms (including storage providers and APIs)
collect metadata or reserve access rights through their terms of
service. These contracts may permit advertisers, law enforcement, or
platform owners to access data in ways that conflict with the
expectations of research participants or IRB approvals.</p>

<p>
Geographic distribution of data storage adds complexity, particularly
in relation to jurisdictional privacy laws (e.g., GDPR in the EU, CCPA
in California). Ethical data stewardship includes ensuring that
sensitive research data is handled in accordance with relevant legal
standards&ndash;including rights such as the right to be forgotten or
the right to erasure&ndash;and that deletion or withdrawal of data is
possible, even when stored in distributed or redundant systems.</p>

<p>
A more unique application of cloud computing for research involves the
crowdsourcing of data analysis and processing functions, that is,
leveraging the thousands of users of various online products and
services to complete research related tasks remotely. Examples include
using a distributed network of video game players to assist in solving
protein folding problems (Markoff 2010), and leveraging Amazon&rsquo;s
Mechanical Turk crowdsourcing marketplace platform to assist with
large scale data processing and coding functions that cannot be
automated (Conley &amp; Tosti-Kharas 2014; J. Chen et al. 2011). Using
cloud-based platforms can raise various critical ethical and
methodological issues.</p>

<p>
First, new concerns over data privacy and security emerge when
research tasks are widely distributed across a global network of
users. Researchers must take great care in ensuring research data
containing personal or sensitive information isn&rsquo;t accessible by
outsourced labor, or that none of the users providing crowdsourced
labor are able to aggregate and store their own copy of the research
dataset. Second, crowdsourcing presents ethical concerns over trust
and validity of the research process itself. Rather than a local team
of research assistants usually under a principal investigator&rsquo;s
supervision and control, crowdsourcing tends to be distributed beyond
the direct management or control of the researcher, providing less
opportunity to ensure sufficient training for the required tasks.
Thus, researchers will need to create additional means of verifying
data results to confirm tasks are completed properly and
correctly.</p>

<p>
Two additional ethical concerns with crowdsourcing involve labor
practices and authorship. Platforms like Amazon Mechanical Turk were
originally designed to facilitate paid microtasks such as data
labeling and transcription&ndash;not to serve as recruitment tools for
research participants. When researchers use such platforms for human
subjects research, they must ensure that workers are not exploited,
that they are legally eligible for paid work, and that compensation is
fair, meaningful, and appropriate to the nature of the research task
(Scholz 2008; Williams 2010
 [<a href="#Oth">Other Internet Resources</a>).</p>
 
<p>
Finally, at the conclusion of a research project that has relied on
crowdsourcing, researchers may face the ethical challenge of how to
appropriately acknowledge the contributions of crowd workers&ndash;who
are often anonymous. Ethical research demands a fair and accurate
account of authorship and contribution. Disciplinary norms for
reporting contributions by collaborators and research assistants vary,
and these complexities are amplified when the work of anonymous crowd
laborers has shaped the research (Silberman et al. 2010).</p>

<h3 id="BigDataCons">4.5 Big Data Considerations</h3>

<p>
Algorithmic processing is a corollary of big data research, and
newfound ethical considerations have emerged. From &ldquo;algorithmic
harms&rdquo; to &ldquo;predictive analytics&rdquo;, the power of
today&rsquo;s algorithms exceeds long-standing privacy beliefs and
norms. Specifically, the National Science and Technology Council
note:</p>

<blockquote>
&ldquo;Analytical algorithms&rdquo; as algorithms for prioritizing,
classifying, filtering, and predicting. Their use can create privacy
issues when the information used by algorithms is inappropriate or
inaccurate, when incorrect decisions occur, when there is no
reasonable means of redress, when an individual&rsquo;s autonomy is
directly related to algorithmic scoring, or when the use of predictive
algorithms chills desirable behavior or encourages other privacy
harms. (NSTC 2016: 18).
</blockquote>

<p>
Although the concept of big data has existed since the 1990s in
technical circles, public awareness and critical engagement with big
data research has only emerged more recently. In particular, Buchanan
(2016) traces the rise of big data-driven research&ndash;especially
involving social media platforms&ndash;from around 2012 onward, noting
that this trend shows no signs of slowing.</p>

<p>
Big data research is challenging for research ethics boards, often
presenting what the computer ethicist James Moor would call
&ldquo;conceptual muddles&rdquo;: the inability to properly
conceptualize the ethical values and dilemmas at play in a new
technological context. Subject privacy, for example, is typically
protected within the context of research ethics through a combination
of various tactics and practices, including engaging in data
collection under controlled or anonymous environments, limiting the
personal information gathered, scrubbing data to remove or obscure
personally identifiable information, and using access restrictions and
related data security methods to prevent unauthorized access and use
of the research data itself. The nature and understanding of privacy
become muddled, however, in the context of big data research, and as a
result, ensuring it is respected and protected in this new domain
becomes challenging.</p>

<p>
For example, the determination of what constitutes &ldquo;private
information&rdquo;&mdash;and thus triggering particular privacy
concerns&mdash;becomes difficult within the context of big data
research. Distinctions within the regulatory definition of
&ldquo;private information&rdquo;&mdash;namely, that it only applies
to information which subjects reasonably expect is not normally
monitored or collected and not normally publicly
available&mdash;become less clearly applicable when considering the
data environments and collection practices that typify big data
research, such as the wholesale scraping of Facebook news feed content
or public OkCupid accounts.</p>

<p>
When considered through the lens of the regulatory definition of
&ldquo;private information&rdquo;, social media postings are often
considered public, especially when users take no visible, affirmative
steps to restrict access. As a result, big data researchers might
conclude subjects are not deserving of particular privacy
consideration. Yet, the social media platforms frequently used for big
data research purposes represent a complex environment of
socio-technical interactions, where users often fail to understand
fully how their social activities might be regularly monitored,
harvested, and shared with third parties, where privacy policies and
terms of service are not fully understood and change frequently, and
where the technical infrastructures and interfaces are designed to
make restricting information flows and protecting one&rsquo;s privacy
difficult.</p>

<p>
As noted
 <a href="#Priv">in &sect;4.1 above</a>
 it becomes difficult to confirm a user&rsquo;s intention when sharing
information on a social media platform, and whether users recognize
that providing information in a social environment also opens it up
for widespread harvesting and use by researchers. This uncertainty in
the intent and expectations of users of social media and
internet-based platforms&mdash;often fueled by the design of the
platforms themselves&mdash;create numerous conceptual muddles in our
ability to properly alleviate potential privacy concerns in big data
research.</p>

<p>
The conceptual gaps that exist regarding privacy and the definition of
personally identifiable information in the context of big data
research inevitably lead to similar gaps regarding when informed
consent is necessary. Researchers mining Facebook profile information
or public X/Twitter streams, for example, typically argue that no
specific consent is necessary due to the fact the information was
publicly available. It remains unknown whether users truly understood
the technical conditions under which they made information visible on
these social media platforms or if they foresaw their data being
harvested for research purposes, rather than just appearing onscreen
for fleeting glimpses by their friends and followers (Fiesler &amp;
Proferes, 2018). In the case of the Facebook emotional contagion
experiment (Kramer, Guillory, &amp; Hancock 2014), which involved
nearly 690,000 users, the failure to obtain informed consent was
initially justified by invoking Facebook&rsquo;s broad terms of
service&ndash;a document over 9,000 words long that makes only a
passing reference to &ldquo;research&rdquo; in its data use policy. It
was later revealed, however, that the data use policy in effect when
the experiment was conducted never mentioned &ldquo;research&rdquo; at
all (Hill 2014).</p>

<p>
Additional ethical concerns have arisen surrounding the large scale
data collection practices connected to machine learning and the
development of artificial intelligence. For example, negative public
attention have surrounded algorithms designed to infer sexual
orientation from photographs and facial recognition algorithms trained
on videos of transgender people. In both cases, ethical concerns have
been raised about both the purpose of these algorithms and the fact
that the data that trained them (dating profile photos and YouTube
videos, respectively) was &ldquo;public&rdquo; but collected from
potentially vulnerable populations without consent (Metcalf 2017;
Keyes 2019). While those building AI systems cannot always control the
conditions under which the data they utilize is collected, their
increased use of big datasets captured from social media or related
sources raises a number of concerns beyond what typically is
considered part of the growing focus on AI ethics: fairness,
accountability and transparency in AI can only be fully possible when
data collection is achieved in a fair, ethical, and just manner (Stahl
&amp; Wright 2018; Kerry 2020).</p>

<p>
Shilton et al. (2021) expand on these concerns by highlighting how
traditional models of privacy and informed consent are often
ill-suited to the realities of large-scale, big data collection. They
challenges researchers to move beyond simplistic public/private
distinctions and instead adopt context-aware approaches rooted in user
expectations, potential harms, and community norms, arguing that the
ethical use of big data requires not just technical compliance with
terms of service or regulatory frameworks, but a deeper engagement
with issues of power, marginalization, and the lived realities of data
subjects.</p>

<h3 id="InteReseInduEthi">4.6 Internet Research and Industry Ethics</h3>

<p>
The Facebook emotional contagion experiment, discussed above, is just
one example in a larger trend of big data research conducted outside
of traditional university-based research ethics oversight mechanisms.
Nearly all online companies and platforms analyze data and test
theories that often rely on data from individual users. Industry-based
data research, once limited to marketing-oriented &ldquo;A/B
testing&rdquo; of benign changes in interface designs or corporate
communication messages, now encompasses information about how users
behave online, what they click and read, how they move, eat, and
sleep, the content they consume online, and even how they move about
their homes. Such research produces inferences about
individuals&rsquo; tastes and preferences, social relations,
communications, movements, and work habits. It implies pervasive
testing of products and services that are an integral part of intimate
daily life, ranging from connected home products to social networks to
smart cars. Except in cases where they are partnering with academic
institutions, companies typically do not put internal research
activities through a formal ethical review process, since results are
typically never shared publicly and the perceived impact on users is
minimal.</p>

<p>
The growth of industry-based big data research, however, presents new
risks to individuals&rsquo; privacy, on the one hand, and to
organizations&rsquo; legal compliance, reputation, and brand, on the
other hand. When organizations process personal data outside of their
original context, individuals may in some cases greatly benefit, but
in other cases may be surprised, outraged, or even harmed. Soliciting
consent from affected individuals can be impractical: Organizations
might collect data indirectly or based on identifiers that do not
directly match individuals&rsquo; contact details. Moreover, by
definition, some non-contextual uses&mdash;including the retention of
data for longer than envisaged for purposes of a newly emergent
use&mdash;may be unforeseen at the time of collection. As Crawford and
Schultz (2014) note,</p>

<blockquote>

<p>
how does one give notice and get consent for innumerable and perhaps
even yet-to-be-determined queries that one might run that create
&ldquo;personal data&rdquo;? (2014: 108)</p>
</blockquote>

<p>
With corporations developing vast &ldquo;living laboratories&rdquo;
for big data research, research ethics has become a critical component
of the design and oversight of these activities. For example, in
response to the controversy surrounding the emotional contagion
experiment, Facebook developed an internal ethical review process
that, according to its facilitators,</p>

<blockquote>

<p>
leverages the company&rsquo;s organizational structure, creating
multiple training opportunities and research review checkpoints in the
existing organizational flow (Jackman &amp; Kanerva 2016: 444).</p>
</blockquote>

<p>
While such efforts are important and laudable, they remain open for
improvement. Hoffmann (2016), for example, has criticized Facebook for
launching an ethics review process that &ldquo;innovates on process
but tells us little about the ethical values informing their product
development.&rdquo; Further, in their study of employees doing the
work of ethics inside of numerous Silicon Valley companies, Metcalf
and colleagues found considerable tension between trying to resolve
thorny ethical dilemmas that emerge within an organization&rsquo;s
data practices and the broader business model and corporate logic that
dominates internal decision-making (Metcalf, Moss, &amp; boyd
2019).</p>

<p>
Moreover, new uses of AI systems trained on platform data--often
scraped or mined without individuals&rsquo; awareness--have raised new
ethical concerns. Examples include predictive algorithms trained on
sensitive data sources such as dating profile photos, biometric scans,
and YouTube videos of vulnerable communities&mdash;often without the
knowledge or consent of the individuals depicted. As King and
Meinhardt (2024) emphasize, organizations now often generate sensitive
personal data not through direct collection, but by using AI systems
to infer characteristics such as mental health status or sexual
orientation from innocuous data like search queries or social media
activity. These practices raise urgent questions about how to ensure
meaningful consent, particularly when individuals are unaware their
data has been included in training datasets, and highlight the broader
challenge of addressing emergent privacy harms that traditional
regulatory frameworks are ill-equipped to manage&#8203; While the data
used may be &ldquo;public,&rdquo; the intent, impact, and power
asymmetries involved often demand a higher ethical standard (Shilton
et al. 2021).</p>

<p>
Taken together, these developments underscore the urgent need for
industry-specific ethical frameworks that go beyond self-regulation or
internal ethics-by-design protocols. Proposals include establishing
third-party ethics audits, enforcing algorithmic transparency
requirements, and building external oversight boards for high-risk
data projects (Bernstein et al. 2021 [Other Internet Resources]).</p>

<h2 id="Conc">5. Conclusion</h2>

<p>
As the Internet continues to serve as a research tool, venue, and
object of study, the ethical landscape of Internet research remains
both expansive and evolving. From early debates about whether existing
frameworks like consequentialism or deontology could adequately
address online research, to current concerns about datafication,
algorithmic harms, and cross-jurisdictional data flows, the core
challenges of Internet Research Ethics persist, but in increasingly
complex and uncertain forms. This entry has highlighted how
foundational principles of human subjects research &ndash; such as
privacy, informed consent, and justice &ndash; are being reshaped in
the face of cloud-based platforms, big data practices, and the growth
of computational methods. Key ethical issues such as participant
recruitment, consent mechanisms, and the ambiguous status of public
data now require not only updated definitions but also flexible,
context-aware interpretations that consider both the technological
infrastructures and social expectations at play. The role of research
ethics boards remains central, yet uneven, in their ability to
navigate emerging dilemmas, especially when industry-based research
often falls outside traditional oversight mechanisms.</p>

<p>
Ultimately, Internet research ethics is not a static set of rules but
a dynamic, interdisciplinary endeavor that must keep pace with rapidly
shifting digital norms, evolving technologies, and emerging forms of
harm. The challenges ahead demand thoughtful engagement across
disciplines and sectors, renewed attention to fairness and
accountability, and ongoing efforts to adapt ethical guidance and
governance in ways that remain sensitive to the lives, identities, and
vulnerabilities of those whose data becomes the object of
research.</p>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Acquisti, Alessandro and Ralph Gross, 2006, &ldquo;Imagined
Communities: Awareness, Information Sharing, and Privacy on the
Facebook&rdquo;, in <em>Privacy Enhancing Technologies: PET 2006</em>,
George Danezis and Philippe Golle (eds.), (Lecture Notes in Computer
Science 4258), Berlin, Heidelberg: Springer Berlin Heidelberg, pp.
36&ndash;58. doi:10.1007/11957454_3</li>

<li>Allen, Christina, 1996, &ldquo;What&rsquo;s Wrong with the
&lsquo;Golden Rule&rsquo;? Conundrums of Conducting Ethical Research
in Cyberspace&rdquo;, <em>The Information Society</em>, 12(2):
175&ndash;188. doi:10.1080/713856146</li>

<li>Annas, George J., 2009, &ldquo;Globalized Clinical Trials and
Informed Consent&rdquo;, <em>New England Journal of Medicine</em>,
360(20): 2050&ndash;2053. doi:10.1056/NEJMp0901474</li>

<li>Ashurst, Carolyn, Emmie Hine, Paul Sedille, and Alexis Carlier,
2022, &ldquo;AI Ethics Statements: Analysis and Lessons Learnt From
NeurIPS Broader Impact Statements,&rdquo; in <em>Proceedings of the
2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, New York: Association for Computing Machinery, pp.
2047&ndash;2056</li>

<li>Aycock, John, Elizabeth Buchanan, Scott Dexter, and David
Dittrich, 2012, &ldquo;Human Subjects, Agents, or Bots: Current Issues
in Ethics and Computer Security Research&rdquo;, in <em>Financial
Cryptography and Data Security</em>, George Danezis, Sven Dietrich,
and Kazue Sako (eds.), (Lecture Notes in Computer Science 7126),
Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 138&ndash;145.
doi:10.1007/978-3-642-29889-9_12</li>

<li>Banks, Will and Michelle Eble, 2007, &ldquo;Digital Spaces, Online
Environments, and Human Participant Research: Interfacing with
Institutional Review Boards&rdquo;, in <em>Digital Writing Research:
Technologies, Methodologies, and Ethical Issues</em>, Heidi A. McKee
and D&agrave;nielle Nicole DeVoss (eds.), Cresskill, NJ: Hampton
Press, pp. 27&ndash;47.</li>

<li>Barbaro, Michael and Tom Zeller Jr., 2006, &ldquo;A Face Is
Exposed for AOL Searcher No. 4417749&rdquo;, <em>The New York
Times</em>, 9 August 2006, pp. A1.</li>

<li>Barratt, Monica Jane and Simon Lenton, 2010, &ldquo;Beyond
Recruitment? Participatory Online Research with People Who Use
Drugs&rdquo;, <em>International Journal of Internet Research
Ethics</em>, 3(1): 69&ndash;86.
 [<a href="http://hdl.handle.net/20.500.11937/23339" target="other">Barratt and Lenton 2010 available online</a>]</li>
 
<li>Bauchspies, Wenda, Alex Romero, Jason Borenstein, and Michael
Steele, 2023, &ldquo;Introduction to NSF&rsquo;s Ethical and
Responsible Research (ER2) Program,&rdquo; in <em>2023 IEEE
International Symposium on Ethics in Engineering, Science, and
Technology</em> (ETHICS),
 [<a href="https://doi.org/10.1109/ETHICS57328.2023.10155101" target="other">Bauchspies et al. 2023 available online</a>]</li>
 
<li>BBC, 2011, &ldquo;US Scientists &lsquo;Knew Guatemala Syphilis
Tests Unethical&rsquo;&rdquo;, BBC News, 30 August 2011, sec. Latin
America &amp; Caribbean.
 [<a href="https://www.bbc.com/news/world-latin-america-14712089" target="other">BBC 2011 available online</a>]</li>
 
<li>Beauchamp, Tom L. and James F. Childress, 2008, <em>Principles of
Biomedical Ethics</em>, Oxford: Oxford University Press.</li>

<li>Blackstone, Mary, Lisa Given, Joseph Levy, Michelle McGinn,
Patrick O&rsquo;Neill, Ted Palys, and Will van den Hoonaard, 2008,
<em>Extending the Spectrum: The TCPS and Ethical Issues Involving
Internet-Based Research</em>, Interagency Advisory Panel and
Secretariat on Research Ethics, Ottawa, Canada.
 [<a href="https://researchoutput.csu.edu.au/en/publications/extending-the-spectrum-the-tcps-and-ethical-issues-involving-inte" target="other">Blackstone et al. 2008 available online</a>]</li>
 
<li>Boehlefeld, Sharon Polancic, 1996, &ldquo;Doing the Right Thing:
Ethical Cyberspace Research&rdquo;, <em>The Information Society</em>,
12(2): 141&ndash;152. doi:10.1080/713856136</li>

<li>Boga, Mwanamvua, Alun Davies, Dorcas Kamuya, Samson M. Kinyanjui,
Ester Kivaya, Francis Kombe, Trudie Lang, Vicki Marsh, Bibi Mbete,
Albert Mlamba, et al., 2011, &ldquo;Strengthening the Informed Consent
Process in International Health Research through Community Engagement:
The KEMRI-Wellcome Trust Research Programme Experience&rdquo;,
<em>PLoS Medicine</em>, 8(9): e1001089.
doi:10.1371/journal.pmed.1001089</li>

<li>Bonneau, Joseph and S&ouml;ren Preibusch, 2010, &ldquo;The Privacy
Jungle: On the Market for Data Protection in Social Networks&rdquo;,
in <em>Economics of Information Security and Privacy</em>, Tyler
Moore, David Pym, and Christos Ioannidis (eds.), Boston: Springer US,
pp. 121&ndash;167. doi:10.1007/978-1-4419-6967-5_8</li>

<li>Booth, Robert, 2014, &ldquo;Facebook Reveals News Feed Experiment
to Control Emotions&rdquo;, <em>The Guardian</em>, 29 June 2014.
 [<a href="https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds" target="other">Booth 2014 available online</a>]</li>
 
<li>Bromseth, Janne C. H., 2002, &ldquo;Public Places: Public
Activities? Methodological Approaches and Ethical Dilemmas in Research
on Computer-mediated Communication Contexts&rdquo;, in <em>Researching
ICTs in Context</em>, Andrew Morrison (ed.), InterMedia Report 3/2002,
Oslo: University of Oslo, pp. 33&ndash;61.
 [<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.7392&amp;rep=rep1&amp;type=pdf" target="other">Bromseth 2002 available online</a>]</li>
 
<li>Brothers, Kyle Bertram and Ellen Wright Clayton, 2010,
&ldquo;&lsquo;Human Non-Subjects Research&rsquo;: Privacy and
Compliance&rdquo;, <em>The American Journal of Bioethics</em>, 10(9):
15&ndash;17. doi:10.1080/15265161.2010.492891</li>

<li>Bruckman, Amy, 2006, &ldquo;Teaching Students to Study Online
Communities Ethically&rdquo;, <em>Journal of Information Ethics</em>,
15(2): 82&ndash;98. doi:10.3172/JIE.15.2.82</li>

<li>Buchanan, Elizabeth A. (ed.), 2004, <em>Readings in Virtual
Research Ethics: Issues and Controversies</em>, Hershey, PA:
Information Science Publishing.</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Introduction: Internet
Research Ethics at a Critical Juncture&rdquo;, <em>Journal of
Information Ethics</em>, 15(2): 14&ndash;17.
doi:10.3172/JIE.15.2.14</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Internet Research Ethics:
Past, Present, and Future&rdquo;, in Consalvo and Ess 2011:
83&ndash;108. doi:10.1002/9781444314861.ch5</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Ethics in Digital
Research&rdquo;, in <em>Handbuch Soziale Praktiken und Digitale
Alltagswelten</em>, Heidrun Friese, Gala Rebane, Marcus Nolden, and
Miriam Schreiter (eds.), Wiesbaden: Springer Fachmedien Wiesbaden, pp.
1&ndash;9. doi:10.1007/978-3-658-08460-8_47-1</li>

<li>Buchanan, Elizabeth A. and Charles M. Ess, 2008, &ldquo;Internet
Research Ethics: The Field and Its Critical Issues&rdquo;, in <em>The
Handbook of Information and Computer Ethics</em>, Kenneth Einar Himma
and Herman T. Tavani (eds.), Hoboken, NJ: John Wiley &amp; Sons, Inc.,
pp. 273&ndash;292. doi:10.1002/9780470281819.ch11</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Internet Research Ethics and
the Institutional Review Board: Current Practices and Issues&rdquo;,
<em>ACM SIGCAS Computers and Society</em>, 39(3): 43&ndash;49.
doi:10.1145/1713066.1713069</li>

<li>Buchanan, Elizabeth A. and Erin E. Hvizdak, 2009, &ldquo;Online
Survey Tools: Ethical and Methodological Concerns of Human Research
Ethics Committees&rdquo;, <em>Journal of Empirical Research on Human
Research Ethics</em>, 4(2): 37&ndash;48.
doi:10.1525/jer.2009.4.2.37</li>

<li>Buchanan, Elizabeth, John Aycock, Scott Dexter, David Dittrich,
and Erin Hvizdak, 2011, &ldquo;Computer Science Security Research and
Human Subjects: Emerging Considerations for Research Ethics
Boards&rdquo;, <em>Journal of Empirical Research on Human Research
Ethics</em>, 6(2): 71&ndash;83. doi:10.1525/jer.2011.6.2.71</li>

<li>Carpenter, Katherine J. and David Dittrich, 2012, &ldquo;Bridging
the Distance: Removing the Technology Buffer and Seeking Consistent
Ethical Analysis in Computer Security Research&rdquo;, in <em>Digital
Ethics: Research &amp; Practice</em> (Digital Formations 85), Don
Heider and Adrienne Massanari (eds.), New York: Peter Lang, pp.
1&ndash;29.</li>

<li>[CASRO] Council of American Survey Research, 2011, &ldquo;CASRO
Code of Standards and Ethics for Survey Research&rdquo;, First adopted
1977 and revised since.
 [<a href="https://www.ftc.gov/sites/default/files/documents/public_comments/preliminary-ftc-staff-report-protecting-consumer-privacy-era-rapid-change-proposed-framework/00356-57963.pdf" target="other">CASRO code available online</a>]</li>
 
<li>Chen, Jenny J., Natala J. Menezes, and Adam D. Bradley, 2011,
&ldquo;Opportunities for Crowdsourcing Research on Amazon Mechanical
Turk&rdquo;, <em>Interfaces</em>, 5(3).
 [<a href="https://www.researchgate.net/publication/228954187_Opportunities_for_Crowdsourcing_Research_on_Amazon_Mechanical_Turk" target="other">J. Chen, Menezes, and Bradley 2011 available online</a>]</li>
 
<li>Colvin, Jan and Jane Lanigan, 2005, &ldquo;Ethical Issues and Best
Practice Considerations for Internet Research&rdquo;, <em>Journal of
Family and Consumer Sciences</em>, 97(3): 34&ndash;39.</li>

<li>Conley, Caryn and Jennifer Tosti-Kharas, 2014,
&ldquo;Crowdsourcing Content Analysis for Managerial Research&rdquo;,
<em>Management Decision</em>, 52(4): 675&ndash;688.
doi:10.1108/MD-03-2012-0156</li>

<li>Consalvo, Mia and Charles Ess (eds.), 2011, <em>The Handbook of
Internet Studies</em>, Oxford: Wiley-Blackwell.
doi:10.1002/9781444314861</li>

<li>Crawford, Kate and Jason Schultz, 2014, &ldquo;Big Data and Due
Process: Toward a Framework to Redress Predictive Privacy
Harms&rdquo;, <em>Boston College Law Review</em>, 55(1):
93&ndash;128.</li>

<li>Dittrich, David, Michael Bailey, and Sven Dietrich, 2011,
&ldquo;Building an Active Computer Security Ethics Community&rdquo;,
<em>IEEE Security &amp; Privacy Magazine</em>, 9(4): 32&ndash;40.
doi:10.1109/MSP.2010.199</li>

<li>[DMA] Digital Markets Act, 2022, &ldquo;Regulation (EU) 2022/1925
of the European Parliament and of the Council of 14 September 2022 on
Contestable and Fair Markets in the Digital Sector and Amending
Directives (EU) 2019/1937 and (EU) 2020/1828 (Digital Markets
Act)&rdquo;.
 [<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=uriserv:OJ.L_.2022.265.01.0001.01.ENG">available online</a>]</li>
 
<li>[DSA] Digital Services Act, 2022, &ldquo;Regulation (EU) 2022/2065
of the European Parliament and of the Council of 19 October 2022 on a
Single Market For Digital Services and Amending Directive 2000/31/EC
(Digital Services Act)&rdquo;.
 [<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32022R2065">available online</a>]</li>
 
<li>Elgesem, Dag, 2002, &ldquo;What Is Special about the Ethical
Issues in Online Research?&rdquo;, <em>Ethics and Information
Technology</em>, 4(3): 195&ndash;203. doi:10.1023/A:1021320510186</li>

<li>Emanuel, Ezekiel J., Robert A. Crouch, John D. Arras, Jonathan D.
Moreno, and Christine Grady (eds.), 2003, <em>Ethical and Regulatory
Aspects of Clinical Research: Readings and Commentary</em>, Baltimore:
Johns Hopkins University Press.</li>

<li>Ess, Charles, 2016, &ldquo;Phronesis for Machine Ethics? Can
Robots Perform Ethical Judgments?&rdquo;, <em>Frontiers in Artificial
Intelligence and Applications</em>, 290: 386&ndash;389.
doi:10.3233/978-1-61499-708-5-386</li>

<li>Ess, Charles and the Association of Internet Researchers (AoIR)
Ethics Working committee, 2002, &ldquo;Ethical Decision-Making and
Internet Research: Recommendations from the AoIR Ethics Working
Committee&rdquo;, Approved by the AoIR, 27 November 2002.
 [<a href="http://aoir.org/reports/ethics.pdf" target="other">Ess and AoIR 2002 available online</a>]</li>
 
<li>Eysenbach, Gunther, 1999, &ldquo;Welcome to the Journal of Medical
Internet Research&rdquo;, <em>Journal of Medical Internet
Research</em>, 1(1): e5. doi:10.2196/jmir.1.1.e5</li>

<li>Eysenbach, Gunther and James E. Till, 2001, &ldquo;Ethical Issues
in Qualitative Research on Internet Communities&rdquo;, <em>BMJ</em>,
323(7321): 1103&ndash;1105. doi:10.1136/bmj.323.7321.1103</li>

<li>Fairfield, Joshua A., 2012, &ldquo;Avatar Experimentation: Human
Subjects Research in Virtual Worlds&rdquo;, <em>U.C. Irvine Law
Review</em>, 2: 695&ndash;772.</li>

<li>Federal Register, 2011, &ldquo;Submission for Review and Comment:
&lsquo;The Menlo Report: Ethical Principles Guiding Information and
Communication Technology Research&rsquo;&rdquo; (&ldquo;Menlo
Report&rdquo;) for the Department of Homeland Security (DHS), Science
and Technology, Cyber Security Division (CSD), Protected Repository
for the Defense of Infrastructure Against Cyber Threats (PREDICT), 28
December 2011, Volume 76, Number 249, Docket No. DHS-2011-0074.
 [<a href="https://www.federalregister.gov/documents/2011/12/28/2011-33231/submission-for-review-and-comment-the-menlo-report-ethical-principles-guiding-information-and" target="other">Federal Register 2011 available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Federal Policy for the
Protection of Human Subjects&rdquo;, 19 January 2017, Volume 82,
Number 12
 [<a href="https://www.federalregister.gov/documents/2017/01/19/2017-01058/federal-policy-for-the-protection-of-human-subjects" target="other">Federal Register 2017 available online</a>]</li>
 
<li>Fiesler, Casey, Christopher Frauenberger, Michael Muller, Jessica
Vitak, and Michael Zimmer, 2022. &ldquo;Research Ethics in HCI: A
SIGCHI Community Discussion&rdquo;, <em>Extended Abstracts of the 2022
CHI Conference on Human Factors in Computing Systems</em> (<em>CHI EA
&rsquo;22</em>), 1&ndash;3</li>

<li>Fiesler, Casey and Nicholas Proferes, 2018,
&ldquo;&lsquo;Participant&rsquo; Perceptions of Twitter Research
Ethics&rdquo;, <em>Social Media + Society</em>, 4(1), first online 10
March 2018. doi:10.1177/2056305118763366</li>

<li>Fiesler, Casey, Michael Zimmer, Nicholas Proferes, Sarah Gilbert,
and Naiyan Jones, 2024. &ldquo;Remember the Human: A Systematic Review
of Ethical Considerations in Reddit Research.&rdquo; <em>Proceedings
of the ACM on Human-Computer Interaction</em> 8 (GROUP), New York:
Association of Computing Machinery, 1&ndash;33.</li>

<li>Flicker, Sarah, Dave Haans, and Harvey Skinner, 2004,
&ldquo;Ethical Dilemmas in Research on Internet Communities&rdquo;,
<em>Qualitative Health Research</em>, 14(1): 124&ndash;134.
doi:10.1177/1049732303259842</li>

<li>Fossheim, Hallvard and Helene Ingierd (eds.), 2016, <em>Internet
Research Ethics:</em>, Oslo: Cappelen Damm Akademisk/NOASP.
doi:10.17585/noasp.3.1</li>

<li>Frankel, Mark S. and Sanyin Siang, 1999, &ldquo;Ethical and Legal
Aspects of Human Subjects Research in Cyberspace&rdquo;, A Report of a
Workshop, 10&ndash;11 June 1999, Washington, DC: American Association
for the Advancement of Science.
 [<a href="https://www.aaas.org/sites/default/files/report2.pdf" target="other">Frankel and Siang 1999 available online</a>]</li>
 
<li>Franzke, Aline Shakti, Anja Bechmann, Michael Zimmer, Charles M.
Ess, and the Association of Internet Researchers (AoIR), 2020,
<em>Internet Research: Ethical Guidelines 3.0</em>, AoIR.
 [<a href="https://aoir.org/reports/ethics3.pdf" target="other">Franzke et al. available online (pdf)</a>]</li>
 
<li>Frauenberger, Christopher, Amy S. Bruckman, Cosmin Munteanu,
Melissa Densmore, and Jenny Waycott, 2017, &ldquo;Research Ethics in
HCI: A Town Hall Meeting&rdquo;, in <em>Proceedings of the 2017 CHI
Conference Extended Abstracts on Human Factors in Computing Systems
&ndash; CHI EA &rsquo;17</em>, Denver: ACM Press, pp. 1295&ndash;1299.
doi:10.1145/3027063.3051135</li>

<li>Gaw, Allan and Michael H. J. Burns, 2011, <em>On Moral Grounds:
Lessons from the History of Research Ethics</em>, Westerwood, Glasgow:
SA Press.</li>

<li>[GDPR] General Data Protection Regulation (GDPR), 2016,
&ldquo;Regulation (EU) 2016/679 of the European Parliament and of the
Council of 27 April 2016 on the protection of natural persons with
regard to the processing of personal data and on the free movement of
such data, and repealing Directive 95/46&rdquo;.
 [<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&amp;from=EN">available online</a>]</li>
 
<li>Gilbert, Brendan James, 2009, &ldquo;Getting to Conscionable:
Negotiating Virtual Worlds&rsquo; End User License Agreements without
Getting Externally Regulated&rdquo;, <em>Journal of International
Commercial Law and Technology</em>, 4(4): 238&ndash;251.
 [<a href="https://www.neliti.com/publications/28835/getting-to-conscionable-negotiating-virtual-worlds%C3%A2tm-end-user-license-agreement#cite" target="other">Gilbert 2009 available online</a>]</li>
 
<li>Glickman, Seth W., Sam Galhenage, Lindsay McNair, Zachry Barber,
Keyur Patel, Kevin A. Schulman, and John G. McHutchison, 2012,
&ldquo;The Potential Influence of Internet-Based Social Networking on
the Conduct of Clinical Research Studies&rdquo;, <em>Journal of
Empirical Research on Human Research Ethics</em>, 7(1): 71&ndash;80.
doi:10.1525/jer.2012.7.1.71</li>

<li>[HHS] Health and Human Services, 2017, &ldquo;Excerpts from the
January 19, 2017 Revised Common Rule Preamble&rdquo;.
 [<a href="https://www.hhs.gov/ohrp/regulations-and-policy/regulations/2018-req-preamble/" target="other">HHS 2017 available online</a>]</li>
 
<li>Hill, Kashmir, 2014, &ldquo;Facebook Added &lsquo;Research&rsquo;
To User Agreement 4 Months After Emotion Manipulation Study&rdquo;,
<em>Forbes.com.</em>, 30 June 2014.
 [<a href="https://www.forbes.com/sites/kashmirhill/2014/06/30/facebook-only-got-permission-to-do-research-on-users-after-emotion-manipulation-study/#23483d1a10c1" target="other">Hill 2014 available online</a>]</li>
 
<li>Hoffmann, Anna Lauren, 2016, &ldquo;Facebook has a New Process for
Discussing Ethics. But is It Ethical?&rdquo; <em>The Guardian</em>, 17
June 2016.
 [<a href="https://www.theguardian.com/technology/2016/jun/17/facebook-ethics-but-is-it-ethical" target="other">Hoffmann 2016 available online</a>]</li>
 
<li>Homeland Security Department, 2011, &ldquo;Submission for Review
and Comment: &lsquo;The Menlo Report: Ethical Principles Guiding
Information and Communication Technology Research&rsquo;&rdquo;,
<em>Federal Register: The Daily Journal of the United States
Government</em>, FR Doc. 2011&ndash;3323, 28 December 2011.
 [<a href="https://www.federalregister.gov/articles/2011/12/28/2011-33231/submission-for-review-and-comment-the-menlo-report-ethical-principles-guiding-information-and" target="other">Homeland Security Department 2011 available online</a>].</li>
 
<li>Hudson, James M. and Amy Bruckman, 2004, &ldquo;&lsquo;Go
Away&rsquo;: Participant Objections to Being Studied and the Ethics of
Chatroom Research&rdquo;, <em>The Information Society</em>, 20(2):
127&ndash;139. doi:10.1080/01972240490423030</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;Using Empirical Data to Reason
about Internet Research Ethics&rdquo;, in <em>ECSCW 2005: Proceedings
of the Ninth European Conference on Computer-Supported Cooperative
Work, 18&ndash;22 September 2005, Paris, France</em>, Hans Gellersen,
Kjeld Schmidt, Michel Beaudouin-Lafon, and Wendy Mackay (eds.),
Berlin/Heidelberg: Springer-Verlag, pp. 287&ndash;306.
doi:10.1007/1-4020-4023-7_15</li>

<li>Hunsinger, Jeremy, Lisbeth Klastrup, and Matthew Allen (eds.),
2010, <em>International Handbook of Internet Research</em>, Dordrecht:
Springer Netherlands. doi:10.1007/978-1-4020-9789-8</li>

<li>Illingworth, Nicola, 2001, &ldquo;The Internet Matters: Exploring
the Use of the Internet as a Research Tool&rdquo;, <em>Sociological
Research Online</em>, 6(2): 79&ndash;90. doi:10.5153/sro.600
 [<a href="https://www.socresonline.org.uk/6/2/illingworth.html" target="other">Illingworth 2001 available online</a>]</li>
 
<li>International Telecommunications Union, 2019, &ldquo;New ITU Data
Reveal Growing Internet Uptake but a Widening Digital Gender
Divide&rdquo;, <em>ITU Media Centre</em>,
 [<a href="https://www.itu.int/en/mediacentre/Pages/2019-PR19.aspx" target="other">ITU 2019 available online</a>]</li>
 
<li>Jackman, Molly and Lauri Kanerva, 2016, &ldquo;Evolving the IRB:
Building Robust Review for Industry Research&rdquo;, <em>Washington
and Lee Law Review Online</em>, 72(3): 442&ndash;457.</li>

<li>Jacobson, David, 1999, &ldquo;Doing Research in Cyberspace&rdquo;,
<em>Field Methods</em>, 11(2): 127&ndash;145.
doi:10.1177/1525822X9901100204</li>

<li>Johns, Mark D., Shing-Ling Sarina Chen, and G. Jon Hall (eds.),
2003, <em>Online Social Research: Methods, Issues, and Ethics</em>,
New York: Peter Lang.</li>

<li>Jones, Arnita, 2008, &ldquo;AHA Statement on IRB&rsquo;s and Oral
History Research&rdquo;, <em>American Historical Association
Activities</em>, 1 February 2008.
 [<a href="https://www.historians.org/publications-and-directories/perspectives-on-history/february-2008/aha-statement-on-irbs-and-oral-history-research" target="other">A. Jones 2008 available online</a>]</li>
 
<li>Jones, Steve (ed.), 1999, <em>Doing Internet Research: Critical
Issues and Methods for Examining the Net</em>, Thousand Oaks, CA:
Sage.</li>

<li>Kaplan, Andreas M. and Michael Haenlein, 2010, &ldquo;Users of the
World, Unite! The Challenges and Opportunities of Social Media&rdquo;,
<em>Business Horizons</em>, 53(1): 59&ndash;68.
doi:10.1016/j.bushor.2009.09.003</li>

<li>Kerry, Cameron F., 2020, &ldquo;Protecting Privacy in an AI-driven
World&rdquo; (AI Governance), 10 February 2020, Center for Technology
Innovation, Brookings Institute.
 [<a href="https://www.brookings.edu/research/protecting-privacy-in-an-ai-driven-world/" target="other">Kerry 2020 available online</a>]</li>
 
<li>Keyes, Os, 2019, &ldquo;Counting the Countless: Why Data Science
is a Profound Threat for Queer People&rdquo;, <em>Real Life</em>, 8
April 2019.
 [<a href="https://reallifemag.com/counting-the-countless/" target="other">Keyes 2019 available online</a>]</li>
 
<li>King, Jennifer and Caroline Meinhardt, 2024 &ldquo;Rethinking
Privacy in the AI Era: Policy Provocations for a Data-Centric
World&rdquo; (White Paper), Stanford: Stanford Institute for
Human-Centered Artificial Intelligence.
 [<a href="https://hai.stanford.edu/policy/white-paper-rethinking-privacy-ai-era-policy-provocations-data-centric-world">King and Meinhardt 2024 available online</a>]</li>
 
<li>King, Storm A., 1996, &ldquo;Researching Internet Communities:
Proposed Ethical Guidelines for the Reporting of Results&rdquo;,
<em>The Information Society</em>, 12(2): 119&ndash;128.
doi:10.1080/713856145</li>

<li>Kitchin, Heather A., 2003, &ldquo;The Tri-Council Policy Statement
and Research in Cyberspace: Research Ethics, the Internet, and
Revising a &lsquo;Living Document&rsquo;&rdquo;, <em>Journal of
Academic Ethics</em>, 1(4): 397&ndash;418.
doi:10.1023/B:JAET.0000025671.83557.fa</li>

<li>&ndash;&ndash;&ndash;, 2008, <em>Research Ethics and the Internet:
Negotiating Canada&rsquo;s Tri-Council&rsquo;s Policy</em>, Winnipeg,
Manitoba: Fernwood Publishing</li>

<li>Kramer, Adam D. I., James E. Guillory, and Jeffrey T. Hancock,
2014, &ldquo;Experimental Evidence of Massive-Scale Emotional
Contagion through Social Networks&rdquo;, <em>Proceedings of the
National Academy of Sciences</em>, 111(24): 8788&ndash;8790.
doi:10.1073/pnas.1320040111</li>

<li>Kraut, Robert, Judith Olson, Mahzarin Banaji, Amy Bruckman,
Jeffrey Cohen, and Mick Couper, 2004, &ldquo;Psychological Research
Online: Report of Board of Scientific Affairs&rsquo; Advisory Group on
the Conduct of Research on the Internet.&rdquo;, <em>American
Psychologist</em>, 59(2): 105&ndash;117.
doi:10.1037/0003-066X.59.2.105</li>

<li>Krogstad, Donald J., Samba Diop, Amadou Diallo, Fawaz Mzayek,
Joseph Keating, Ousmane A. Koita, and Y&eacute;ya T. Tour&eacute;,
2010, &ldquo;Informed Consent in International Research: The Rationale
for Different Approaches&rdquo;, <em>The American Journal of Tropical
Medicine and Hygiene</em>, 83(4): 743&ndash;747.
doi:10.4269/ajtmh.2010.10-0014</li>

<li>Lawson, Danielle, 2004, &ldquo;Blurring the Boundaries: Ethical
Considerations for Online Research Using Synchronous CMC
Forums&rdquo;, in Buchanan 2004: 80&ndash;100.</li>

<li>Leibovici, Didier G., Suchith Anand, Jerry Swan, James Goulding,
Gobe Hobona, Lucy Bastin, Sergiusz Pawlowicz, Mike Jackson, and
Richard James, 2010, &ldquo;Workflow Issues for Health Mapping
&lsquo;Mashups&rsquo; of OGC&rdquo;, University of Nottingham, CGS
Technical Report, 2010 DL1.
 [<a href="https://www.researchgate.net/publication/282698491_Workflow_issues_for_Health-mapping_mashups" target="other">Leibovici et al. 2010 available online</a>]</li>
 
<li>Madejski, Michelle, Maritza Lupe Johnson, and Steven Michael
Bellovin, 2011, &ldquo;The Failure of Online Social Network Privacy
Settings&rdquo;. Columbia Research Report CUCS-010-11, Columbia
University. doi:10.7916/D8NG4ZJ1</li>

<li>Mann, Chris, 2003, &ldquo;Generating Data Online: Ethical Concerns
and Challenges for the C21 Researcher&rdquo;, in Thorseth 2003:
31&ndash;49.</li>

<li>Markham, Annette N., 1998, <em>Life Online: Researching Real
Experience in Virtual Space</em>, Walnut Creek, CA: Altamira
Press.</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Fabrication as Ethical
Practice: Qualitative Inquiry in Ambiguous Internet Contexts&rdquo;,
<em>Information, Communication &amp; Society</em>, 15(3):
334&ndash;353. doi:10.1080/1369118X.2011.641993</li>

<li>Markham, Annette N. and Nancy K. Baym (eds.), 2008, <em>Internet
Inquiry: Conversations about Method</em>, Thousand Oaks, CA: Sage
Publications.</li>

<li>Markham, Annette, N. and Elizabeth Buchanan, 2012. : <em>Ethical
Decision-making and Internet Research: Version 2.0. Recommendations
from the AoIR Ethics Working Committee</em> Association of Internet
Researchers.
 [<a href="https://aoir.org/reports/ethics2.pdf">Markham and Buchanan 2012 available online</a>]</li>
 
<li>Markoff, John, 2010, &ldquo;In a Video Game, Tackling the
Complexities of Protein Folding&rdquo;, <em>New York Times</em>, 4
August 2010.
 [<a href="http://www.nytimes.com/2010/08/05/science/05protein.html" target="other">Markoff 2010 available online</a>]</li>
 
<li>Mayer, Jonathan. R. and John. C. Mitchell, 2012,
&ldquo;Third-Party Web Tracking: Policy and Technology&rdquo;,
<em>2012 IEEE Symposium on Security and Privacy</em>, 413&ndash;427.
 [<a href="https://doi.org/10.1109/SP.2012.47/" target="other">Mayer &amp; Mitchell 2012 available online</a></li>
 
<li>McKee, Heidi A. and James E. Porter, 2009, <em>The Ethics of
Internet Research: A Rhetorical, Case-based Process</em>, New York:
Peter Lang Publishing.</li>

<li>Mendelson, Cindy, 2007, &ldquo;Recruiting Participants for
Research From Online Communities&rdquo;, <em>CIN: Computers,
Informatics, Nursing</em>, 25(6): 317&ndash;323.
doi:10.1097/01.NCN.0000299653.13777.51</li>

<li>Metcalf, Jacob, 2017, &ldquo;&lsquo;The Study Has Been Approved by
the IRB&rsquo;: Gayface AI, Research Hype and the Pervasive Data
Ethics Gap&rdquo;. PERVADE Team: Pervasive Data Ethics for
Computational Research, Report.
 [<a href="https://medium.com/pervade-team/the-study-has-been-approved-by-the-irb-gayface-ai-research-hype-and-the-pervasive-data-ethics-ed76171b882c" target="other">Metcalf 2017 available online</a>]</li>
 
<li>Metcalf, Jacob, Emanuel Moss, and danah boyd, 2019, &ldquo;Owning
Ethics: Corporate Logics, Silicon Valley, and the Institutionalization
of Ethics&rdquo;, <em>Social Research: An International
Quarterly</em>, 86(2): 449&ndash;476.</li>

<li>Milne, George R. and Mary J. Culnan, 2004, &ldquo;Strategies for
Reducing Online Privacy Risks: Why Consumers Read (or Don&rsquo;t
Read) Online Privacy Notices&rdquo;, <em>Journal of Interactive
Marketing</em>, 18(3): 15&ndash;29. doi:10.1002/dir.20009</li>

<li>Mondschein, Christopher F. and Cosimo Monda, 2018, &ldquo;The
EU&rsquo;s General Data Protection Regulation (GDPR) in a Research
Context&rdquo;, in <em>Fundamentals of Clinical Data Science</em>,
Pieter Kubben, Michel Dumontier, and Andre Dekker (eds.), Cham:
Springer International Publishing, pp. 55&ndash;71.
doi:10.1007/978-3-319-99713-1_5</li>

<li>Moor, James H., 1985, &ldquo;What Is Computer Ethics?&rdquo;,
<em>Metaphilosophy</em>, 16(4): 266&ndash;275.
doi:10.1111/j.1467-9973.1985.tb00173.x</li>

<li>Alexander, Larry and Michael Moore, 2007, &ldquo;Deontological
Ethics&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em>
(Winter 2007 Edition), Edward N. Zalta (ed.). URL =
 &lt;<a href="https://plato.stanford.edu/archives/win2007/entries/ethics-deontological/" target="other">https://plato.stanford.edu/archives/win2007/entries/ethics-deontological/</a>&gt;</li>
 
<li>Narayanan, Arvind and Vitaly Shmatikov, 2008, &ldquo;Robust
De-anonymization of Large Sparse Datasets&rdquo;, <em>Proceedings of
the 29<sup>th</sup> IEEE Symposium on Security and Privacy, Oakland,
CA, May 2008</em>, IEEE, pp. 111&ndash;125. doi:10.1109/SP.2008.33
 [<a href="http://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf" target="other">Narayanan and Shmatikov 2008 available online (pdf)</a>]</li>
 
<li>[NCPHSBBR] The National Commission for the Protection of Human
Subjects of Biomedical and Behavioral Research, 1979, &ldquo;The
Belmont Report: Ethical Principles and Guidelines for the Protection
of Human Subjects of Research&rdquo;, Office for Human Research
Protections, Department of Health and Human Services, United States.
 [<a href="http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html" target="other">NCPHSBBR 1979 available online</a>]</li>
 
<li>[NESH] The National Committee for Research Ethics in the Social
Sciences and the Humanities [Norway], 2006, &ldquo;Guidelines for
Research Ethics in the Social Sciences, Law, and Humanities&rdquo;,
Published September 2006.
 [<a href="http://graduateschool.nd.edu/assets/21765/guidelinesresearchethicsinthesocialscienceslawhumanities.pdf" target="other">NESH 2006 available online</a>].</li>
 
<li>&ndash;&ndash;&ndash;, 2019, &ldquo;A Guide to Internet Research
Ethics&rdquo;.
 [<a href="https://www.forskningsetikk.no/en/guidelines/social-sciences-humanities-law-and-theology/a-guide-to-internet-research-ethics/" target="other">NESH 2019 available online</a>].</li>
 
<li>Nissenbaum, Helen, 2009, <em>Privacy in Context: Technology,
Policy, and the Integrity of Social Life</em>, Stanford, CA: Stanford
University Press.</li>

<li>[NSTC] National Science and Technology Council, 2016,
&ldquo;National Privacy Research Strategy&rdquo;, Office of the
President of the United States, June 2016.
 [<a href="https://www.nitrd.gov/pubs/NationalPrivacyResearchStrategy.pdf" target="other">NSTC 2016 available online</a>]</li>
 
<li>Nuremberg Code, 1947 (1996), &ldquo;The Nuremberg Code&rdquo;,
<em>BMJ</em>, 313. doi:https://doi.org/10.1136/bmj.313.7070.1448</li>

<li>Ohm, Paul, 2010, &ldquo;Broken Promises of Privacy: Responding to
the Surprising Failure of Anonymization&rdquo;, <em>UCLA Law
Review</em>, 57: 1701&ndash;1777.</li>

<li>Overbeke, Grace, 2008, &ldquo;Pro-Anorexia Websites: Content,
Impact, and Explanations of Popularity&rdquo;, <em>Mind Matters: The
Wesleyan Journal of Psychology</em>, 3: 49&ndash;62.
 [<a href="https://www.academia.edu/6383345/Pro-Anorexia_Websites_Content_Impact_and_Explanations_of_Popularity" target="other">Overbeke 2008 available online</a>]</li>
 
<li>[PRIM&amp;R] Public Responsibility in Medicine and Research,
Bankert, E., Gordon, B., Hurley, E., and Shriver, S. (eds), 2021,
<em>Institutional Review Board: Management and Function</em> (third
edition). Burlington, MA: Jones and Bartlett.</li>

<li>Reid, Elizabeth, 1996, &ldquo;Informed Consent in the Study of
On-Line Communities: A Reflection on the Effects of Computer-Mediated
Social Research&rdquo;, <em>The Information Society</em>, 12(2):
169&ndash;174. doi:10.1080/713856138</li>

<li>Reynolds, Ren, and Melissa de Zwart, 2010, &ldquo;The Duty to
&lsquo;Play&rsquo;: Ethics, EULAs and MMOs&rdquo;, <em>International
Journal of Internet Research Ethics</em>, 3(1): 48&ndash;68.
 [<a href="https://www.globethics.net/gel/4410705" target="other">Reynolds &amp; de Zwart 2010 available online</a>]</li>
 
<li>Ritchie, Donald A., 2003, <em>Doing Oral History: A Practical
Guide</em>, New York: Oxford University Press.</li>

<li>Rosenberg, &Aring;sa, 2010, &ldquo;Virtual World Research Ethics
and the Private/Public Distinction&rdquo;, <em>International Journal
of Internet Research Ethics</em>, 3(1): 23&ndash;37.</li>

<li>Rosser, B. R. Simon, J. Michael Oakes, Joseph Konstan, Simon
Hooper, Keith J. Horvath, Gene P. Danilenko, Katherine E. Nygaard, and
Derek J. Smolenski, 2010, &ldquo;Reducing HIV Risk Behavior of Men Who
Have Sex with Men through Persuasive Computing: Results of the
Men&#700;s INTernet Study-II&rdquo;, <em>AIDS</em>, 24(13):
2099&ndash;2107. doi:10.1097/QAD.0b013e32833c4ac7</li>

<li>[SACHRP] Secretary&rsquo;s Advisory Committee to the Office for
Human Research Protections, United States Department of Health &amp;
Human Services, 2010,
 &ldquo;<a href="https://web.archive.org/web/20170209180021/http://archive.hhs.gov/ohrp/sachrp/mtgings/mtg07-10/present.html" target="other">SACHRP July 20&ndash;21, 2010 Meeting Presentations</a>&rdquo;.</li>
 
<li>&ndash;&ndash;&ndash;, 2013,
 &ldquo;<a href="https://www.hhs.gov/ohrp/sachrp-committee/recommendations/2013-may-20-letter-attachment-b/index.html" target="other">Attachment B: Considerations and Recommendations concerning Internet Research and Human Subjects Research Regulations, with Revisions</a>&rdquo;,
 Final document approved 12&ndash;13 March 2013.
 (<a href="https://www.hhs.gov/ohrp/sites/default/files/ohrp/sachrp/mtgings/2013%20March%20Mtg/internet_research.pdf" target="other">SACHRP 2013 pdf version</a>)</li>
 
<li>&ndash;&ndash;&ndash;, 2013, &ldquo;Considerations and
Recommendations Concerning Internet Research and Human Subjects
Research Regulations, with Revisions&rdquo;.
 [<a href="https://www.hhs.gov/ohrp/sites/default/files/ohrp/sachrp/mtgings/2013%20March%20Mtg/internet_research.pdf">SACHRP 2013 available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 2015,
 &ldquo;<a href="https://www.hhs.gov/ohrp/sachrp-committee/recommendations/2015-april-24-attachment-a/index.html" target="other">Attachment A: Human Subjects Research Implications of &lsquo;Big Data&rsquo; Studies</a>&rdquo;,
 24 April 2015.</li>

<li>Samuel, Gabrielle and Elizabeth Buchanan, 2020, &ldquo;Guest
Editorial: Ethical Issues in Social Media Research&rdquo;, <em>Journal
of Empirical Research on Human Research Ethics</em>, 15(1&ndash;2):
3&ndash;11. doi:10.1177/1556264619901215</li>

<li>Scholz, Trebor, 2008, &ldquo;Market Ideology and the Myths of Web
2.0&rdquo;, <em>First Monday</em>, 13(3): 3 March 2008.
 [<a href="https://firstmonday.org/ojs/index.php/fm/article/view/2138" target="other">Scholz 2008 available online</a>]</li>
 
<li>Schwartz, Paul M. and Daniel J. Solove, 2011, &ldquo;The PII
Problem: Privacy and a New Concept of Personally Identifiable
Information&rdquo;, <em>New York University Law Review</em>, 86(6):
1814&ndash;1893.</li>

<li>Seaboldt, James A. and Randy Kuiper, 1997, &ldquo;Comparison of
Information Obtained from a Usenet Newsgroup and from Drug Information
Centers&rdquo;, <em>American Journal of Health-System Pharmacy</em>,
54(15): 1732&ndash;1735. doi:10.1093/ajhp/54.15.1732</li>

<li>Sharf, Barbara F., 1997, &ldquo;Communicating Breast Cancer
On-Line: Support and Empowerment on the Internet&rdquo;, <em>Women
&amp; Health</em>, 26(1): 65&ndash;84. doi:10.1300/J013v26n01_05</li>

<li>Shilton, Katie, Emanuel Moss, Sarah A. Gilbert, Matthew J. Bietz,
Casey Fiesler, Jacob Metcalf, Jessica Vitak, and Michael Zimmer, 2021.
&ldquo;Excavating Awareness and Power in Data Science: A Manifesto for
Trustworthy Pervasive Data Research.&rdquo; <em>Big Data &amp;
Society</em>, 8(2). doi:10.1177/20539517211040759</li>

<li>Sieber, Joan E., 1992, <em>Planning Ethically Responsible
Research: A Guide for Students and Internal Review Boards</em>,
Thousand Oaks, CA: Sage.</li>

<li>&ndash;&ndash;&ndash;, 2015, <em>Planning Ethically Responsible
Research: A Guide for Students and Internal Review Boards</em>, second
edition, Thousand Oaks, CA: Sage.</li>

<li>Silberman, M. Six, Lilly Irani, and Joel Ross, 2010, &ldquo;Ethics
and Tactics of Professional Crowdwork&rdquo;, <em>XRDS: Crossroads:
The ACM Magazine for Students</em>, 17(2): 39&ndash;43.</li>

<li>Skloot, Rebecca, 2010, <em>The Immortal Life of Henrietta
Lacks</em>, New York: Crown Publishers.</li>

<li>Sloan, Luke, Curtis Jessop, Tarek Al Baghal, and Matthew Williams,
2020, &ldquo;Linking Survey and Twitter Data: Informed Consent,
Disclosure, Security, and Archiving&rdquo;, <em>Journal of Empirical
Research on Human Research Ethics</em>, 15(1&ndash;2):
63&ndash;76.</li>

<li>Smith, Michael A. and Brant Leigh, 1997, &ldquo;Virtual Subjects:
Using the Internet as an Alternative Source of Subjects and Research
Environment&rdquo;, <em>Behavior Research Methods, Instruments, &amp;
Computers</em>, 29(4): 496&ndash;505. doi:10.3758/BF03210601</li>

<li>Stahl, Bernd Carsten and David Wright, 2018, &ldquo;Ethics and
Privacy in AI and Big Data: Implementing Responsible Research and
Innovation&rdquo;, <em>IEEE Security &amp; Privacy</em>, 16(3):
26&ndash;33. doi:10.1109/MSP.2018.2701164</li>

<li>Sveningsson, Malin, 2004, &ldquo;Ethics in Internet
Ethnography&rdquo; in Buchanan 2004: 45&ndash;61.</li>

<li>Sweeney, Latanya, 2002, &ldquo;K-Anonymity: A Model for Protecting
Privacy&rdquo;, <em>International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems</em>, 10(5): 557&ndash;570.
doi:10.1142/S0218488502001648</li>

<li>Thomas, Jim, 2004, &ldquo;Reexamining the Ethics of Internet
Research: Facing the Challenge of Overzealous Oversight&rdquo;, in
Johns, Chen, and Hall 2004: 187&ndash;201.</li>

<li>Thorseth, May (ed.), 2003, <em>Applied Ethics in Internet
Research</em> (Programme for Applied Ethics Publication Series No. 1),
Trondheim, Norway: NTNU University Press.</li>

<li>Tsai, Janice, Lorrie Faith Cranor, Alessandro Acquisti, and
Christina M. Fong, 2006, &ldquo;What&rsquo;s It To You? A Survey of
Online Privacy Concerns and Risks&rdquo;. NET Institute Working Paper
No. 06&ndash;29. doi:10.2139/ssrn.941708</li>

<li>Turkle, Sherry,1997, <em>Life on the Screen: Identity in the Age
of the Internet</em>, New York: Touchstone.</li>

<li>Van Heerden, Alastair, Doug Wassenaar, Zaynab Essack, Khanya
Vilakazi, and Brandon A. Kohrt, 2020, &ldquo;In-Home Passive Sensor
Data Collection and Its Implications for Social Media Research:
Perspectives of Community Women in Rural South Africa&rdquo;,
<em>Journal of Empirical Research on Human Research Ethics</em>,
15(1&ndash;2): 97&ndash;107. doi:10.1177/1556264619881334</li>

<li>Vitak, Jessica, Nicholas Proferes, Katie Shilton, and Zahra
Ashktorab, 2017, &ldquo;Ethics Regulation in Social Computing
Research: Examining the Role of Institutional Review Boards&rdquo;,
<em>Journal of Empirical Research on Human Research Ethics</em>,
12(5): 372&ndash;382. doi:10.1177/1556264617725200</li>

<li>Walstrom, Mary K., 2004, &ldquo;Ethics and Engagement in
Communication Scholarship: Analyzing Public, Online Support Groups as
Researcher/Participant-Experiencer&rdquo;, in Buchanan 2004:
174&ndash;202.</li>

<li>Walther, Joseph B., 2002, &ldquo;Research Ethics in
Internet-Enabled Research: Human Subjects Issues and Methodological
Myopia&rdquo;, <em>Ethics and Information Technology</em>, 4(3):
205&ndash;216. doi:10.1023/A:1021368426115</li>

<li>White, Michele, 2002, &ldquo;Representations or People?&rdquo;,
<em>Ethics and Information Technology</em>, 4(3): 249&ndash;266.
doi:10.1023/A:1021376727933</li>

<li>World Medical Association, 1964/2008, &ldquo;Declaration of
Helsinki: Ethical Principles for Medical Research Involving Human
Subjects&rdquo;. Adopted by the 18<sup>th</sup> World Medical
Assembly. Amended 1975, 1983, 1989, 1996, 2000, 2002, 2004, 2008.
 [<a href="https://www.wma.net/what-we-do/medical-ethics/declaration-of-helsinki/" target="other">Declaration of Helsinki available online</a>]</li>
 
<li>Wright, David R., 2006, &ldquo;Research Ethics and Computer
Science: An Unconsummated Marriage&rdquo;, in <em>Proceedings of the
24th Annual Conference on Design of Communication: SIGDOC
&rsquo;06</em>, Myrtle Beach, SC: ACM Press, pp. 196&ndash;201.
doi:10.1145/1166324.1166369</li>

<li>Zimmer, Michael, 2010, &ldquo;&lsquo;But the Data Is Already
Public&rsquo;: On the Ethics of Research in Facebook&rdquo;,
<em>Ethics and Information Technology</em>, 12(4): 313&ndash;325.
doi:10.1007/s10676-010-9227-5</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;OkCupid Study Reveals the
Perils of Big-Data Science&rdquo;, <em>Wired.com</em>, 14 May 2016.
 [<a href="https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science/" target="other">Zimmer 2016 available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Addressing conceptual gaps in
big data research ethics: An application of contextual
integrity,&rdquo; <em>Social Media+ Society</em>, 4(2).
doi:10.1177/205630511876830</li>

<li>Zimmer, Michael and Edward Chapman, 2020, &ldquo;Ethical Review
Boards and Pervasive Data Research: Gaps and Opportunities&rdquo;,
Paper presented at AoIR 2020: The 21st Annual Conference of the
Association of Internet Researchers.
 [<a href="https://spir.aoir.org/ojs/index.php/spir/article/download/11369/9983" target="other">Zimmer and Chapman 2020 extended abstract available online (pdf)</a>]</li>
 
<li>Zimmer, Michael and Katharina Kinder-Kurlanda (eds.), 2017,
<em>Internet Research Ethics for the Social Age: New Challenges,
Cases, and Contexts</em>, New York: Peter Lang Publishing.</li>
</ul>
</div> 

<div id="academic-tools">
<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ethics-internet-research" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/ethics-internet-research/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=ethics-internet-research&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/ethics-internet-research/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>
</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<h3 id="CiteEntr">Cited in Entry</h3>

<ul class="hanging">

<li>Dittrich, David and Erin Kenneally, 2012,
 <a href="https://www.caida.org/publications/papers/2012/menlo_report_actual_formatted/" target="other">The Menlo Report: Ethical Principles Guiding Information and Communication Technology Research</a>,
 Homeland Security, United States Government.</li>

<li>Hoofnagle, Chris Jay and Jennifer King, 2008, &ldquo;What
Californians Understand About Privacy Online&rdquo;, Research Report
from Samuelson Law Technology &amp; Public Policy Clinic, UC Berkeley
Law: Berkeley, CA. doi:10.2139/ssrn.1262130
 [<a href="https://www.law.berkeley.edu/wp-content/uploads/2016/06/Californians.pdf" target="other">Hoofnagle and King 2008 available online</a>]</li>
 
<li>Matias, J. Nathan, Susan Benesch, Rebekah Tromble, Alex Abdo, J.
Bob Alotta, David Karpf, David Lazer, Nathalie Mar&eacute;chal, Nabiha
Syed, and Ethan Zuckerman, 2022, &ldquo;Manifesto: The Coalition for
Independent Technology Research&rdquo;, Coalition for Independent
Technology Research.
 [<a href="https://independenttechresearch.org/manifesto-the-coalition-for-independent-technology-research/" target="other">CITR 2022 available online</a>]</li>
 
<li>[NASEM] National Academies of Sciences, Engineering, and Medicine,
2022, &ldquo;Fostering Responsible Computing Research: Foundations and
Practices&rdquo;. Washington, DC: The National Academies Press.
 [<a href="https://doi.org/10.17226/26507" target="other">https://doi.org/10.17226/26507</a>]</li>
 
<li>[NHRPAC] National Human Subjects Protection Advisory Committee,
2002, p
 &ldquo;<a href="https://web.archive.org/web/20161221060957/http://archive.hhs.gov/ohrp/nhrpac/documents/dataltr.pdf" target="other">Recommendations on Public Use Data Files</a>&rdquo;.</li>
 
<li>[NIH] National Institutes of Health, 2010,
 &ldquo;<a href="https://oma.od.nih.gov/DMS/Documents/Privacy/Guide%20for%20Handling%20Sensitive%20Information%20at%20NIH.pdf" target="other">Guide for Identifying and Handling Sensitive Information at the NIH</a>&rdquo;</li>
 
<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Certificates of
Confidentiality (CoC)&rdquo;, National Institutes of Health
 [<a href="https://grants.nih.gov/policy/humansubjects/coc.htm" target="other">NIH 2021 available online</a>]</li>
 
<li>[NTIA] National Telecommunications and Information Administration,
2024. Ethical Guidelines for Research Using Pervasive Data. 89 FR
99844.
 [<a href="https://www.federalregister.gov/d/2024-29064" target="other">NTIA 2024 available online</a>]</li>
 
<li>Resnik, David and Florian Hofweber, 2025,
 <a href="https://www.niehs.nih.gov/research/resources/bioethics/timeline" target="other">Research Ethics Timeline</a>,
 National Institutes of Health.</li>

<li>Rudder, Christian, 2014, &ldquo;We Experiment on Humans!&rdquo;,
OkTrends, 28 July 2014.
 [<a href="https://web.archive.org/web/20140801010636/http://blog.okcupid.com/index.php/we-experiment-on-human-beings/" target="other">Rudder 2014 available online</a>]</li>
 
<li>Stone, Brad, 2009, &ldquo;Facebook Rolls Out New Privacy
Settings&rdquo;, <em>New York Times</em>, 9 December 2009.
 [<a href="https://bits.blogs.nytimes.com/2009/12/09/facebook-rolls-out-new-privacy-settings/" target="other">Stone 2009 available online</a>]</li>
 
<li>U.K. Data Archive, 2011, &ldquo;Managing and Sharing Data: Best
Practices for Researchers&rdquo;.
 [<a href="https://ukdataservice.ac.uk/media/622417/managingsharing.pdf" target="other">UK Data Archive available online</a>]</li>
 
<li>Williams, George, 2010,
 &ldquo;<a href="https://web.archive.org/web/20101110165848/http://chronicle.com/blogs/profhacker/the-ethics-of-amazons-mechanical-turk/23010" target="other">The Ethics of Amazon&rsquo;s Mechanical Turk</a>&rdquo;,
 ProfHacker Blog, The Chronicle of Higher Education, 1 March
2010.</li>

<li>Zimmer, Michael, 2009, &ldquo;Facebook&rsquo;s Privacy Upgrade is
a Downgrade for User Privacy&rdquo;, MichaelZimmer.Org.
 [<a href="https://michaelzimmer.org/2009/12/10/facebooks-privacy-upgrade-is-a-downgrade-for-user-privacy/" target="other">Zimmer 2009 available online</a>]</li>
 </ul>

<h3 id="LawsGoveDocu">Laws and Government Documents</h3>

<h4>United States</h4>

<ul>

<li>Department of Health and Human Services, Code of Federal
Regulations [C.F.R.], United States

<ul class="hanging">

 <li><a href="https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/" target="other">45 C.F.R. &sect; 46, &ldquo;Protection of Human Subjects&rdquo;,</a>
 and in particular the Common Rule,
 <a href="https://www.ecfr.gov/current/title-45/subtitle-A/subchapter-A/part-46" target="other">45 C.F.R. 46 Subpart A</a></li>
 
 <li><a href="https://www.ecfr.gov/current/title-45/subtitle-A/subchapter-C/part-164/subpart-E/section-164.514" target="other">45 C.F.R. &sect; 164.514, &ldquo;Other requirements relating to uses and disclosures of protected health information&rdquo;</a></li>
 </ul> </li>

<li>[OHRP] U.S Department of Health and Human Services, 2008,
&ldquo;Office for Human Research Protections&rdquo;,
 [<a href="https://www.hhs.gov/ohrp/" target="other">Office for Human Research Protection</a>]</li>
 
 <li><a href="https://videocast.nih.gov/pdf/ohrp_belmont_report.pdf" target="other">The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research</a></li>
 </ul>

<h4>Elsewhere</h4>

<ul>

 <li><a href="https://ethics.gc.ca/eng/policy-politique_tcps2-eptc2_2022.html" target="other">Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans</a>,
 Canada, 2022</li>

<li>European Parliament and Council of European Union (2016)
Regulation (EU) 2016/679. ... Data Protection Act 2018, c. 12
 [<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&amp;from=EN" target="other">Data Protection Act 2018 available online</a>]</li>
 </ul>

<h3 id="ProfStan">Professional Standards</h3>

<ul>

 <li><a href="https://www.counseling.org/docs/default-source/default-document-library/ethics/2014-aca-code-of-ethics.pdf" target="other">American Counseling Association: Ethics and Professional Standards</a>,
 2014 revision</li>

 <li><a href="https://www.apa.org/science/leadership/bsa/internet/" target="other">American Psychological Association: Advisory Group on Conduction Research on the Internet</a></li>
 
 <li><a href="https://aoir.org/ethics/" target="other">Association of Internet Researchers Ethics Guidelines</a></li>
 </ul>

<h3 id="Jour">Journals</h3>

<ul>

 <li><a href="https://www.tandfonline.com/journals/gacr20" target="other">Accountability in Research</a></li>
 
 <li><a href="https://onlinelibrary.wiley.com/journal/25782363" target="other">Ethics &amp; Human Research</a></li>
 
 <li><a href="https://journals.sagepub.com/home/jre" target="other">Journal of Empirical Research on Human Research Ethics</a></li>
 
 <li><a href="https://www.jmir.org" target="other">Journal of Medical Internet Research</a></li>
 
 <li><a href="https://journals.sagepub.com/home/rea" target="other">Research Ethics</a></li>
 </ul>

<h3 id="OtheReso">Other Resources</h3>

<ul>

<li>Bernstein, Michael S., Margaret Levi, David Magnus, Betsy Rajala,
Debra Satz, and Charla Waeiss, 2021, &ldquo;ESR: Ethics and society
review of artificial intelligence research&rdquo; arXiv preprint
arXiv:2106.11521
 [<a href="https://arxiv.org/abs/2106.11521" target="other">Bernstein et al. 2021 available online</a>]</li>
 
 <li><a href="https://www.dlapiperdataprotection.com" target="other">Data Protection Laws of the World</a>,
 DLA Piper.</li>

 <li><a href="https://www.forskningsetikk.no/en/guidelines/" target="other">Research Ethics Guidelines for Internet Research</a>,
 The (Norwegian) National Committee for Research Ethics in the Social
Sciences and the Humanities, 2003.</li>

 <li><a href="https://www.cessda.org/" target="other">Council of European Social Science Data Archives (CESSDA)</a></li>
 
 <li><a href="https://ccnmtl.columbia.edu/projects/cire/pac/foundation/" target="other">Foundation Texts</a>
 of the learning module, <em>Current Issues in Research Ethics:
Privacy and Confidentiality</em>, Joyce Plaza and Ruth Fischbach,
Columbia University, New York: Columbia Center for New Media Teaching
&amp; Learning.</li>

 <li><a href="https://www.aaas.org/resources/ethical-and-legal-aspects-human-subjects-research-cyberspace" target="other">Ethical and Legal Aspects of Human Subjects Research in Cyberspace</a>,
 American Association for the Advancement of Science.</li>
</ul>
</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../clinical-research/">ethics, biomedical: clinical research</a> |
 <a href="../ethics-deontological/">ethics: deontological</a> |
 <a href="../informed-consent/">informed consent</a> |
 <a href="../privacy/">privacy</a>
</p>
</div> 
</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2025</a> by

<br />
Elizabeth A. Buchanan
&lt;<a href="m&#97;ilto:elizabeth&#37;2ebuchanan&#37;40uri&#37;2eedu"><em>elizabeth<abbr title=" dot ">&#46;</abbr>buchanan<abbr title=" at ">&#64;</abbr>uri<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;<br />
<a href="http://michaelzimmer.org" target="other">Michael Zimmer</a>
&lt;<a href="m&#97;ilto:michael&#37;2ezimmer&#37;40marquette&#37;2eedu"><em>michael<abbr title=" dot ">&#46;</abbr>zimmer<abbr title=" at ">&#64;</abbr>marquette<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
            <li role="menuitem"><a href="../../new.html">What's New</a></li>
            <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li role="menuitem"><a href="../../published.html">Chronological</a></li>
            <li role="menuitem"><a href="../../archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
            <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
            <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
            <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
            <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
            <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
            <li role="menuitem"><a href="../../accessibility.html">Accessibility</a></li>
            <li role="menuitem"><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
            <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
            <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2025</a> by <a href="https://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>
</html>
