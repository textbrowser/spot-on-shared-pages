<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-20JYM3ZFN0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-20JYM3ZFN0');
</script>
<title>Virtual Machinations: Using Large Language Models as Neural Computers - ACM Queue</title>
<meta name="description" value />
<meta name="keywords" value="AI" />

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P52H78L');</script>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" href="favicon.ico" />
<script type="text/javascript" src="/js/jquery-1.2.6.min.js"></script>
<script type="text/javascript" src="/js/jquery.validate.min.js"></script>
<script type="text/javascript" src="/js/global.js"></script>

<link rel="alternate" type="application/rss+xml" title="All Queue Content RSS 2.0" href="/rss/feeds/queuecontent.xml" />
<link rel="alternate" type="application/rss+xml" title="Curmudgeon RSS 2.0" href="/rss/feeds/curmudgeon.xml" />
<link rel="alternate" type="application/rss+xml" title="Opinion RSS 2.0" href="/rss/feeds/opinion.xml" />
<link rel="alternate" type="application/rss+xml" title="Kode Vicious RSS 2.0" href="/rss/feeds/kodevicious.xml" />
<link rel="alternate" type="application/rss+xml" title="ACM TechNews RSS" href="https://www.infoinc.com/acm/TechNews.rss" />
<link rel="alternate" type="application/rss+xml" title="Washington Updates RSS" href="https://usacm.acm.org/weblog2/?feed=rss2" />
<link rel="alternate" type="application/rss+xml" title="RISKS Forum RSS" href="/rss/feeds/risksforum.xml" />
<link rel="alternate" type="application/rss+xml" title="AI RSS 2.0" href="/rss/feeds/ai.xml" />
<link rel="alternate" type="application/rss+xml" title="API Design RSS 2.0" href="/rss/feeds/apidesign.xml" />
<link rel="alternate" type="application/rss+xml" title="Bioscience RSS 2.0" href="/rss/feeds/bioscience.xml" />
<link rel="alternate" type="application/rss+xml" title="Blockchain RSS 2.0" href="/rss/feeds/blockchain.xml" />
<link rel="alternate" type="application/rss+xml" title="Business/Management RSS 2.0" href="/rss/feeds/business/management.xml" />
<link rel="alternate" type="application/rss+xml" title="Compliance RSS 2.0" href="/rss/feeds/compliance.xml" />
<link rel="alternate" type="application/rss+xml" title="Component Technologies RSS 2.0" href="/rss/feeds/componenttechnologies.xml" />
<link rel="alternate" type="application/rss+xml" title="Computer Architecture RSS 2.0" href="/rss/feeds/computerarchitecture.xml" />
<link rel="alternate" type="application/rss+xml" title="Concurrency RSS 2.0" href="/rss/feeds/concurrency.xml" />
<link rel="alternate" type="application/rss+xml" title="Cryptocurrency RSS 2.0" href="/rss/feeds/cryptocurrency.xml" />
<link rel="alternate" type="application/rss+xml" title="DSPs RSS 2.0" href="/rss/feeds/dsps.xml" />
<link rel="alternate" type="application/rss+xml" title="Data RSS 2.0" href="/rss/feeds/data.xml" />
<link rel="alternate" type="application/rss+xml" title="Databases RSS 2.0" href="/rss/feeds/databases.xml" />
<link rel="alternate" type="application/rss+xml" title="Debugging RSS 2.0" href="/rss/feeds/debugging.xml" />
<link rel="alternate" type="application/rss+xml" title="Development RSS 2.0" href="/rss/feeds/development.xml" />
<link rel="alternate" type="application/rss+xml" title="Distributed Computing RSS 2.0" href="/rss/feeds/distributedcomputing.xml" />
<link rel="alternate" type="application/rss+xml" title="Distributed Development RSS 2.0" href="/rss/feeds/distributeddevelopment.xml" />
<link rel="alternate" type="application/rss+xml" title="Education RSS 2.0" href="/rss/feeds/education.xml" />
<link rel="alternate" type="application/rss+xml" title="Email and IM RSS 2.0" href="/rss/feeds/emailandim.xml" />
<link rel="alternate" type="application/rss+xml" title="Embedded Systems RSS 2.0" href="/rss/feeds/embeddedsystems.xml" />
<link rel="alternate" type="application/rss+xml" title="Failure and Recovery RSS 2.0" href="/rss/feeds/failureandrecovery.xml" />
<link rel="alternate" type="application/rss+xml" title="File Systems and Storage RSS 2.0" href="/rss/feeds/filesystemsandstorage.xml" />
<link rel="alternate" type="application/rss+xml" title="Game Development RSS 2.0" href="/rss/feeds/gamedevelopment.xml" />
<link rel="alternate" type="application/rss+xml" title="Graphics RSS 2.0" href="/rss/feeds/graphics.xml" />
<link rel="alternate" type="application/rss+xml" title="HCI RSS 2.0" href="/rss/feeds/hci.xml" />
<link rel="alternate" type="application/rss+xml" title="Managing Megaservices RSS 2.0" href="/rss/feeds/managingmegaservices.xml" />
<link rel="alternate" type="application/rss+xml" title="Mobile Computing RSS 2.0" href="/rss/feeds/mobilecomputing.xml" />
<link rel="alternate" type="application/rss+xml" title="Networks RSS 2.0" href="/rss/feeds/networks.xml" />
<link rel="alternate" type="application/rss+xml" title="Object-Relational Mapping RSS 2.0" href="/rss/feeds/object-relationalmapping.xml" />
<link rel="alternate" type="application/rss+xml" title="Open Source RSS 2.0" href="/rss/feeds/opensource.xml" />
<link rel="alternate" type="application/rss+xml" title="Patching and Deployment RSS 2.0" href="/rss/feeds/patchinganddeployment.xml" />
<link rel="alternate" type="application/rss+xml" title="Performance RSS 2.0" href="/rss/feeds/performance.xml" />
<link rel="alternate" type="application/rss+xml" title="Power Management RSS 2.0" href="/rss/feeds/powermanagement.xml" />
<link rel="alternate" type="application/rss+xml" title="Privacy and Rights RSS 2.0" href="/rss/feeds/privacyandrights.xml" />
<link rel="alternate" type="application/rss+xml" title="Processors RSS 2.0" href="/rss/feeds/processors.xml" />
<link rel="alternate" type="application/rss+xml" title="Programming Languages RSS 2.0" href="/rss/feeds/programminglanguages.xml" />
<link rel="alternate" type="application/rss+xml" title="Purpose-built Systems RSS 2.0" href="/rss/feeds/purpose-builtsystems.xml" />
<link rel="alternate" type="application/rss+xml" title="Quality Assurance RSS 2.0" href="/rss/feeds/qualityassurance.xml" />
<link rel="alternate" type="application/rss+xml" title="RFID RSS 2.0" href="/rss/feeds/rfid.xml" />
<link rel="alternate" type="application/rss+xml" title="SIP RSS 2.0" href="/rss/feeds/sip.xml" />
<link rel="alternate" type="application/rss+xml" title="Search Engines RSS 2.0" href="/rss/feeds/searchengines.xml" />
<link rel="alternate" type="application/rss+xml" title="Security RSS 2.0" href="/rss/feeds/security.xml" />
<link rel="alternate" type="application/rss+xml" title="Semi-structured Data RSS 2.0" href="/rss/feeds/semi-structureddata.xml" />
<link rel="alternate" type="application/rss+xml" title="Social Computing RSS 2.0" href="/rss/feeds/socialcomputing.xml" />
<link rel="alternate" type="application/rss+xml" title="System Administration RSS 2.0" href="/rss/feeds/systemadministration.xml" />
<link rel="alternate" type="application/rss+xml" title="System Evolution RSS 2.0" href="/rss/feeds/systemevolution.xml" />
<link rel="alternate" type="application/rss+xml" title="Testing RSS 2.0" href="/rss/feeds/testing.xml" />
<link rel="alternate" type="application/rss+xml" title="Virtual Machines RSS 2.0" href="/rss/feeds/virtualmachines.xml" />
<link rel="alternate" type="application/rss+xml" title="Virtualization RSS 2.0" href="/rss/feeds/virtualization.xml" />
<link rel="alternate" type="application/rss+xml" title="Visualization RSS 2.0" href="/rss/feeds/visualization.xml" />
<link rel="alternate" type="application/rss+xml" title="VoIP RSS 2.0" href="/rss/feeds/voip.xml" />
<link rel="alternate" type="application/rss+xml" title="Web Development RSS 2.0" href="/rss/feeds/webdevelopment.xml" />
<link rel="alternate" type="application/rss+xml" title="Web Security RSS 2.0" href="/rss/feeds/websecurity.xml" />
<link rel="alternate" type="application/rss+xml" title="Web Services RSS 2.0" href="/rss/feeds/webservices.xml" />
<link rel="alternate" type="application/rss+xml" title="Workflow Systems RSS 2.0" href="/rss/feeds/workflowsystems.xml" />
<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-6562869-1']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
<script type="text/javascript">
function plusone_vote( obj ) {
_gaq.push(['_trackEvent','plusone',obj.state]);
}
</script>
<style>
body {
	font-family: jaf-bernino-sans, 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans', Geneva, Verdana, sans-serif;
	color: #333;
	max-width: 100%;
}
div.container p {
	line-height: 1.65em;
}
h1 {
	font-size: 32px;
}
h3 {
	font-size: 18px;
}
h4 {
	font-size: 14px;
}

div.container {
	margin-left: auto;
	margin-right: auto;
}

div {
	margin: 64px;
//	max-width: 800px;
	position: relative;
}

@media only screen and (min-width: 1024px) {
	div {
		max-width: 800px;
	}
}

img {
    max-width: 100%;
    height: auto;
    width: auto\9; /* ie8 */
}
a {
	color: #009;
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
hr {
	margin:64px;
}
label {
	font-size: 0.8em;
	color: #666;
}
input {
	color: #999;
}

/* NAVBAR */
.navbar {
//	position: fixed;
	background: #EEEEEE;
	top: -64px;
	z-index: 10000;
	width: 100%;
	clear: both;
	padding: 0px;
	margin: 0px;
	padding-top: 10px;
	padding-left: 10px;
	padding-right: 10px;
}

/*  SECTIONS  */
.section {
	clear: both;
	padding: 0px;
	margin: 0px;
}

/*  COLUMN SETUP  */
.col {
	display: block;
	float:left;
	margin: 1% 0 1% 1.6%;
}
.col:first-child { margin-left: 0; }


/*  GROUPING  */
.group:before,
.group:after {
	content:"";
	display:table;
}
.group:after {
	clear:both;
}
.group {
    zoom:1; /* For IE 6/7 */
}

/*  GRID OF THREE  */
.span_3_of_3 {
	width: 100%;
}
.span_2_of_3 {
	width: 66.1%;
}
.span_1_of_3 {
	width: 32.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.col {
		margin: 1% 0 1% 0%;
	}
}

@media only screen and (max-width: 480px) {
	.span_3_of_3 {
		width: 100%;
	}
	.span_2_of_3 {
		width: 100%;
	}
	.span_1_of_3 {
		width: 100%;
	}
}

.span_2_of_2 {
	width: 100%;
}

.span_1_of_2 {
	width: 49.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.span_2_of_2 {
		width: 100%;
	}
	.span_1_of_2 {
		width: 100%;
	}
}
</style>
<style>
body {
	font-size: 19px;
}
#form-search > .st-default-search-input {
	width: 170px;
  display: inline-block;
  height: 16px;
  padding: 7px 11px 7px 28px;
  border: 1px solid #bbb;
  border: 1px solid rgba(0,0,0,0.25);
  font-weight: 400;
  color: #3B454F;
  font-size: 14px;
  line-height: 16px;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-box-shadow: none;
  -moz-box-shadow: none;
  box-shadow: none;
  font-family: system, -apple-system, BlinkMacSystemFont, "Helvetica Neue", "Lucida Grande", sans-serif;
}


blockquote
{
    color: #666;
    font-size: 1.1em;
    background: none;
    border-left: .2rem solid #d3d3d3;

    display: block;
    padding: 20px 20px 10px 45px;
    margin: 20px 0;
    font-style: italic;

    margin-block-start: 1em;
    margin-block-end: 1em;
    margin-inline-start: 40px;
    margin-inline-end: 40px;

	font-family: Georgia, Palatino, "Palatino Linotype", Times, "Times New Roman", serif;
}

.ldq {
	display: block;
    padding-left: 10px;
    content: "\201C";
    font-size: 60px;
    position: relative;
    left: -50px;
    top: 0;
    height: 0;
    color: #7a7a7a;
}
code {
//	font-size:1.25em;
}
a {overflow-wrap: break-word;}
</style>
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<div class="container">
<div class="navbar">
<form id="form-search" name="searchform" onsubmit="return false;" style="float:right;">
<input type="text" class="st-default-search-input">
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','UyYECD1kdsPnbHJtPyzG','2.0.0');
</script>
<br/>
<a href="issuedetail.cfm?issue=3695735" style="width:150px;font-size:0.7em;">Current Issue</a> &nbsp; <a href="pastissues.cfm" style="width:150px;font-size:0.7em;">Past Issues</a> &nbsp; <a href="topics.cfm" style="width:150px;font-size:0.7em;">Topics</a>
</form>
<a href="/"><img src="https://queue.acm.org/img/acmqueue_logo.gif" /></a>
</div>

<br/>
<label>July 19, 2024<br/><b><a class="descriptor" href="issuedetail.cfm?issue=3676308">Volume 22, issue 3 </a></b></label>
<p>

&nbsp;
<a href="https://portal.acm.org/citation.cfm?id=3676287">
<img src="img/icon_pdf.png" alt="Download PDF version of this article" />
PDF
</a>
</p>
<h1 class="hidetitle">Virtual Machinations: Using Large Language Models as Neural Computers</h1>
<h2>LLMs can function not only as databases, but also as dynamic, end-user programmable neural computers.</h2>
<h3>Erik Meijer</h3>
<p>Many people view LLMs (large language models) as databases of compressed world knowledge. Just like databases, they deliver answers to given user queries. Rather than rows and columns—as in a relational database—an LLM's knowledge is encoded into billions (or even trillions) of weights that are learned during pretraining and subsequent fine-tuning (see figure 1). Additionally, LLMs are not queried using a synthetic language such as SQL but instead directly by the user, who can engage with them via natural-language conversations.</p>
<img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/meijer1.png" alt="Virtual Machinations: Using Large Language Models as Neural Computers" />
<p>The conceptual equivalent of a query engine inside an LLM is the next-token prediction function <code><span style="font-family:&quot;Cambria Math&quot;,serif">next ∈ token* → ℙ(token)</span></code>, which computes a probability distribution of next-possible tokens <code><span style="font-family:&quot;Cambria Math&quot;,serif">next(q) ∈ ℙ (token)</span></code>, given an initial sequence of tokens <code><span style="font-family:&quot;Cambria Math&quot;,serif">q ∈ token*</span></code>. It is exactly this probabilistic nature of the next function that enables diverse and creative responses, thus differentiating LLMs from traditional databases with their deterministic answers. In the context of this article, tokens can represent any atomic unit in a sequence of items ingested or generated by the model. These units can encompass indices in the vocabulary array of the model handled by the LLM along with any higher-level entities such as words and sentences or, more generally, any <code><span style="color:#0000ff">user</span></code>, <code><span style="color:#009900">assistant</span></code>, or <code><span style="color:gray">system</span></code> message derived from chat conversations.</p>
<p>While the prediction of individual tokens using <code>next</code> forms the foundation of LLM inference, the full completion function <code><span style="fo&#x6;nt-family:&quot;Cambria Math&quot;,serif">LLM ∈ token ∗ → ℙ(token*)</span></code> extends this process to probabilistic generation of multiple tokens to allow a comprehensive response to a given prompt. The implementation of the <code>LLM</code> algorithm relies on various sampling, sometimes called <i>decoding</i>, heuristics such as greedy search, beam search, or random sampling to provide a more interesting answer than would result from just picking the highest-probability option. This sampling process can be controlled using hyper-parameters such as temperature, frequency and presence penalties, logit bias, top-p, and top-k. It is also possible to terminate completion early, based on a set of given stop tokens.<sup>3</sup></p>
<p>If we abstract from the ability of a model to surface probabilities, we can represent <code>next</code> and <code>LLM</code> simply as nondeterministic functions <code><span style="font-family:&quot;Cambria Math&quot;,serif">next ∈ token* → {token},</span></code> respectively <code><span style="font-family:&quot;Cambria Math&quot;,serif">LLM ∈ token* → {token∗}</span></code>, or equivalently as binary relations <code><span style="font-family:&quot;Cambria Math&quot;,serif">next ⊆ token∗ × token</span></code> and <code><span style="font-family:&quot;Cambria Math&quot;,serif">LLM ⊆ token∗ × token∗</span></code>. Then we can describe their nondeterministic computational behavior using big-step semantics notation.<sup>17</sup> In this case, there are two evaluation relations:</p>
<p>&nbsp;</p>
<p><code>• <span style="font-family:&quot;Cambria Math&quot;,serif">q <img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/next-downarr.png" style="max-width:2em;" /> t</span></code></p>
<p>&nbsp;</p>
<p>which indicates that <code><span style="font-family:&quot;Cambria Math&quot;,serif">t ∈ next(q)</span></code> or <code><span style="font-family:&quot;Cambria Math&quot;,serif">(q, t) ∈ next</span></code> (i.e., <code>t</code> is the next token generated by <code>next</code> given the initial token sequence <code>q</code>).</p>
<p><code>• <span style="font-family:&quot;Cambria Math&quot;,serif">q <img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/llm-downarr.png" style="max-width:2em;" /> r</span></code></p>
<p>&nbsp;</p>
<p>which indicates that <code><span style="font-family:&quot;Cambria Math&quot;,serif">r ∈ LLM(q)</span></code> or <code><span style="font-family:&quot;Cambria Math&quot;,serif">(q, r) LLM</span></code> (i.e., <code>r</code> is the answer generated by <code>LLM</code> for prompt <code>q</code>).</p>
<p>&nbsp;</p>
<p>Using big-step semantics, we can concisely explain how <code>LLM</code> uses <code>next</code> to generate longer sequences of tokens as follows: </p>
<p>• Either the <code>LLM</code> function stops generating new tokens—for example, because the next predicted token would have been a stop token, since the output token limit has been reached, or simply because the generation is complete:</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq1.png" style="max-width:3.5em;" /></p>
<p>&nbsp;</p>
<p>• Or, reading from top to bottom, it <i>auto-regressively</i> generates new tokens from the previous sequence <code>q</code> extended with the token <code>t</code> predicted by <code><span style="font-family:&quot;Cambria Math&quot;,serif">next(q)</span>:</code></p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq2.png" style="max-width:8em;" /></p>
<p>&nbsp;</p>
<p>These two formulas pack a lot of operational information in a small space, which is why we like big-step semantics.</p>
<p>&nbsp;</p>
<h3>Dynamic Data Access Using Virtual Memory</h3>
<p>No matter how many weights an LLM has, there will always be knowledge that is out of scope since it is not or cannot be stored within the weights:</p>
<p>• LLMs are trained on static datasets that contain data collected only up to a certain point in time at which data collection has been cut off. For example, if we ask the model </p>
<p><code><span style="font-size: 10.0pt;color:#0000ff">user</span>: What is the current temperature in Palo Alto?</code></p>
<p>it will reply something along the lines of</p>
<p><code><span style="color:#009900">assistant</span>: I'm sorry, but I don't have access to realtime information or the ability to provide current weather updates,</code></p>
<p>thus explicitly acknowledging that knowledge of the current weather is not present in the model's weights. LLMs are not always so well-behaved and often hallucinate answers that sound plausible but are unfounded, rather than simply saying they don't know. This tendency to fabricate responses is a notable challenge when working with LLMs.</p>
<p>• For applications such as chatbots, we need to keep track of the conversation history <code>(<span style="color:#0000ff">user</span>; <span style="color:#009900">assistant</span>)<span style="font-family:&quot;Cambria Math&quot;,serif">∗</span></code> between the user and the model. Without keeping context, it is impossible to have multi-turn conversations, which is precisely what makes LLM-based chat interfaces compelling. Obviously, this is another kind of data that exists outside the temporal scope of the model's training data.</p>
<p>• Neither privacy-sensitive information nor data with access restrictions should be directly encoded in the model's weights since this would effectively make that data part of the model's public knowledge base and thus leak this information in an uncontrolled manner. Furthermore, access restrictions may change or require updates over time, which is at odds with the static nature of trained model weights. It is impractical to retrain the model every time its training data is <code>chmod</code>-ed.</p>
<p>Fortunately, instead of statically ingesting information in the model's weights during training, we can dynamically load additional information via the model's input context and ask the model to take that into account as follows:<sup>4</sup></p>
<p><b><code><span style="color:#0000ff">user</span></code></b></p>
<p><code>Answer the question based on the following information:</code></p>
<p><code><i>As of 7:30 am PST. Coastal Flood Advisory. <br/> Today. 55°/39°. 2%. Thu 11 | Day. 55°. 2%. <br/> NW 6 mph. Sunshine to start, <br/> then a few afternoon clouds.</i></code></p>
<p><code>Question: What is the current temperature in Palo Alto?</code></p>
<p>Based on the provided context, the model now is capable of answering what the current weather is:</p>
<p><code><b><span style="color:#009900">assistant</span></b></code></p>
<p><code>Based on the provided information, the current temperature in Palo Alto is 55°F (Fahrenheit) as of 7:30 am PST.</code></p>
<p>&nbsp;</p>
<p>While providing dynamic information via the model's context works well,<sup>2</sup> the context capacity of LLMs is fundamentally limited. As of early 2024, the largest models in widespread production had an input context of 200K to 128K tokens and an output context of 4K tokens, or they divided 32K tokens across the whole context. Even with 200K tokens, or roughly 500 printed pages, it is impossible in practice to incorporate all the necessary dynamic information within the context alone.</p>
<p>Comparing the context of an LLM to the RAM of a conventional computer (although it is not really <i>random access</i> since the model actually seems to pay more attention to the beginning and end of its input<sup>10</sup>), we can borrow the idea of <i>virtual memory</i> from traditional operating systems.</p>
<p>Virtual memory provides the illusion of an infinite memory by "paging" in and out the relevant information between secondary memory and RAM. The operating system analyzes memory-access patterns and predicts which pages are likely to be needed in the near future to decide which pages to swap in and out. We also can do exactly the same thing for finding the "chunks" to load into and evict from the context of an LLM to answer the next question. This is called RAG (retrieval-augmented generation).<sup>6</sup> Figure 2 shows the use of RAG as virtual memory.</p>
<img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/meijer2.png" alt="Virtual Machinations: Using Large Language Models as Neural Computers" />
<p>A common technique for implementing RAG is to compute an <i>embedding</i><sup>1</sup> of the user query and use that to find the best-matching information in a vector database<sup>21</sup> (used as secondary storage) to include in the context. A useful mental model for embeddings is <i>semantics-sensitive hashing</i>, which maps high-dimensional data such as text and images to a low-dimensional vector such that semantically similar items are mapped to vectors that are geometrically close in the embedding space.</p>
<h3>Dynamic Tool Invocation as Instruction Dispatch</h3>
<p>Current weather is not a great example of data to fetch from secondary storage via RAG as it changes too rapidly. To access realtime (or transient) information that requires actual computation instead of retrieval, the model should have the ability to use external "tools" (see figure 3). These tools can query a database or act as interfaces to external services or APIs, thereby allowing the model to ingest dynamic and context-specific information based on actual arguments required to invoke these tools that are generated by the model itself.</p>
<img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/meijer3.png" alt="Virtual Machinations: Using Large Language Models as Neural Computers" />
<p>If any of these tools are backed by another LLM, this is often referred to as a <i>multi-agent</i> system. This is a bit pretentious since we don't call traditional OO (object-oriented) programs that use multiple objects <i>multi-object</i> systems.</p>
<p>Building on the analogy of models as databases and the fact that LLM is a relation,<sup>7</sup> we also model tools as relations since they could be implemented using models themselves, or in general be stochastic or nondeterministic. For example, we can consider the current weather in a given city as a binary relation, as shown in table 1.</p>
<img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/meijer-t1.png" alt="Virtual Machinations: Using Large Language Models as Neural Computers" />
<p>When reasoning about invoking a tool f, we use the big step notation <code>e <img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/f-downarr.png" style="max-width:1em;" /> v</code> to denote that <code>v <span style="font-family:&quot;Cambria Math&quot;,serif">∈</span> f(e)</code>, treating the tool f as a (non-deterministic) function, or equivalently <code>(e, v) <span style="font-family:&quot;Cambria Math&quot;,serif">∈</span> f</code>, treating the tool as a relation. In both cases, we say that the tool invocation <code>f(e)</code> returned <code>v</code>. For example, based on table 1, the tool invocation <code>WEATHER("Palo Alto")</code> may return <code>"Rainy and 40°F"</code>, which in big-step notation looks like this:</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq3.png" style="max-width:20em;" /></p>
<p>Most language models that support the use of tools expose them as functions,<sup>15</sup> but we believe that relations provide a more powerful, flexible, and intuitive framework than using functions.</p>
<p>A popular technique for integrating tool invocation into LLM interactions is the ReAct (Reason+Act) approach,<sup>23</sup> which, given a set of possible tools, nudges the model to cycle through a sequence of explanations and tool invocations to arrive at the final answer. Here is a possible relational ReAct-style interaction sequence where the user asks for the current weather and the model hallucinates (in gray) the tool invocation to compute the answer:</p>
<p><code><span style="color:gray">system</span><br/>... prompt that defines the WEATHER tool</code></p>
<p><code><span style="color:#0000ff">user</span><br/>What is the current weather in Palo Alto?</code></p>
<p><code><span style="color:#009900">assistant</span><br/>To get the weather in Palo Alto, I'll use the weather tool [WEATHER("Palo Alto", <span style="color:gray">"Rainy and 40°F")]. So, the weather in Palo Alto is wet and 40°F.</span></code></p>
<p>However, the <i>reasoning engine</i> that orchestrates the flow beteen the tools and the LLM, intercepts the the LLM generation at the closing bracket immediately after the tool call <code>[WEATHER("Palo Alto", "Sunny and 75F")]</code>, thus effectively ignoring the model's musings about the weather it doesn't actually know. Instead, we invoke the <code>WEATHER</code> tool and find <code>"Sunny and 80°F" <span style="font-size: 10.0pt;font-family:&quot;Cambria Math&quot;,serif">∈</span> WEATHER("Palo Alto")</code>. Subsequently, we ask the model to continue from where we interrupted it with the correct information (in bold):</p>
<p><code><span style="color:#009900">assistant</span><br/>I need to invoke the weather service to get the current weather in Palo Alto. [WEATHER("Palo Alto", <b>"Sunny and 80°F"</b>)]</code></p>
<p><code><span style="color:#009900">assistant</span><br/> The current weather in Palo Alto is sunny and 80°F.</code></p>
<p>In the second turn, no tool invocation was necessary to generate the final answer to the user's question, and the model used the provided information from the external tool to correctly answer the question.<br/> <br/> </p>
<h4>Big-step semantics of ReAct</h4>
<p>ReAct-style reasoning can be captured to generate an answer to a user question by way of a sequence of tool calls</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq4.png" style="max-width:22em;" /></p>
<p>using big-step semantics as follows:</p>
<p>• Either the LLM completes the query without making a tool call, in which case we are done and can combine the previous tool invocations with the final answer as the result</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq5.png" style="max-width:30em;" /></p>
<p>• Or, reading from top to bottom, the LLM makes a tool call, as witnessed by generating <code><span style="color:#009900">assistant</span>: [f(e, w)</code>. In that case, we ignore the model's hallucinated suggestion <code>w</code> and instead insert the correct computed value <code>v <span style="font-family:&quot;Cambria Math&quot;,serif">∈</span> f(e)</code> into the list of tool invocations performed so far for <code><span style="color:#009900">assistant</span>: (calls; [f(e, <b>v</b>)]),</code> and recurse to compute the final result</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq6.png" style="max-width:30em;" /></p>
<p><br/> With both rules, we assume an empty sequence of tool calls means <code><span style="color:#009900">assistant</span></code> is empty.</p>
<p>Note how this second inference rule nicely illustrates the analogy of the LLM as a <i>branch predictor</i>. Given the query <code><span style="color:#0000ff">user</span>: question</code> and the tools executed thus far by <code><span style="color:#009900">assistant</span>: calls</code>, the LLM <i>predicts</i> the next tool invocation <code>[f(e, w)</code>. We then actually invoke that "instruction" <code>f(e)</code> to compute the true value <code><b>v</b></code>, and continue executing with <code>[f(e, <b>v</b>)]</code>.</p>
<p>The major difference between a conventional branch predictor and the neural one is that the latter completely hallucinates the program that it executes one instruction at a time, while a regular branch predictor predicts the next instruction to call in an already existing program.</p>
<p>&nbsp;</p>
<h4>Addressing unintended model interference</h4>
<p>The root problem with the vanilla ReAct approach to adding external tools is that it allows the model full visibility into the values that float between itself and the tool invocation, leading to potential reinterpretations and corrections of tool outputs.</p>
<p>If the model "thinks" based on the knowledge stored in its weights that a value returned from a tool contradicts its internal knowledge, it can—and <i>will</i>—throw away the value obtained from the external tool. Then the model will hypothesize a new value that is more consistent with its internal-world model and use that instead of the value returned by the tool call.</p>
<p>For example, if the tool invocation returns a temperature of 10000°F for the weather in Palo Alto, the model will assume that is too high and silently replace it with 100°F, which it believes to be more reasonable.</p>
<p><code><span style="color:#0000ff">user</span><br/> What is the current weather in Palo Alto?</code></p>
<p><code><span style="color:#009900">assistant</span> To get the weather in Palo Alto, I'll use the weather tool [WEATHER("Palo Alto", <b>"Sunny and 10000°F"</b>)]</code></p>
<p><code><span style="color:#009900">assistant</span><br/> So, the weather in Palo Alto is Sunny and 100°F.</code></p>
<p>&nbsp;</p>
<p>The reason we added tool calls was to fix the model's inability to perform certain calculations or access realtime data. Obviously, when the model starts to interfere with the values passed to and from tools, it becomes impossible to build reliable systems on top of such a mechanism.</p>
<p>Maybe this problem can be solved by once again taking inspiration from traditional computer architecture. Instead of dealing with immediate values, a conventional CPU typically uses a register-to-register model for specifying instructions, where both operands and results reside in named registers. We can use the same approach: To prevent the model from interpreting the values it manipulates, we simply won't give it values anymore. Instead:</p>
<p>• Inside the external reasoning engine we maintain an environment where variables are bound to values (analogous to a register set) and the model is allowed to handle only <i>names</i> of variables when interacting with external tools.</p>
<p>• To achieve this, we need to prompt the model to hallucinate variable names when it invokes a tool: <code>[WEATHER("Palo Alto", @weatherInCity)]</code> instead of hallucinating values <code>[WEATHER("Palo Alto", "Rainy and 20°F")]</code> as it did previously.</p>
<p>• Moreover, since the answer computed by the model may now contain variables that the model does not know the values of, we need to add expressions such as <code>[@weatherInCity]</code> to the model's vocabulary so it can formulate the answer to the original question as an expression over these opaque variables. The reasoning engine, of course, does know the values of variables and so can show them to the user only when the model generates an expression.</p>
<p>• In rare cases where the model actually needs to see values to produce its prediction, we can expose those in a controlled fashion as special privileged tools.</p>
<p>Here is a transcript of an exchange of <code><span style="color:#0000ff">user</span> and <span style="color:#009900">assistant</span></code> messages between the client and the model using our new variable-passing approach to answer the question:<code> What is the weather between Mountain View and Menlo Park?</code></p>
<p>As before, we start with a <code>system</code> message that describes the set of available tools but also instruct the model to provide a variable to bind the result of each tool call:</p>
<p><code><span style="font-family:&quot;Merlo Medium&quot;;color:gray">system</span><br/> ... description of available tools ...</code><br/>In response to the user's implicit question about the current weather, the model again uses its internal knowledge to figure out that we are looking for the weather in Palo Alto, but it now invokes the tool with a hallucinated variable <code>@weatherPaloAlto</code>.</p>
<p><code><span style="color:#009900">assistant</span> <br/>The city between Mountain View and Menlo Park is Palo Alto. <br/>Let's find out the weather there. <br/>[WEATHER("Palo Alto", @weatherPaloAlto)</code></p>
<p>&nbsp;</p>
<p>As before, we stop generation at the closing bracket. But instead of ignoring the model's suggestion, this time we use the variable <code>@weatherPaloAlto</code> and bind it in the environment of the reasoning engine to the result of the tool call <code>{@weatherPaloAlto = "Sunny and 80°F"}</code>. We then resume generation <i>without</i> telling the model what the result of the call was by just closing the ] bracket:</p>
<p><code><span style="color:#009900">assistant</span> <br/>The city between Mountain View and Menlo Park is Palo Alto. <br/>Let's find out the weather there. <br/>[WEATHER("Palo Alto", @weatherPaloAlto)<b>]</b></code></p>
<p>&nbsp;</p>
<p>We rely upon the model keeping track in its own mind that the variable <code>@weatherPaloAlto</code> contains the current weather in Palo Alto.</p>
<p>In the next round, we stop generation at the generated expression where the model informs us about the weather in Palo Alto via the variable <code>@weatherPaloAlto</code> that was bound in the previous step:</p>
<p><span style="color:#009900">assistant</span><br/> The current weather in Palo Alto is <br/> [@weatherPaloAlto</p>
<p>&nbsp;</p>
<p>We use the reasoning engine to compute the value of the generated expression <code>@weatherPaloAlto</code>, showing it to the user, indicated by <code><span style="font-family:&quot;Cambria Math&quot;,serif">⇝</span> Sunny and 80°F</code>, but not to the model, and then continue generation by closing the ] bracket:</p>
<p><code><span style="color:#009900">assistant</span><br/> The current weather in Palo Alto is <br/> [@weatherPaloAlto]<span style="font-family:&quot;Cambria Math&quot;,serif">⇝</span> Sunny and 80°F</code></p>
<p><code><span style="color:#009900">assistant</span><br/></code> .</p>
<p>Finally, the model signals that the answer is complete by not generating any further tool calls.</p>
<p>&nbsp;</p>
<h4>Operational semantics</h4>
<p>Here is an example of operational semantics, which for simplicity returns only the final modified environment and leaves out the <code><span style="color:#0000ff">user</span></code> and <code><span style="color:#009900">assistant</span></code> tags of the trace. These semantics clearly show how the reasoning engine threads the environment <code>σ</code> across calls to the tool-script interpreter, while hiding any values from the model:</p>
<p>• When the model responds without a tool call or expression, we are done and immediately return the current environment unchanged.</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq7.png" style="max-width:6em;" /></p>
<p>&nbsp;</p>
<p>• When the LLM suggests evaluating an expression <code>e</code>, we execute it in the current environment <code>σ</code> and show it to the user <code>(<span style="font-size: 10.0pt;font-family:&quot;Cambria Math&quot;,serif">⇝</span> v)</code>, and then continue execution to compute <code>σ′</code> without showing the value to the model.</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq8.png" style="max-width:22em;" /></p>
<p>&nbsp;</p>
<p>• When the model wants to use a tool <code>f(e,@x)</code>, we invoke <code>f(e)</code> inside the reasoning engine so we can use the current environment to find a value <code>v <span style="font-family:&quot;Cambria Math&quot;,serif">∈</span> f(e)</code>, and bind that to <code>@x</code> to create the updated environment that is then recursively used to generate the rest of the computation.</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq9.png" style="max-width:30em;" /></p>
<p>&nbsp;</p>
<p>The environment <code>σ</code> is similar to the register set of a conventional CPU that also binds (register) names to values. The main difference is that, in a conventional CPU, there are only a fixed number of registers, but instructions such as <code>ADD r3, r1, r2</code> typically also operate on register names instead of on immediate values.</p>
<p>&nbsp;</p>
<h3>Neural Computers</h3>
<p>Now that you have seen how to extend LLMs with tool usage, let's zoom out and reframe the toolset that the model has at its disposal as the <i>instruction set</i> of a kind of "neural computer." </p>
<p>In this analogy, the ReAct-style interception and dispatching of tool invocations (as captured by the ReAct big-step semantics shown earlier) mirrors the <i>control unit</i> of a conventional computer that fetches the next instruction, sends it to the ALU (arithmetic logic unit) for execution, passes operands, and receives results via registers. The LLM's role then is indeed akin to a <i>branch predictor</i> within a conventional CPU, which predicts the next most likely instruction to execute, as guided by the sequence of instructions executed so far.</p>
<p>These striking parallels between LLMs and conventional computers, summarized in table 2, strongly suggest that instead of looking at LLMs as glorified databases, they should instead be treated as the core component of a new kind of neural computer.</p>
<img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/meijer-t2.png" alt="Virtual Machinations: Using Large Language Models as Neural Computers" />
<p>&nbsp;</p>
<h3>A Natural Language-based Reasoning Language</h3>
<p>Having "invented" a new kind of computer, we naturally also need a novel programming language that can be used to instruct neural computers. For this, we choose a (restricted) form of logic programming embedded in natural-language comments—thereby closing the cycle between modern generative AI and old-school symbolic AI.<sup>9</sup></p>
<p>Let's revisit the ReAct-style interaction sequence from before, only now assuming we have an abstract set of tools <code>f, ... , g</code> at our disposal. If we combine all partial <code><span style="color:#009900">assistant</span></code>: responses as we did in the big-step semantics, then a ReAct-style interaction corresponds to a <i>goal</i> consisting of a sequence of predicates/tool calls interleaved with comments:</p>
<p>&nbsp;</p>
<p><code><span style="color:#0000ff">user</span><br/>question</code></p>
<p><code><span style="color:#009900">assistant</span><br/>..., [f( ..., x, ...)], ..., [g( ..., y, ...)], ...</code></p>
<p>&nbsp;</p>
<p>where <code>x</code> and <code>y</code> are terms over <code>@</code>-variables.</p>
<p>Each tool call is an interrupted hallucination by the LLM, providing a peek inside the model's "mind" regarding how it reasons when answering the given question by generating a chain of thoughts<sup>20</sup> in the form of a sequence of tool invocations with explanations.</p>
<p>By externalizing this internal reasoning performed by the model in the form of a programming language—the <i>mentalese</i><sup>5</sup> of LLMs, so to speak—we can regard the <code><span style="color:#009900;">assistant</span></code> trace as the <i>source code</i> synthesized by the model from the natural-language question issued by the <span style="color:#0000ff;">user</span>.</p>
<p>By appealing to Tennent's principles of abstraction and parameterization,<sup>19</sup> we then can give these goals a name and parameters <code>h( ..., x, ...)</code> and hence get <i>rules</i> of the form <code> ...[h( ..., x, ... )] ...:- ...[f( ..., y, ... )], ...[g( ..., z, ... )], ... </code>embedded in natural-language explanations, where <code>x, y, z </code>are terms over <code>@</code>-variables. To escape code fragments inside natural-language explanations, we use <code>'h</code>edges" <code>[ and ].</code> And, voilà, we've (re-)discovered literate Prolog as the externalized reasoning language for chain of thought.</p>
<p>Here is a concrete example of how to write a tool in our proposed reasoning language. Say we want to implement the <code>WEATHER(@city, @weather)</code> tool using the National Weather Service forecast API.<sup>13</sup> Accomplishing this means going through the following sequence of steps that invoke more primitive tools:</p>
<p>
<codefind the current weather br>
[WEATHER(@city, @weather)]
<br/> in [@city] using the National Weather Service API :-
<br/> To find the current weather in [@city],
<br/> we first need to find the coordinates of the city via [GEO_CODE(@city, @lat, @lon)].
<br/> Then given its [@lat] and [@lon] coordinates,
<br/> we can invoke [WEATHER_GOV(@lat, @lon,
<br/> {... "forecast": @url ...})],
<br/> to obtain the forecast url [@url]
<br/> using which we can finally fetch
<br/> [HTTP_GET(@url, {... "detailedForecast":
<br/> @weather ...})] the current [@weather] for [@city].
</codefind></p>
<p>&nbsp;</p>
<p>Primitive tools such as <code>GEO_CODE(@city, @lat, @lon)</code> are <i>facts</i> that are implemented in a traditional (imperative) programming language.</p>
<p>With such a reasoning language, users not only can define new tools from scratch, but also take the model's previously generated code sequences and apply them as new tools to the model, thus creating a self-learning system. Moreover, given that the language is syntax for the model's internal reasoning, the user can also use it to provide few-shot examples to instruct the model on how to use tools.</p>
<p>&nbsp;</p>
<h3>Safety Using AI-enabled Formal Methods</h3>
<p>Ensuring that superhuman AI systems benefit humanity while mitigating unintended risks is a significant priority. One interesting approach to ensuring AI safety, borrowed from programming languages, is to use <i>proof-carrying code</i>.<sup>14,18</sup> The basic idea is that the user defines (potentially with the help of a model) a security policy that the AI system must obey and then, when the model generates code, it must also provide a proof that the generated code satisfies the required safety policy.</p>
<p>Since our language consists of only Horn clauses (like a poor man's dependent type system), the same language for writing programs can be used to define security policies using invariants, pre- and post-conditions. By keeping the language simple, we lower the bar for models to prove properties about the code while also making the code more amendable for formal verification. Although proving nontrivial specifications to be correct is challenging, the anticipated 10- to 100-fold increase in model capabilities suggests that future AI capabilities could help users create specifications and generate code, together with its corresponding formal correctness proofs. This means AI could finally make the dream of formal methods and correct-by-construction software feasible.</p>
<p>We believe this offers a pragmatic and extensible method for implementing AI safety that allows even nontechnical users to monitor and enforce the logical correctness and ethical compliance of all computations created by AI models. Powerful AI will finally make the dream of formal methods and correct-by-construction software feasible.</p>
<p>&nbsp;</p>
<h3>Tree of Thought</h3>
<p>One topic that remains to be addressed is how to deal with errors during execution of code generated by the model. For example, what happens if the expression or statement generated by the LLM is invalid, throws an exception, or fails to verify against the specification? One common case where this happens is when the model generates the wrong arguments for a tool or does not convert values to the proper types, thus causing the tool invocation to crash.</p>
<p>The big-step operational semantics, however, specify only terminating and valid runs. When errors surface during execution, there is simply no derivation leading to a final result. For example, for every environment <code>σ</code>, there is no related environment <code>σ′</code> that results from executing an ill-typed math expression such as</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq10.png" style="max-width:16em;" /></p>
<p>because there is no value <code>v</code> that results from executing</p>
<p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3676287/assets/html/eq11.png" style="max-width:12em;" /></p>
<p>&nbsp;</p>
<p>The absence of a derivation signals that the computation could not proceed normally because of errors, or nontermination.</p>
<p>When actually implementing big-step semantics as a reasoning engine that coordinates between the LLM and the script interpreter, we have to deal with errors as we cannot rely upon an oracle to guard against missteps. The obvious algorithm to search for a valid derivation is to use backtracking over all possible derivations. In particular, whenever the execution of an expression or a tool invocation suggested by the model fails, it's always best to back up and try to generate an alternative.</p>
<p>Backtracking brings our language even closer to Prolog; the only thing missing is true logic variables. Even in pure Prolog, however, logic variables are somewhat of a gimmick.<sup>12</sup> While it seems cute in theory to run predicates backwards—for example, to split a list in two using the same code that appends it—there is often a clear direction implied in the rules because of operational constraints on relations such as<code> @X is @Y+@Z</code>, which requires <code>@Y</code> and <code>@Z </code>to be values in order to perform the addition.</p>
<p>Since we are relying on a rich set of externally defined tools, these constraints become even more prevalent, even when rules are reversible. While we can imagine representing <code>STT(@speech, @text) and TTS(@text, @speech)</code> as a single reversible predicate, the practicality of finding a city given the weather by running <code>WEATHER(@city, @weather)</code> in reverse using <code>@city <span style="font-family:&quot;Cambria Math&quot;,serif">∈</span> WEATHER(@weather)</code> is more questionable. Too many cities have the same weather.</p>
<p>The subset of Prolog that we implemented is traditionally called "fixed-mode," with clear inputs and outputs, and where all inputs need to be fully evaluated (not to contain un-instantiated variables) and dependencies flow strictly from left to right. This means that we do not need unification but only one-way pattern-matching. While these restrictions could be lifted and true logic variables implemented, we have not yet found any real use cases for such a general model.</p>
<p>&nbsp;</p>
<h3>Related Work</h3>
<p>Given the rapid spread of generative AI, the concept of using LLMs in a manner akin to more traditional computational resources has been explored to varying extents. Andrej Karpathy's tweet<sup>8</sup> about an LLM OS has generated a lot of follow-up blog posts and papers such as one on the LLM Agent Operating System.<sup>11</sup> The latter creates an abstract multi-agent operating system where various of the operating system components such as the scheduler and memory management system are implemented using LLMs. Karpathy's original tweet pictures the LLM as a CPU. The MemGPT system<sup>16</sup> adds virtual-memory capabilities to a "processor LLM," but it uses function calls to let the model <i>itself</i> decide how to move data between the context and external storage. We prefer to decouple the LLM processor from the mechanism that manages overall context. This is actually more similar to how virtual memory works in traditional systems where the operating system—not the CPU—manages virtual memory.</p>
<p>What is unique about our approach is the definition of a reasoning language to run on top of the LLM-based computer or operating system. This, of course, touches on the wider field of <i>neuro-symbolic AI,</i> which aims to combine neural networks with symbolic reasoning.</p>
<p>&nbsp;</p>
<h4>References</h4>
<p>1. Bengio, Y., Ducharme, R., Vincent, P., Janvin, C. 2003. A neural probabilistic language model. <i>The Journal of Machine Learning Research</i>, 3, 1137–1155; <a href="https://dl.acm.org/doi/10.5555/944919.944966">https://dl.acm.org/doi/10.5555/944919.944966</a>.</p>
<p>2. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D. 2020. Language models are few-shot learners. <i>Proceedings of the 34th International Conference on Neural Information Processing Systems</i>. Article 159, 1877-1901; <a href="https://dl.acm.org/doi/abs/10.5555/3495724.3495883">https://dl.acm.org/doi/abs/10.5555/3495724.3495883</a>.</p>
<p>3. Chip, H. 2024. Sampling for text generation; <a href="https://huyenchip.com/2024/01/16/sampling.html">https://huyenchip.com/2024/01/16/sampling.html</a>. </p>
<p>4. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., Sui, Z. 2023. A survey on in-context learning. arXiv:2301.00234; <a href="https://arxiv.org/abs/2301.00234">https://arxiv.org/abs/2301.00234</a>.</p>
<p>5. Fodor, J. A. 1975. <i>The Language of Thought</i>. New York, NY: Thomas Y. Crowell Co.</p>
<p>6. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Guo, Q., Wang, M., Wang, H. 2024. Retrieval-augmented generation for large language models: a survey. arXiv 2312.10997; <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a>.</p>
<p>7. Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., Bau, D. 2024. Linearity of relation decoding in transformer language models. arXiv:2308.09124; <a href="https://arxiv.org/abs/2308.09124">https://arxiv.org/abs/2308.09124</a>.</p>
<p>8. Karpathy, A. 2023. LLM OS. Bear with me I'm still cooking. X; <a href="https://x.com/karpathy/status/1723135784142979392">https://x.com/karpathy/status/1723135784142979392</a>.</p>
<p>9. Körner, P., Leuschel, M., Barbosa, J., Santos Costa, V., Dahl, V., Hermenegildo, M. V., Morales, J. F., Wielemaker, J., Diaz, D., Abreu, S., Ciatto, G. 2022. Fifty years of Prolog and beyond. arXiv:2201.10816; <a href="https://arxiv.org/abs/2201.10816">https://arxiv.org/abs/2201.10816</a>.</p>
<p>10. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang, P. 2023. Lost in the middle: how language models use long contexts. arXi:2307.03172; <a href="https://arxiv.org/abs/2307.03172">https://arxiv.org/abs/2307.03172</a>.</p>
<p>11. Mei, K., Li, Z., Xu, S., Ye, R., Ge, Y., Zhang, Y. 2024. AIOS: LLM agent operating system. arXiv 2403.16971; <a href="https://arxiv.org/abs/2403.16971">https://arxiv.org/abs/2403.16971</a>.</p>
<p>12. Mellish, C. 1985. Some global optimizations for a Prolog compiler. <i>Journal of Logic Programming</i> 2(1), 43–66; <a href="https://www.sciencedirect.com/science/article/pii/0743106685900044">https://www.sciencedirect.com/science/article/pii/0743106685900044</a>.</p>
<p>13. National Weather Service. API Web Service documentation; <a href="https://www.weather.gov/documentation/services-web-api">https://www.weather.gov/documentation/services-web-api</a>.</p>
<p>14. Necula, G. C. 2000. Proof-carrying code (abstract): design, implementation and applications. In <i>Proceedings of the 2nd ACM SIGPLAN International Conference on Principles and Practice of Declarative Programming</i>, 175–177; <a href="https://dl.acm.org/doi/10.1145/351268.351289">https://dl.acm.org/doi/10.1145/351268.351289</a>.</p>
<p>15. OpenAI. Function calling. OpenAI Platform documentation; <a href="https://platform.openai.com/docs/guides/function-calling">https://platform.openai.com/docs/guides/function-calling</a>.</p>
<p>16. Packer, C., Fang, V., Patil, S. G., Lin, K., Wooders, S., Gonzalez, J. E. 2023. MemGPT: towards LLMs as operating systems. arXiv:2310.08560; <a href="https://arxiv.org/abs/2310.08560">https://arxiv.org/abs/2310.08560</a>.</p>
<p>17. Siek, J. 2012. Crash course on notation in programming language theory; <a href="https://siek.blogspot.com/2012/07/crash-course-on-notation-in-programming.html">https://siek.blogspot.com/2012/07/crash-course-on-notation-in-programming.html</a>.</p>
<p>18. Tegmark, M., Omohundro, S. 2023. Provably safe systems: the only path to controllable AGI. arXiv:2309.01933; <a href="https://arxiv.org/abs/2309.01933">https://arxiv.org/abs/2309.01933</a>.</p>
<p>19. Tennent, R. D. 1977. Language design methods based on semantic principles. <i>Acta Informatica</i> 8, 97–112; <a href="https://link.springer.com/article/10.1007/BF00289243">https://link.springer.com/article/10.1007/BF00289243</a>.</p>
<p>20. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D. 2023. Chain-of-thought prompting elicits reasoning in large language models. arXiv:2201.11903; <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.</p>
<p>21. Wikipedia. Vector database; <a href="https://en.wikipedia.org/wiki/Vector_database">https://en.wikipedia.org/wiki/Vector_database</a>.</p>
<p>22. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., Narasimhan, K. 2023. Tree of thoughts: deliberate problem solving with large language models. arXiv:2305.10601; <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.</p>
<p>23. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y. 2023. ReAct: synergizing reasoning and acting in language models. arXiv 2210.03629; <a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>. </p>
<p><b>Erik Meijer</b> brings a rare combination of technical expertise and people leadership to his latest quest to use AI to democratize end-user programming. As a renowned computer scientist, entrepreneur, and tech influencer, Meijer has made pioneering contributions to programming languages, compilers, cloud infrastructures, and AI throughout his tenures at Microsoft, Meta (Facebook), Utrecht University, and Delft University of Technology.</p>
<p>Copyright © 2024 held by owner/author. Publication rights licensed to ACM.</p>
<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'8d117f539e618254',t:'MTcyODY3NzQ2Ni4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script>
<p>
<img class="floatLeft" src="img/q stamp_small.jpg" width="26" height="45" alt="acmqueue"><br><br>
<em>Originally published in Queue vol. 22, no. 3</em>&#8212;
<br>
Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3676287">ACM Digital Library</a>
</p>
<br/>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
<br/>

<br/>
<div class="g-plusone" data-size="small" data-annotation="inline" data-width="120"></div>

<script type="text/javascript">
	addthis_pub             = 'acm';
	addthis_logo            = 'http://queue.acm.org/img/logo_queue_small.gif';
	addthis_logo_background = '#ffffff';
	addthis_logo_color      = '000000';
	addthis_brand           = 'ACM Queue';
	addthis_options         = 'reddit, slashdot, facebook, favorites, email, delicious, digg, technorati, blinklist, furl, myspace, google, live, more';
</script>




<hr noshade size="1" />
More related articles:
<p>
<span>Jim Waldo, Soline Boussard</span> - <a href="detail.cfm?id=3688007"><b>GPTs and Hallucination</b></a>
<br/>
The findings in this experiment support the hypothesis that GPTs based on LLMs perform well on prompts that are more popular and have reached a general consensus yet struggle on controversial topics or topics with limited data. The variability in the applications's responses underscores that the models depend on the quantity and quality of their training data, paralleling the system of crowdsourcing that relies on diverse and credible contributions. Thus, while GPTs can serve as useful tools for many mundane tasks, their engagement with obscure and polarized topics should be interpreted with caution.
</p>
<br/>
<p>
<span>Mansi Khemka, Brian Houck</span> - <a href="detail.cfm?id=3675416"><b>Toward Effective AI Support for Developers</b></a>
<br/>
The journey of integrating AI into the daily lives of software engineers is not without its challenges. Yet, it promises a transformative shift in how developers can translate their creative visions into tangible solutions. As we have seen, AI tools such as GitHub Copilot are already reshaping the code-writing experience, enabling developers to be more productive and to spend more time on creative and complex tasks. The skepticism around AI, from concerns about job security to its real-world efficacy, underscores the need for a balanced approach that prioritizes transparency, education, and ethical considerations.
</p>
<br/>
<p>
<span>Divyansh Kaushik, Zachary C. Lipton, Alex John London</span> - <a href="detail.cfm?id=3639452"><b>Resolving the Human-subjects Status of Machine Learning's Crowdworkers</b></a>
<br/>
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diversity of both the tasks performed and the uses of the resulting data render it difficult to determine when crowdworkers are best thought of as workers versus human subjects. These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements.
</p>
<br/>
<p>
<span>Harsh Deokuliar, Raghvinder S. Sangwan, Youakim Badr, Satish M. Srinivasan</span> - <a href="detail.cfm?id=3631340"><b>Improving Testing of Deep-learning Systems</b></a>
<br/>
We used differential testing to generate test data to improve diversity of data points in the test dataset and then used mutation testing to check the quality of the test data in terms of diversity. Combining differential and mutation testing in this fashion improves mutation score, a test data quality metric, indicating overall improvement in testing effectiveness and quality of the test data when testing deep learning systems.
</p>
<br/>
<hr noshade size="1" />
<hr noshade size="1" />
<p>
<a href="#"><img src="https://queue.acm.org/img/logo_acm.gif" /></a>
<br/>
&copy; ACM, Inc. All Rights Reserved.
</p>
</div>
</body>
</html>