<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">



<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-20JYM3ZFN0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-20JYM3ZFN0');
</script>



	  
	  <title>Safe Coding - ACM Queue</title>

	  

	  <meta name='description' value='' />
	  <meta name='keywords' value='Security' />

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P52H78L');</script>
<!-- End Google Tag Manager -->

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" href="favicon.ico" />

<script type="text/javascript" src="/js/jquery-1.2.6.min.js"></script>
<script type="text/javascript" src="/js/jquery.validate.min.js"></script>
<script type="text/javascript" src="/js/global.js"></script>



<!--
<link rel="alternate" type="application/rss+xml" title="Latest Queue Content RSS 2.0" href="/rss/feeds/latestitems.xml" />
-->
<link rel="alternate" type="application/rss+xml" title="All Queue Content RSS 2.0" href="/rss/feeds/queuecontent.xml" />
<link rel="alternate" type="application/rss+xml" title="Curmudgeon RSS 2.0"     href="/rss/feeds/curmudgeon.xml" />
<link rel="alternate" type="application/rss+xml" title="Opinion RSS 2.0"        href="/rss/feeds/opinion.xml" />
<link rel="alternate" type="application/rss+xml" title="Kode Vicious RSS 2.0"   href="/rss/feeds/kodevicious.xml" />
<link rel="alternate" type="application/rss+xml" title="ACM TechNews RSS"       href="https://www.infoinc.com/acm/TechNews.rss" />
<link rel="alternate" type="application/rss+xml" title="Washington Updates RSS" href="https://usacm.acm.org/weblog2/?feed=rss2" />
<link rel="alternate" type="application/rss+xml" title="RISKS Forum RSS"        href="/rss/feeds/risksforum.xml" />


<link rel="alternate" type="application/rss+xml" title="AI RSS 2.0"        href="/rss/feeds/ai.xml" />

<link rel="alternate" type="application/rss+xml" title="API Design RSS 2.0"        href="/rss/feeds/apidesign.xml" />

<link rel="alternate" type="application/rss+xml" title="Bioscience RSS 2.0"        href="/rss/feeds/bioscience.xml" />

<link rel="alternate" type="application/rss+xml" title="Blockchain RSS 2.0"        href="/rss/feeds/blockchain.xml" />

<link rel="alternate" type="application/rss+xml" title="Business/Management RSS 2.0"        href="/rss/feeds/business/management.xml" />

<link rel="alternate" type="application/rss+xml" title="Compliance RSS 2.0"        href="/rss/feeds/compliance.xml" />

<link rel="alternate" type="application/rss+xml" title="Component Technologies RSS 2.0"        href="/rss/feeds/componenttechnologies.xml" />

<link rel="alternate" type="application/rss+xml" title="Computer Architecture RSS 2.0"        href="/rss/feeds/computerarchitecture.xml" />

<link rel="alternate" type="application/rss+xml" title="Concurrency RSS 2.0"        href="/rss/feeds/concurrency.xml" />

<link rel="alternate" type="application/rss+xml" title="Cryptocurrency RSS 2.0"        href="/rss/feeds/cryptocurrency.xml" />

<link rel="alternate" type="application/rss+xml" title="DSPs RSS 2.0"        href="/rss/feeds/dsps.xml" />

<link rel="alternate" type="application/rss+xml" title="Data RSS 2.0"        href="/rss/feeds/data.xml" />

<link rel="alternate" type="application/rss+xml" title="Databases RSS 2.0"        href="/rss/feeds/databases.xml" />

<link rel="alternate" type="application/rss+xml" title="Debugging RSS 2.0"        href="/rss/feeds/debugging.xml" />

<link rel="alternate" type="application/rss+xml" title="Development RSS 2.0"        href="/rss/feeds/development.xml" />

<link rel="alternate" type="application/rss+xml" title="Distributed Computing RSS 2.0"        href="/rss/feeds/distributedcomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="Distributed Development RSS 2.0"        href="/rss/feeds/distributeddevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Education RSS 2.0"        href="/rss/feeds/education.xml" />

<link rel="alternate" type="application/rss+xml" title="Email and IM RSS 2.0"        href="/rss/feeds/emailandim.xml" />

<link rel="alternate" type="application/rss+xml" title="Embedded Systems RSS 2.0"        href="/rss/feeds/embeddedsystems.xml" />

<link rel="alternate" type="application/rss+xml" title="Failure and Recovery RSS 2.0"        href="/rss/feeds/failureandrecovery.xml" />

<link rel="alternate" type="application/rss+xml" title="File Systems and Storage RSS 2.0"        href="/rss/feeds/filesystemsandstorage.xml" />

<link rel="alternate" type="application/rss+xml" title="Game Development RSS 2.0"        href="/rss/feeds/gamedevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Graphics RSS 2.0"        href="/rss/feeds/graphics.xml" />

<link rel="alternate" type="application/rss+xml" title="HCI RSS 2.0"        href="/rss/feeds/hci.xml" />

<link rel="alternate" type="application/rss+xml" title="Managing Megaservices RSS 2.0"        href="/rss/feeds/managingmegaservices.xml" />

<link rel="alternate" type="application/rss+xml" title="Mobile Computing RSS 2.0"        href="/rss/feeds/mobilecomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="Networks RSS 2.0"        href="/rss/feeds/networks.xml" />

<link rel="alternate" type="application/rss+xml" title="Object-Relational Mapping RSS 2.0"        href="/rss/feeds/object-relationalmapping.xml" />

<link rel="alternate" type="application/rss+xml" title="Open Source RSS 2.0"        href="/rss/feeds/opensource.xml" />

<link rel="alternate" type="application/rss+xml" title="Patching and Deployment RSS 2.0"        href="/rss/feeds/patchinganddeployment.xml" />

<link rel="alternate" type="application/rss+xml" title="Performance RSS 2.0"        href="/rss/feeds/performance.xml" />

<link rel="alternate" type="application/rss+xml" title="Power Management RSS 2.0"        href="/rss/feeds/powermanagement.xml" />

<link rel="alternate" type="application/rss+xml" title="Privacy and Rights RSS 2.0"        href="/rss/feeds/privacyandrights.xml" />

<link rel="alternate" type="application/rss+xml" title="Processors RSS 2.0"        href="/rss/feeds/processors.xml" />

<link rel="alternate" type="application/rss+xml" title="Programming Languages RSS 2.0"        href="/rss/feeds/programminglanguages.xml" />

<link rel="alternate" type="application/rss+xml" title="Purpose-built Systems RSS 2.0"        href="/rss/feeds/purpose-builtsystems.xml" />

<link rel="alternate" type="application/rss+xml" title="Quality Assurance RSS 2.0"        href="/rss/feeds/qualityassurance.xml" />

<link rel="alternate" type="application/rss+xml" title="RFID RSS 2.0"        href="/rss/feeds/rfid.xml" />

<link rel="alternate" type="application/rss+xml" title="SIP RSS 2.0"        href="/rss/feeds/sip.xml" />

<link rel="alternate" type="application/rss+xml" title="Search Engines RSS 2.0"        href="/rss/feeds/searchengines.xml" />

<link rel="alternate" type="application/rss+xml" title="Security RSS 2.0"        href="/rss/feeds/security.xml" />

<link rel="alternate" type="application/rss+xml" title="Semi-structured Data RSS 2.0"        href="/rss/feeds/semi-structureddata.xml" />

<link rel="alternate" type="application/rss+xml" title="Social Computing RSS 2.0"        href="/rss/feeds/socialcomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="System Administration RSS 2.0"        href="/rss/feeds/systemadministration.xml" />

<link rel="alternate" type="application/rss+xml" title="System Evolution RSS 2.0"        href="/rss/feeds/systemevolution.xml" />

<link rel="alternate" type="application/rss+xml" title="Testing RSS 2.0"        href="/rss/feeds/testing.xml" />

<link rel="alternate" type="application/rss+xml" title="Virtual Machines RSS 2.0"        href="/rss/feeds/virtualmachines.xml" />

<link rel="alternate" type="application/rss+xml" title="Virtualization RSS 2.0"        href="/rss/feeds/virtualization.xml" />

<link rel="alternate" type="application/rss+xml" title="Visualization RSS 2.0"        href="/rss/feeds/visualization.xml" />

<link rel="alternate" type="application/rss+xml" title="VoIP RSS 2.0"        href="/rss/feeds/voip.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Development RSS 2.0"        href="/rss/feeds/webdevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Security RSS 2.0"        href="/rss/feeds/websecurity.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Services RSS 2.0"        href="/rss/feeds/webservices.xml" />

<link rel="alternate" type="application/rss+xml" title="Workflow Systems RSS 2.0"        href="/rss/feeds/workflowsystems.xml" />

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-6562869-1']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<script type="text/javascript">
function plusone_vote( obj ) {
_gaq.push(['_trackEvent','plusone',obj.state]);
}
</script>



<style>
body {
	font-family: jaf-bernino-sans, 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans', Geneva, Verdana, sans-serif;
	color: #333;
	max-width: 100%;
}
div.container p {
	line-height: 1.65em;
}
h1 {
	font-size: 32px;
}
h3 {
	font-size: 18px;
}
h4 {
	font-size: 14px;
}

div.container {
	margin-left: auto;
	margin-right: auto;
}

div {
	margin: 64px;
//	max-width: 800px;
	position: relative;
}

@media only screen and (min-width: 1024px) {
	div {
		max-width: 800px;
	}
}

img {
    max-width: 100%;
    height: auto;
    width: auto\9; /* ie8 */
}
a {
	color: #009;
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
hr {
	margin:64px;
}
label {
	font-size: 0.8em;
	color: #666;
}
input {
	color: #999;
}

/* NAVBAR */
.navbar {
//	position: fixed;
	background: #EEEEEE;
	top: -64px;
	z-index: 10000;
	width: 100%;
	clear: both;
	padding: 0px;
	margin: 0px;
	padding-top: 10px;
	padding-left: 10px;
	padding-right: 10px;
}

/*  SECTIONS  */
.section {
	clear: both;
	padding: 0px;
	margin: 0px;
}

/*  COLUMN SETUP  */
.col {
	display: block;
	float:left;
	margin: 1% 0 1% 1.6%;
}
.col:first-child { margin-left: 0; }


/*  GROUPING  */
.group:before,
.group:after {
	content:"";
	display:table;
}
.group:after {
	clear:both;
}
.group {
    zoom:1; /* For IE 6/7 */
}

/*  GRID OF THREE  */
.span_3_of_3 {
	width: 100%;
}
.span_2_of_3 {
	width: 66.1%;
}
.span_1_of_3 {
	width: 32.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.col {
		margin: 1% 0 1% 0%;
	}
}

@media only screen and (max-width: 480px) {
	.span_3_of_3 {
		width: 100%;
	}
	.span_2_of_3 {
		width: 100%;
	}
	.span_1_of_3 {
		width: 100%;
	}
}

.span_2_of_2 {
	width: 100%;
}

.span_1_of_2 {
	width: 49.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.span_2_of_2 {
		width: 100%;
	}
	.span_1_of_2 {
		width: 100%;
	}
}
</style>


<style>
body {
	font-size: 19px;
}
#form-search > .st-default-search-input {
	width: 170px;
  display: inline-block;
  height: 16px;
  padding: 7px 11px 7px 28px;
  border: 1px solid #bbb;
  border: 1px solid rgba(0,0,0,0.25);
  font-weight: 400;
  color: #3B454F;
  font-size: 14px;
  line-height: 16px;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-box-shadow: none;
  -moz-box-shadow: none;
  box-shadow: none;
  font-family: system, -apple-system, BlinkMacSystemFont, "Helvetica Neue", "Lucida Grande", sans-serif;
}


blockquote
{
    color: #666;
    font-size: 1.1em;
    background: none;
    border-left: .2rem solid #d3d3d3;

    display: block;
    padding: 20px 20px 10px 45px;
    margin: 20px 0;
    font-style: italic;

    margin-block-start: 1em;
    margin-block-end: 1em;
    margin-inline-start: 40px;
    margin-inline-end: 40px;

	font-family: Georgia, Palatino, "Palatino Linotype", Times, "Times New Roman", serif;
}

.ldq {
	display: block;
    padding-left: 10px;
    content: "\201C";
    font-size: 60px;
    position: relative;
    left: -50px;
    top: 0;
    height: 0;
    color: #7a7a7a;
}
code {
//	font-size:1.25em;
}
a {overflow-wrap: break-word;}
pre {
	overflow-x: auto;
	white-space: pre-wrap;
	word-wrap: break-word;
}
</style>
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
<!-- Google Tag Manager (noscript)
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P52H78L"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
 End Google Tag Manager (noscript) -->



<div class=container>
	<div class="navbar">
		<form id="form-search" name="searchform" onsubmit="return false;" style='float:right;'>
				<input type="text" class="st-default-search-input">
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','UyYECD1kdsPnbHJtPyzG','2.0.0');
</script>
				<br />
			
			<a href="issuedetail.cfm?issue=3786297" style='width:150px;font-size:0.7em;'>Current Issue</a> &nbsp; <a href="pastissues.cfm" style='width:150px;font-size:0.7em;'>Past Issues</a> &nbsp; <a href="topics.cfm" style='width:150px;font-size:0.7em;'>Topics</a>
			
		</form>
		<a href='/'><img src='https://queue.acm.org/img/acmqueue_logo.gif' /></a>

	</div>

<div style='border:1px red solid;padding:20px;margin:20px;padding-left:40px;margin-left:40px;font-size:1.2em;'>
We are redesigning the Queue website.
<br />
<a href='https://spawn-queue.acm.org/' target='_new'>Please take a look and let us know what you think</a>.
</div>

<!--
<p style='text-align:center;'>
<a href='/app/' target='_new'><img src='/app/2021_03-04_lrg.png' with=90 height=120 style='float:right;width:90px;height:120px;' alt='March/April 2021 issue of acmqueue' /></a>
<b><a href='/app/'>The March/April 2021 issue of acmqueue is out now</a></b>
<br />
<br />
<a href='https://cdn.coverstand.com/3rd_pty/acm/login.html?&btx_i=705849'>Subscribers and ACM Professional members login here</a>
<br clear=all />
<hr style='display:block;color:red;margin:5px;' />
</p>
-->
<br />



<label>November 7, 2025<br /><b><a class="descriptor" href="issuedetail.cfm?issue=3775067">Volume 23, issue 5 </a></b></label>


<p>
<!-- // Check for existence of associated MP3 file-->

 &nbsp;
	
				<a href="https://spawn-queue.acm.org/doi/pdf/10.1145/3773098">
					<img src="img/icon_pdf.png" alt="Download PDF version of this article" />
					PDF
				</a>
			
</p>


 
  <h1 class="hidetitle">Safe Coding</h1> 
  <h2>Rigorous modular reasoning about software safety</h2> 
  <h3>Christoph Kern</h3> 
  <p>Many dangerous and persistent software vulnerabilities, including memory-safety violations and code injection, stem from a common root cause: developers unintentionally violating implicit safety preconditions when using common programming constructs. In large, complex systems, these preconditions often rely on nonlocal, whole-program invariants that are difficult for any single developer to reason about correctly and consistently. Traditional approaches such as developer education and reactive bug detection have proven insufficient to reduce these vulnerabilities to an acceptable level. Fundamentally, development environments make it too easy for well-intentioned developers to introduce subtle yet potentially catastrophic coding errors.</p> 
  <p>This article introduces <i>safe coding</i>, a collection of software design patterns and practices that cost-effectively provides a high degree of assurance against entire classes of such vulnerabilities. The core idea is to shift responsibility for safety from the individual developer to the programming language, libraries, and frameworks. Safe coding achieves this by identifying <i>risky operations</i>—those with complex safety preconditions—and systematically eliminating their direct use in application code. Instead, risky operations must be encapsulated within <i>safe abstractions:</i> modules whose public APIs are safe to use by design and whose implementations take full responsibility for satisfying all internal safety preconditions.</p> 
  <p>These design patterns have been successfully applied at Google, nearly eliminating classes of software vulnerabilities such as XSS (cross-site scripting) and SQL injection. Safe abstractions are also a core design principle in the Rust developer community, critical for achieving high-assurance memory safety despite the necessary presence of unsafe Rust in performance-critical and low-level systems code.</p> 
  <p>Safe coding embodies a modular, compositional approach to building and reasoning about the safety of large, complex systems. Difficult and subtle reasoning about the safety of abstractions is localized to their implementations; the safety of risky operations within an abstraction must rely solely on assumptions supported by the abstraction's APIs and type signatures. Conversely, the composition of safe abstractions with <i>safe code</i> (i.e., code free of risky operations, which constitutes the vast majority of a program) is automatically verified by the implementation language's type checker. While not a formal method itself, safe coding is grounded in principles and techniques from rigorous, formal software verification. It pragmatically adapts concepts such as function contracts and modular proofs for practical large-scale use by lifting safety preconditions into type invariants of custom data types within the chosen implementation language.</p> 
  <p>This article explores these technical and formal underpinnings, demonstrating how they enable cost-effective yet rigorous reasoning about software safety in very-large-scale industrial software development. An extended version of this article provides further discussion of some of the more technical aspects of this approach.<sup>14</sup></p> 
  <p>&nbsp;</p> 
  <h3>Software Specifications, Correctness, and Safety</h3> 
  <p>In a formal sense, a program or component is <i>correct</i> relative to a specification when:</p> 
  <p>• It implements all behaviors required by the specification (for example, an API service responds to requests with a specified answer or an appropriate error).</p> 
  <p>• Every possible behavior of the program or component is permitted by the specification.</p> 
  <p>The most rigorous approach to demonstrating program correctness relies on capturing both its specification and implementation in a formal, mathematical framework such as a program logic and constructing a mathematical proof that the implementation satisfies the specification.<sup>9</sup></p> 
  <p>For industrial-scale software, however, developing a comprehensive formal specification—let alone formally proving an implementation's correctness against the specification—is difficult and costly. Despite ongoing improvements in theory and tooling, formal methods still tend to be applied primarily to safety-, security- and reliability-critical components, if at all.<sup>7,10</sup></p> 
  <p>Specifications for end-to-end, large-scale industrial software systems such as application servers and their web and mobile clients are usually presented in informal prose, for example as a set of CUJs (critical user journeys) that describe key expected behaviors of the software. Such informal specifications are usually incomplete, and they neither comprehensively describe all required behaviors nor precisely delineate permitted from undesired behaviors.</p> 
  <p>&nbsp;</p> 
  <h4>Software failures</h4> 
  <p>With respect to informal, partial specifications, software failures fall into two broad categories:</p> 
  <p>• The software's behavior directly conflicts with its (partial) specification—for example, it doesn't produce the specified result when exercised according to a CUJ.</p> 
  <p>• The software exhibits behaviors that, while not necessarily contradicting an explicit specification, are nonetheless undesired. Such <i>erroneous behavior</i> often arises from unusual, unexpected, or malicious inputs that exercise execution paths the system's designers did not anticipate. Since informal specifications rarely delineate the full scope of permitted behavior, the line between correct and erroneous behavior is often implicit and underspecified.</p> 
  <p>Many practically relevant kinds of erroneous behaviors arise from the improper use of foundational components the software is based on, such as the programming language, library APIs, and software frameworks and platforms. Examples of such classes of erroneous behavior include:</p> 
  <p>• Undefined behavior (see sidebar, "Undefined Behavior"), including memory-safety violations, in unsafe languages such as C and C++ and in unsafe language fragments such as unsafe Rust.</p> 
  <p>• Runtime errors raised by certain APIs and operations when presented with invalid arguments—for example, division by zero or a bounds-checked, out-of-bounds array access.</p> 
  <p>• When untrusted, external inputs are passed to an API that parses or interprets strings according to a syntactic and semantic structure, allowing the attacker to control interpretation of a string in an unintended fashion; this often leads to so-called injection vulnerabilities such as XSS, SQL injection, shell injection, and path traversal.</p> 
  <p>• Deadlocks in multithreaded software architectures.</p> 
  <p>Some of these erroneous behaviors can lead to catastrophic software failures: For example, memory-safety violations and code injection can allow an attacker to execute arbitrary code with the privileges of the vulnerable software, in turn permitting them to completely evade the software's intended security policy. In many cases, this represents a much more severe impact than many failures of the first category, such as not supporting a CUJ in certain edge cases. Indeed, the majority of the CWE Top 25 Most Dangerous Software Weaknesses<sup>5</sup> and Top 10 Known-Exploited Vulnerabilities<sup>4</sup> have their root causes in these common types of erroneous behaviors.</p> 
  <p>&nbsp;</p> 
  <div style="border:1px solid black;padding:20px;margin:20px;"> 
   <h4>Undefined Behavior </h4> 
   <p>A particularly troublesome class of erroneous behavior lies in so-called <i>undefined behavior:</i> For various reasons (including performance—avoiding the need for runtime mechanisms to trap errors), the standards defining languages such as C and C++ declare that executing certain erroneous operations constitutes <i>undefined behavior</i>. Undefined behaviors in the ISO C standard include:<sup>11</sup></p> 
   <p>• Division by zero</p> 
   <p>• Conversion to or from an integer type producing a value outside the range that can be represented</p> 
   <p>• An array subscript being out of range</p> 
   <p>• An object referred to outside of its lifetime</p> 
   <p>• The execution of a program containing a data race</p> 
   <p>• And many more (Annex J.2 of the ISO C standard lists more than 200 items)</p> 
   <p>The language standard imposes no requirements whatsoever on a program that encounters undefined behavior—the program is permitted to continue executing and do absolutely anything.<sup>17</sup> Even worse, "allowed to do anything" applies not just <i>after</i> the erroneous operation—it can <a href="https://llvm.org/docs/UndefinedBehavior.html#time-travel">"time travel"</a>:</p> 
   <p>• The <i>entire</i> program execution is considered meaningless.</p> 
   <p>• The compiler is allowed to make optimizations based on the assumption that no execution of the program will encounter undefined behavior.</p> 
   <p>Consider the following program as an example:</p> 
   <p><code> #include
     <limits.h> 
      <br />#define LEN 42 
      <br />int a[LEN] = {1,2,3}; 
      <br /> 
      <br />int f(int base, int off) { 
      <br /> &nbsp; &nbsp;if (base &lt; 0 || off &lt; 0) return -1; 
      <br /> &nbsp; &nbsp;int idx = base + off; 
      <br /> &nbsp; &nbsp;if (idx &lt; 0) return -2; 
      <br /> &nbsp; &nbsp;if (idx &gt;= LEN) return -3; 
      <br /> &nbsp; &nbsp;return a[idx]; 
      <br />} 
      <br /> 
      <br />int main() { 
      <br /> &nbsp; &nbsp;return f(INT_MAX,2); 
      <br />} 
     </limits.h></code></p> 
   <p>This program accesses an element of array <code>a</code> whose index is determined as the sum of a <code>base</code> index and an offset. The code appears to carefully check that both <code>base</code> and <code>off</code> are non-negative and then checks <i>again</i> that their sum <code>idx</code> is non-negative and in bounds of the array.</p> 
   <p>Per the C standard, however, signed integer overflow constitutes undefined behavior. The compiler is allowed to assume that the expression <code>base + off</code> does not overflow and can conclude from both operands being non-negative that <code>idx</code> is non-negative as well. An optimizing compiler is therefore permitted to eliminate the check whether <code>idx</code> is non-negative, which is redundant under this assumption.</p> 
   <p>Indeed, gcc 14.2 with option <code>–O1</code> does exactly that: The check does not appear in the resulting x86–64 assembly (see this example in Compiler Explorer: <a href="https://godbolt.org/z/4hqnj6TEW">https://godbolt.org/z/4hqnj6TEW</a>; Clang produces similar code). In fact, the compiler applies inlining and constant folding optimizations to function <code>f</code> <i>after</i> removing the "redundant" check, resulting in assembly code for the <code>main</code> function that consists of a "hardwired" out-of-bounds access of the array:</p> 
   <p><code> main: <br /> &nbsp;movabs eax, <br /> &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;DWORD PTR [a-8589934588] <br /> &nbsp;ret </code></p> 
  </div> 
  <p>&nbsp;</p> 
  <h4>Software safety</h4> 
  <p>A program is deemed <i>safe</i>—regarding a set of erroneous behaviors (e.g., division by zero, out-of-bounds access, undefined behaviors, or execution of untrusted inputs)— if no possible execution exhibits such behaviors. To be safe, a program must avoid these behaviors even when interacting with (and exposed to arbitrary inputs from) a malicious external environment.</p> 
  <p>Note that in the PL (programming languages) research community, safety is usually defined as the absence of any <i>untrapped</i> errors.<sup>3</sup> The definition of safety used in this article is more general—and relative to a set of erroneous behaviors relevant to the application domain; this allows us, for example, to consider erroneous behavior related to higher-level APIs (such as a SQL query API prone to code injection) and to include (uncaught) trapped errors in application domains where they are unacceptable (such as safety- and reliability-critical systems).</p> 
  <p>While the absence of erroneous behaviors is often not explicitly mentioned in informal software specifications and requirements documents, it is nevertheless a critical aspect of correctness: If a program can encounter undefined behavior and execute arbitrary, attacker-controlled code, it's hard to argue the program is correct in any meaningful sense. Furthermore, failures of the second category often form the root cause of the first: for example, undefined behavior encountered during a CUJ that results in a system crash and data corruption.</p> 
  <p>&nbsp;</p> 
  <h4>Demonstrating safety and correctness</h4> 
  <p>Since absence of erroneous behavior is such an important aspect of correctness, it is necessary to achieve a high degree of confidence that software is indeed safe and will not exhibit the types of erroneous behavior relevant in the application's domain, even when placed in an adversarial environment.</p> 
  <p><i>Testing</i> can be quite effective at building confidence that software works as desired in expected, explicitly considered usage scenarios. For example, an integration test for a software system would confirm that it satisfies an informally specified CUJ such as "when a user completes the signup flow, a corresponding account record is created," and appropriately handles expected error conditions—for example, "An account record with the chosen user ID already exists."</p> 
  <p>In contrast, to rule out <i>erroneous behavior</i> with a high degree of confidence, one has to show that all possible executions avoid the undesired behavior, even for the most obscure edge cases and when exposed to adversarial inputs. Even with techniques such as coverage-guided fuzzing, testing is inherently limited in its ability to provide high confidence in statements quantified over all possible executions of a nontrivial program—as E.W. Dijkstra observed more than 50 years ago, " testing can [show] the presence of bugs but never [ ] their absence."<sup>6</sup></p> 
  <p>Since the achievable confidence is limited with approaches such as testing and fuzzing that rely on <i>observing behavior</i> over a <i>sample</i> of executions, we need to consider approaches based on <i>reasoning</i> about <i>all</i> possible behaviors of the software in the <i>abstract</i>.</p> 
  <p>&nbsp;</p> 
  <h3>Safe Coding</h3> 
  <p>Safe coding is a collection of software design practices and patterns that allow for <i>cost-effectively</i> achieving a high degree of confidence that a large, industrial-scale program will not exhibit erroneous behavior, even when exposed to an adversarial environment.</p> 
  <p>Safe coding offers a pragmatic approach to achieving high-assurance software safety, drawing inspiration from modular formal verification techniques. While structurally analogous to formal methods that rely on function contracts, safe coding adapts these concepts for large-scale industrial development. Instead of complex formal specifications, safety contracts are expressed through the type system and enforced by the language's standard type checker. </p> 
  <p>Rigorous (though typically informal) expert-led reasoning about correctness remains necessary but is localized to the self-contained implementations of <i>safe abstractions</i>—which are usually a small and stable portion of the overall program. This design avoids the challenges of traditional static analysis, which often struggles to achieve sufficient soundness and precision for whole-program analysis of large complex codebases.<sup>15,16</sup> By deliberately structuring code for easy analysis, the problem of verifying safety is largely reduced to scalable type checking. This positions safe coding as a semi-formal method, and our experience at Google over the past decade has shown it provides a favorable cost-assurance tradeoff.</p> 
  <p>&nbsp;</p> 
  <h4>Reasoning about safety</h4> 
  <p>The root cause of many classes of erroneous behavior are operations or APIs that can be used safely only if certain conditions on the program state hold true at the moment of execution. An operation's <i>safety precondition</i> is defined as a predicate on program state that must hold immediately before its execution to ensure it does not exhibit erroneous behavior.</p> 
  <p>Consider a few common examples:</p> 
  <p>• Array access: Accessing <code>a[i]</code> has the safety precondition that <code>i</code> must be within the array's valid bounds.</p> 
  <p>• Pointer dereference: Dereferencing <code>*p</code> requires that <code>p</code> points to a valid, allocated memory location.</p> 
  <p>• SQL query execution: An API such as <code>db.Query(sql)</code> assumes <code>sql</code> is a safely constructed query, not one tainted by untrusted input.</p> 
  <p>Violating these preconditions can lead to some of the most severe software vulnerabilities (here, memory-safety violations and SQL injection). It is typically the programmer's responsibility to ensure that these preconditions are met. An operation with such a programmer-visible safety precondition is referred to as a <i>risky operation</i>. A block of code containing one or more risky operations is called <i>risky code.</i> (We use the term <i>risky code</i> instead of the more common <i>unsafe code</i> because it only potentially exhibits erroneous behavior but is in fact safe if the programmer correctly ensures its safety preconditions.)</p> 
  <p>In large and complex software systems, verifying that a risky operation's safety precondition is always met can be exceptionally difficult. The state that determines the precondition's validity might be established by code that is syntactically distant, perhaps in an entirely different module. For example, the validity of a pointer passed to a function may depend on complex object lifecycle logic managed elsewhere in the application.</p> 
  <p>Consequently, reasoning about safety often degrades into a nonlocal, <i>whole-program</i> affair. Expecting developers to reason correctly about intricate, systemwide invariants for every risky operation is unrealistic and error prone. This reliance on brittle, manual, whole-program reasoning is a systemic root cause for the continued prevalence of many common classes of vulnerabilities. The key objective of the safe-coding approach is to restructure programs to eliminate this need.</p> 
  <p>&nbsp;</p> 
  <h3>Safe Abstractions</h3> 
  <p>Since every use of a risky operation represents a potential hazard, it's clear that safe coding should aim to minimize the use of risky operations throughout a program's codebase.</p> 
  <p>Real-world programs, however, cannot be written without risky operations altogether: One obviously can't write a program that relies on a SQL database without calling SQL query APIs somewhere.</p> 
  <p>This creates a problem: Suppose <code>db.Query(sql)</code> is called from a function <code>getWidgetsBy(criterion)</code> whose implementation concatenates its argument <code>criterion</code> (intended to be a predicate in SQL syntax that specifies what kinds of widgets to return) into <code>sql</code>. In turn, <code>getWidgetsBy</code> is called many times throughout the application's codebase (see figure 1). In this scenario, the call to <code>db.Query</code> can exhibit erroneous behavior (resulting in a SQL injection vulnerability) if the parameter to <code>getWidgetsBy</code> incorporates untrusted program inputs without appropriate validation. That is, <code>getWidgetsBy</code> also has an implicit safety precondition and therefore is itself a risky operation. Since it's being used widely throughout the codebase, we're back to whole-program reasoning about safety.</p> 
  <img src="https://spawn-queue.acm.org/cms/attachment/html/10.1145/3773098/assets/html/kern1.svg" alt="FIGURE 1: Transitively risky operations" /> 
  <br /> FIGURE 1: Transitively risky operations 
  <p>To address this issue, the safe-coding paradigm stipulates that all risky code should be encapsulated in a <i>safe abstraction</i>. This is a software module that has both of the following properties:</p> 
  <p>• The module's public APIs have <i>no safety preconditions</i> that are the programmer's responsibility to ensure.</p> 
  <p>• The module's implementation comprehensively ensures that the <i>safety preconditions of all risky operations within the module</i> are always satisfied.</p> 
  <p>That is, a safe abstraction must take full responsibility for its <i>internal</i> safety preconditions without placing a burden on developers to ensure preconditions of its <i>external</i> API—if it did, it wouldn't be a <i>safe</i> abstraction. However, for a given risky operation and its safety precondition, it's often not straightforward to come up with a precondition-free safe abstraction. The following sections discuss helpful design patterns for safe-abstraction APIs.</p> 
  <p>&nbsp;</p> 
  <h3>Runtime Safety Checks</h3> 
  <p>A straightforward way for a safe abstraction to ensure the safety precondition of a risky operation within its implementation is to explicitly check the precondition. This approach conforms to the stipulations for a safe abstraction: The operation's safety precondition trivially holds because it's ensured by a runtime check right then and there, and the check happens within the abstraction's implementation and is not the responsibility of its callers.</p> 
  <p>For example, in C and C++, the subscript operator in an expression like <code>a[i]</code> is a risky operation with the safety precondition that the index <code>i</code> is within the bounds of the array <code>a</code>.</p> 
  <p>In contrast, the implementations of the subscript operator of array, vector, and slice types in memory-safe languages such as Java, Go, and Rust include a runtime bounds check that verifies that the provided index is in bounds and otherwise raises an error.</p> 
  <p>The introduction of runtime safety checks often requires a change to the implementation to add sufficient metadata and bookkeeping information to check against. For example, a C array does not carry any information about the allocated bounds of the array (an array is essentially equivalent to a simple pointer). To enable bounds checks, vector types in safe languages need to carry the allocated size of that instance. For example, a conceptual, simplified Rust vector might look like the following:</p> 
  <p><code> pub struct Vec&lt;T&gt; { <br /> &nbsp; &nbsp;buf: *const T, <br /> &nbsp; &nbsp;len: usize, <br />} <br />impl&lt;T&gt; Vec&lt;T&gt; { <br /> &nbsp; &nbsp;pub fn new(len: usize) -&gt; Vec&lt;T&gt; { ... } <br /> &nbsp; &nbsp;pub fn index(&amp;self, i: usize) -&gt; &amp;T { <br /> &nbsp; &nbsp; &nbsp; &nbsp;if i &lt; self.len { <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;unsafe { &amp;*self.buf.add(i) } <br /> &nbsp; &nbsp; &nbsp; &nbsp;} <br /> &nbsp; &nbsp; &nbsp; &nbsp;else { panic!("Index out of bounds!") } <br /> &nbsp; &nbsp;} <br />} </code></p> 
  <p>The constructor <code>Vec::new</code> must ensure that the raw pointer <code>buf</code> points to allocated memory of sufficient size to hold <code>len</code> elements of type <code>T</code>. The implementation of the <code>index</code> method then checks <code>i &lt; self.len</code> before returning a reference to the vector's <code>i</code>th element at the appropriate offset from <code>buf</code>.</p> 
  <p>Dereferencing a raw pointer at an offset is a risky operation with the safety precondition that the offset is in bounds (as well as the pointer itself being valid, to which we will return later). In Rust, risky operations are made conspicuous by requiring that they are enclosed in an <code>unsafe</code> block. Because the bounds check immediately precedes the risky code, however, it's straightforward to establish that it is in fact safe; thus our <code>Vec</code> datatype is a safe abstraction around the raw pointer arithmetic required to implement array indexing.</p> 
  <p>This safety comes at the cost of a runtime check for each access—which, however, typically can be mitigated through compiler optimizations and careful design. When a check fails, the program can abort, raise an exception, or return an error to the caller. The best approach depends on the application's needs and the programming language's idioms. For a detailed discussion of these tradeoffs, see the extended version of this article.<sup>14</sup></p> 
  <p>Adding a runtime check does not actually remove the operation's precondition; rather, it is downgraded from a <i>safety</i> precondition to a standard, functional precondition: Violating the precondition no longer results in erroneous behavior and a potential vulnerability; instead, the failure is handled through well-defined behavior that is deemed tolerable in the given application domain.</p> 
  <p>&nbsp;</p> 
  <h3>Preconditions as Type invariants</h3> 
  <p>Revisiting the <code>index</code> method of the <code>Vec</code> example from the previous section points out two details worth noting:</p> 
  <p>1. The bounds check explicitly tests only the upper bound but does not check that <code>i</code> is non-negative.</p> 
  <p>2. Beyond the bounds check, the code relies on further implicit preconditions for the risky pointer dereference <code>&amp;*self.buf.add(i)</code> to be safe: The pointer <code>self.buf</code> must point to a valid allocated memory region of sufficient size that has been initialized with a valid representation of <code>len</code> elements of type <code>T</code>.</p> 
  <p>The non-negativity of <code>i</code> is actually guaranteed by its type: <code>usize</code> is an unsigned integer type in Rust, so its values are always non-negative. Thus, an explicit check such as <code>i &gt;= 0</code> is unnecessary.</p> 
  <p>The validity of <code>self.buf</code> at the point of dereferencing <code>index</code> is more subtle. Its validity can be inferred by reasoning about the properties of the <code>Vec</code> type as a whole. Given that the constructor <code>Vec::new</code> allocates memory for the buffer and that no other method in the type's implementation frees this memory during the object's lifetime, one can conclude that the pointer remains valid. Since <code>buf</code> is a private field, no code outside the module can access and invalidate it—for example, by freeing the memory it points to.</p> 
  <p>Properties that are guaranteed to hold true for all instances of a type throughout their lifetimes, such as the continued validity of <code>self.buf</code> and the correctness of <code>self.len</code> in relation to the allocated buffer, are known as <i>type invariants</i>. When code is using a value of a particular type, it can rely on that type's invariants holding true.</p> 
  <p>For the <code>Vec&lt;T&gt;</code> type, the key invariants are:</p> 
  <p>• <code>buf</code> points to a valid, allocated memory region.</p> 
  <p>• This memory region is large enough to hold <code>len</code> elements of type <code>T</code>.</p> 
  <p>• Each of the <code>len</code> elements in this memory region is initialized to a valid value of type <code>T</code>.</p> 
  <p>Confidence in these invariants is established by carefully inspecting all methods of <code>Vec</code>:</p> 
  <p>The type's constructor <code>Vec::new</code> (not shown in the snippet) must establish the invariants when an instance is created: It is responsible for allocating and initializing sufficient memory and for initializing <code>buf</code> and <code>len</code> accordingly.</p> 
  <p>All of the type's methods must maintain the invariants: Under the assumption that the invariants hold before a method is called, they must also hold after the method completes. In this case, none of <code>Vec</code>'s methods frees memory while the <code>Vec</code> instance is live (deallocation occurs only when the instance is destructed in an implementation of the <code>Drop</code> trait for <code>Vec</code>, omitted from the listing for brevity).</p> 
  <p>Type encapsulation ensures that the invariants are not disturbed by code elsewhere in the program: <code>buf</code> is a private field, meaning no code outside the <code>Vec</code> module can access <code>buf</code> directly, for example, to free the memory it points to or change <code>buf</code> to an invalid pointer.</p> 
  <p>When <code>Vec::index</code> is invoked, Rust's type system ensures that <code>self</code> is a valid reference to an instance of <code>Vec</code>. Because of this, the <code>index</code> method can rely on <code>Vec</code>'s established type invariants. These invariants, in conjunction with the explicit <code>i &lt; self.len</code> check, support the conclusion—without any additional runtime checks—that the safety preconditions of the pointer dereference operation <code>&amp;*self.buf.add(i)</code> are always satisfied.</p> 
  <p>The example illustrates a powerful design pattern for ensuring safety preconditions of risky operations, which is applicable even when these preconditions are difficult, expensive, or infeasible to verify with a runtime check. The key elements of this approach are:</p> 
  <p>• <b>Elevating</b> safety preconditions into type invariants by designing safe abstractions that leverage types whose invariants capture the safety preconditions of relevant risky operations. Often, this involves introducing custom <i>vocabulary types</i> or <i>wrapper types</i> specifically designed to carry the invariant.</p> 
  <p>• <b>Designing</b> these types' APIs (constructors and methods and related functionality such as builder APIs) and their implementations to rigorously establish and maintain their invariants. The implementations should be encapsulated so that client code cannot disturb the type invariant.</p> 
  <p>• <b>Encapsulating</b> risky operations within safe abstractions whose APIs require these invariant-carrying types. The safe abstraction is designed such that the safety preconditions of any risky operations within its implementation are implied by the type invariants of its methods' receiver and argument types, in conjunction with runtime checks if applicable.</p> 
  <h4>Example: temporal memory safety</h4> 
  <p>A key challenge in languages such as C/C++ is ensuring temporal memory safety—that is, preventing access to memory that has already been deallocated (a "use-after-free" error). Raw pointers lack the metadata needed to perform a runtime check to determine if the memory they point to is still valid. The core difficulty is that pointers are not exclusive; code elsewhere in a program can hold another pointer to the same memory and free it, invalidating the original pointer without its knowledge. This makes reasoning about pointer validity a complex, whole-program analysis problem.</p> 
  <p>In the <code>Vec</code> example, this general problem for the internal <code>buf</code> pointer is addressed by establishing a strong type invariant: <code>buf</code> is effectively a unique, private pointer to memory whose lifetime is managed solely by the <code>Vec</code> instance itself. The rules of safe Rust ensure exclusive mutability of each instance and prevent code outside the module from prematurely freeing this memory.</p> 
  <p>For more general scenarios involving shared ownership of heap-allocated data, this pattern of creating safe abstractions with underlying type invariants is also key. There are several approaches with the common goal of ensuring that live pointers and references always point to a valid memory allocation:</p> 
  <p>• Reference counting is implemented by <i>managed pointer</i> types such as Rust's <code>Rc&lt;T&gt;</code>, which associate a counter with each managed allocation. Their implementations increment and decrement this count every time an instance of the managed pointer is copied or destroyed, respectively—thus maintaining the invariant that the reference count always equals the number of live copies of the managed pointer. The underlying memory is freed only when the last copy of the managed pointer is destructed, thus ensuring the invariant that live instances of the managed pointer type always refer to valid, allocated memory.</p> 
  <p>• In garbage-collected languages such as Go, Java, Python, and many others, the language's runtime takes full responsibility for managing heap-allocated memory and ensures that memory is freed only through a garbage-collection algorithm that identifies allocations that can no longer be reached via any live reference. This, in turn, ensures the global invariant that all live references in the program point to valid, allocated memory.</p> 
  <p>• Rust's type system incorporates lifetimes, and its compiler ensures statically (without incurring runtime overhead) that the lifetime of a referenced location always exceeds the lifetime of its references.</p> 
  <p>&nbsp;</p> 
  <h4>Example: code-injection safety</h4> 
  <p>Code-injection vulnerabilities arise when an API that consumes strings that will be interpreted as code is passed values derived (at least in part) from untrustworthy inputs. If these inputs are not appropriately validated, encoded, or otherwise neutralized, an attacker can potentially control the meaning of these strings, adversarially influencing their interpretation and execution. This broad class of vulnerabilities includes well-known examples such as XSS, SQL injection, shell command injection, and LDAP (Lightweight Directory Access Protocol) injection. SQL injection is used here as a representative example of such a vulnerability that consistently ranks high in lists such as the CWE Top 25.<sup>5</sup></p> 
  <p>APIs for interacting with SQL databases typically accept a string-typed value that is then parsed and evaluated as a SQL statement. For example, the Go standard library's <code>database/sql</code> package provides a <code>Query</code> method:</p> 
  <p><code> func (db *DB) Query( <br /> &nbsp; &nbsp;query string, args ...any) <br />(*Rows, error) </code></p> 
  <p>This API carries an implicit safety precondition: The <code>query</code> string must represent a SQL statement that has been safely constructed, meaning it should originate from trustworthy (or appropriately neutralized) inputs or query fragments.</p> 
  <p>This flavor of safety precondition presents two key challenges:</p> 
  <p>1. This property depends not just on the <i>value</i> of the string, but also on its <i>provenance</i>—how it was constructed. A standard <code>string</code> type, being merely a sequence of characters, carries no metadata about its origin or how it was assembled. This means there is no straightforward way to check this precondition at runtime.</p> 
  <p>2. The phrase "has been safely constructed" is vague. To reason about safety rigorously, a more precise and enforceable definition is needed.</p> 
  <p>A widely recommended best practice for preventing SQL injection is to avoid direct concatenation of untrusted strings into a query. Instead, queries should be parameterized using placeholders, with actual values supplied via a mechanism known as parameter binding.</p> 
  <p>This practice corresponds to a stronger, more restrictive, but also more readily verifiable formulation of the safety precondition: The query must be a concatenation solely of developer-controlled strings, such as string literals embedded in the program's source code.</p> 
  <p>This precondition implies that no part of the query consists of external (and possibly untrustworthy) program inputs. It also ensures that code follows best practice: Since external inputs cannot be concatenated into a query, external parameters must be supplied via parameter binding. This, in turn, supports a high-confidence assertion that call sites of the SQL query API that adhere to this narrower safety precondition are not vulnerable to SQL injection attacks.</p> 
  <p>Consider the following code snippets. The first satisfies this stronger safety precondition and consequently adheres to the "parameter binding" best practice:</p> 
  <p><code> q1 := "SELECT y FROM table" <br />q1 += "WHERE x = ?" <br />rows, err := db.Query(q1, inputX) </code></p> 
  <p>In contrast, the following code violates the precondition and is vulnerable to injection if <code>inputX</code> is an attacker-controlled input:</p> 
  <p><code> q2 := "SELECT y FROM table" <br />q2 += "WHERE x = " + inputX <br />rows, err := db.Query(q2) </code></p> 
  <p>To design a safe abstraction for the SQL query API, there needs to be a way to programmatically distinguish between these two cases. With an API that consumes simple strings, this is difficult: When the combined string <code>q2</code> is passed to the <code>Query</code> API, there is no record in the representation of <code>q2</code> that part of the query originated from the untrusted string <code>inputX</code>.</p> 
  <p>This can be solved by introducing a simple wrapper type for strings, called <code>TrustedSqlString</code>, and elevating the required safety precondition as this type's invariant.</p> 
  <p>This invariant is upheld by carefully designing the constructors, factory functions, and builder APIs that are exclusively permitted to produce instances of <code>TrustedSqlString</code>. A key primitive is to restrict the inputs to these builders: For example, the <code>Append</code> method of a builder API for <code>TrustedSqlString</code> would be constrained to accept only compile-time-constant string expressions. This can be enforced in Go by using a module-private type alias for string or in languages such as Java through a simple custom static analysis check (for example, the <code>CompileTimeConstant</code> check in the Error Prone framework: <a href="https://errorprone.info/bugpattern/CompileTimeConstant">https://errorprone.info/bugpattern/CompileTimeConstant</a>). For a deeper discussion, see chapter 12 of <i>Building Secure and Reliable Systems</i>.<sup>1</sup></p> 
  <p>With this wrapper type in place, it is straightforward to create a safe abstraction around the underlying database APIs. This abstraction simply wraps the original API but modifies its signatures to require queries to be of type <code>TrustedSqlString</code> instead of a plain <code>string</code>:</p> 
  <p><code> type SafeDB struct { <br /> &nbsp; &nbsp;db *sql.DB <br />} <br />func (sdb *SafeDB) Query( <br /> &nbsp; &nbsp;query TrustedSqlString, args ...any) ( <br /> &nbsp; &nbsp;*sql.Rows, error) { <br /> &nbsp; &nbsp;return sdb.db.Query(query.String(), args) <br />} </code></p> 
  <p>Changing the sink API to require a <code>TrustedSqlString</code> lifts the implicit safety precondition into an explicit type contract. Any code that successfully compiles against this safe API is, by construction, not vulnerable to SQL injection, because the type system itself guarantees that the query string was constructed safely. A similar approach can be used to prevent other classes of injection vulnerabilities such as XSS.<sup>12,19</sup></p> 
  <p>&nbsp;</p> 
  <h3>Putting it all Together: Modular, Compositional Reasoning</h3> 
  <p>Safe coding restructures the problem of ensuring safety by replacing fallible, whole-program reasoning of human developers with a modular and compositional approach. The core idea is to partition the codebase into two categories: the vast majority of code that is kept free of risky operations and a small number of carefully crafted safe abstractions that encapsulate all risky operations.</p> 
  <p>Code that does not contain any risky operations is referred to as <i>safe code</i>. By definition, safe code cannot be the source of erroneous behavior arising from such operations. Whether a piece of code is "safe" in this sense can be determined automatically (see the following section).</p> 
  <p>All risky operations must be encapsulated within <i>safe abstractions</i>. As we have seen, reasoning about the correctness of a safe abstraction is <i>modular:</i> It can be done in isolation by inspecting only the implementation of the abstraction itself. This reasoning relies on the abstraction's API contract, which is expressed through its type signatures. The abstraction is assumed to be used by well-behaved code that respects the language's type and encapsulation rules and won't disturb the type invariants on which the abstraction's safety depends. This focused, modular review by domain experts is much more tractable—and likely to be correct—than reasoning about whole-program invariants.</p> 
  <p>Safe coding takes advantage of <i>compositional reasoning:</i> If a program is composed entirely of safe code and safe abstractions, then the program as a whole is safe. The validation of correct composition does not rely on further manual review but is accomplished automatically by the programming language's type checker. Ensuring that all risky operations are contained within expert-reviewed safe abstractions and that the rest of the codebase is safe code, free of risky operations, scalably provides a high degree of confidence in the safety of the entire system. For a more in-depth discussion of this topic, see the extended version of this article.<sup>14</sup></p> 
  <p>&nbsp;</p> 
  <h3>Safe Coding in Developer Platforms and Ecosystems</h3> 
  <p>The principle of compositional reasoning is powerful, but its validity in a real-world setting hinges on a critical assumption: that <i>all</i> uses of risky operations are indeed encapsulated in safe abstractions. Upholding this property across a large codebase maintained by many developers requires expert curation and disciplined practice, supported by robust tooling. Safe coding works most effectively when it is integrated into developer platforms and incorporated into the processes and workflows of the entire developer ecosystem.<sup>13</sup></p> 
  <p>A primary challenge is ensuring that the boundary between safe and risky code is strictly maintained. It is all too easy for a developer to accidentally introduce a risky operation in a module that is not a formally reviewed safe abstraction. When this happens, the module's APIs may themselves become implicitly risky without being explicitly recognized as such. This allows unsafety to "bleed out" and undermines the foundation of high-confidence compositional reasoning about desired whole-program safety properties.</p> 
  <p>To prevent this, it is instrumental for the development environment to enforce that code is <i>safe by default</i>. This can be achieved through mandatory conformance checks, integrated into developer workflows, that disallow risky operations by default. The most effective approach is to make this part of the language and compiler itself, as Rust does by disallowing risky operations outside <code>unsafe</code> blocks, and via the <code>#![forbid(unsafe_code)]</code> attribute, which disallows unsafe Rust throughout an entire library. Alternatively, use of risky APIs and operations can be blocked through lightweight static analysis checks.<sup>2</sup> It is helpful to deploy conformance checks as part of compilation, or mandatory pre-submit checks. This treats conformance violations like any other build failure, making the policy clear and actionable for developers.</p> 
  <p>When a conformance check fails due to an attempted use of a risky operation, the error message should point developers to corresponding safe abstractions as an alternative. This provides developer guidance more effectively than point-in-time mandatory secure-coding training: Guidance is provided in context when needed and—because of the underlying automated conformance check—can't be forgotten.</p> 
  <p>A second, related challenge is that designing and implementing abstractions that are truly safe is difficult and often requires deep domain expertise. Expert attention to risky operations and the safety of their surrounding abstractions can be ensured by layering toolchain-enforced expert reviews on top of conformance checks that forbid risky operations: Conformance checks are augmented with an exception mechanism governed by a central allowlist that is managed by the appropriate team of experts. Before risky operations can be used in a code module (which then should be a safe abstraction), that module must be added to the allowlist, which gives the domain experts the opportunity to review the code <i>before</i> it is committed to the source repository. </p> 
  <p>When reviewing abstractions for their safety, it is important to consider the entire module, not just the code immediately surrounding a risky operation (such as an <code>unsafe</code> block in Rust): Code anywhere in the module can be responsible for upholding invariants that are necessary to ensure the safety preconditions of risky operations. The allowlist mechanism can be implemented through custom tooling integrated into CI/CD (continuous integration/continuous delivery) workflows; in smaller repositories, a simple <code>grep</code>-based presubmit check can suffice.</p> 
  <p>Conformance checks and mandatory expert reviews intentionally place friction on the introduction of risky code into a codebase. For this to be sustainable, both in terms of impact on developer velocity and with respect to available expert bandwidth, it is crucial that developers almost never need to write custom, application-specific code involving risky operations. This means that the developer ecosystem and its platforms, frameworks, and standard libraries must provide a comprehensive, expert-curated set of safe abstractions that are sufficiently expressive and ergonomic to support the development of almost all application code typically encountered in a development organization. Some of these abstractions tend to be common and shareable across broad classes of applications (e.g., web or mobile apps); others might be more specifically tuned to the needs of a particular product development organization.</p> 
  <p>&nbsp;</p> 
  <h3>Formal Foundations</h3> 
  <p>The safe-coding approach, while not a formal method itself, is grounded in principles and techniques from formal software verification. Modular verification typically relies on breaking a program into components (like functions), each with a formal contract specifying its pre- and postconditions in the form of logical predicates. This allows each component to be proven correct in isolation, while abstracting away implementation details of called functions via their contract.</p> 
  <p>Safe coding adopts this modular structure but expresses function contracts solely through their type signature rather than arbitrary predicates. It elevates risky operations' safety preconditions into the type invariants of purposely designed vocabulary and wrapper types. As in formal modular verification, demonstrating that abstractions are safe and adhere to their contract involves validating that the safety preconditions of enclosed risky operations are always met and that the relevant type invariants are maintained. For practicality, however, safe coding typically relies on rigorous but informal reasoning by human domain experts rather than formal verification tools; importantly, the necessary reasoning is local to the abstraction's implementation and its dependencies. (In principle, it is possible to formalize contracts expressed as type signatures and to formally verify the safety and correctness of abstractions. This may be desirable in domains where safety reasoning is particularly challenging and subtle, such as for safe abstraction around <code>unsafe</code> Rust.<sup>18</sup>)</p> 
  <p>Safe coding allows the verification of <i>correct composition</i> of components to reduce to language-native type checking, which is both automated and highly efficient and scalable.</p> 
  <p>Safe coding is clearly not formally sound because of its partial reliance on informal reasoning, as well as limitations of the underlying language (for example, language features such as reflection that can break encapsulation). These soundness gaps, however, have been shown to have a limited effect on the assurance achieved in practice. Thus, safe coding can be viewed as a semi-formal approach that offers a favorable cost-assurance tradeoff and achieves a degree of "soundiness"<sup>16</sup> that effectively prevents common classes of erroneous behavior, safety violations, and vulnerabilities. For a detailed exploration of safe coding's formal underpinnings and soundness limitations, please see the extended version of this article.<sup>14</sup></p> 
  <p>&nbsp;</p> 
  <h3>Safe Coding in Practice</h3> 
  <p>The principles of safe coding have been applied successfully in large-scale, real-world software development environments, demonstrating both their effectiveness in drastically reducing entire classes of defects and their cost effectiveness over time.</p> 
  <p>At Google, the application of safe-coding principles has led to the near elimination of several classes of security vulnerabilities that consistently rank among the most dangerous software weaknesses.<sup>8</sup> For example, by integrating safe-coding practices into its core web application frameworks, Google has virtually eradicated XSS from hundreds of user-facing web applications. These frameworks use mechanisms such as strictly auto-escaping template systems<sup>12</sup> and the Trusted Types API,<sup>19</sup> which ensure that untrusted data cannot be inadvertently interpreted as code in a browser context. </p> 
  <p>The results have been striking: Over the past several years, hundreds of complex web applications built on these hardened frameworks have averaged less than one XSS vulnerability report per year in total. Products such as Google Photos, developed from the outset on these secure-by-design frameworks, have had no XSS vulnerabilities reported in their entire lifetime. Similarly, SQL injection vulnerabilities have been systematically addressed by redesigning database query APIs to rely on the <code>TrustedSqlString</code> type invariant. The application of safe coding to database APIs has resulted in no reported SQL injection vulnerabilities for more than a decade across the hundreds of applications using these secure database interfaces.</p> 
  <p>A cornerstone of safe coding in practice is the adoption of memory-safe languages such as Rust, Java, and Go. While memory-safety violations constitute the majority of severe security defects in code written in unsafe languages such as C/C++, they are exceedingly rare in software predominantly written in memory-safe languages.</p> 
  <p>The experience of the Android team provides a powerful case study. Following a strategic shift around 2019 to prioritize memory-safe languages for new development, Android has seen a dramatic and disproportionate reduction in memory-safety vulnerabilities. The percentage of Android's total vulnerabilities attributed to memory-safety issues fell from 76 percent in 2019 to just 24 percent in 2024. This decline occurred even while the bulk of the existing codebase remained in memory-unsafe C/C++, illustrating that it can be highly effective to focus preventive measures on areas of new and active development. This dynamic and its underlying rationale is discussed in detail in the article, <a href="https://queue.acm.org/detail.cfm?id=3773096">"A Practical Guide to Transitioning to Memory-Safe Languages"</a> in this issue of <i>acmqueue</i>. </p> 
  <p>While there is an upfront investment in developing a mature ecosystem of safe abstractions and the tooling to ensure their consistent use, this approach is highly cost effective in the long run. The cost of building and maintaining a central set of safe libraries and frameworks is amortized across hundreds of applications. </p> 
  <p>At Google, a small team of security experts maintains the safe-coding libraries and conformance checks that support thousands of application developers. This creates a powerful force multiplier, freeing application teams from the need to become security experts and allowing them to focus on product features. It also shifts from a reactive model with ongoing, per-project costs borne by security and product teams (testing, security reviews, incident response, remediation) to a proactive investment in prevention, providing continuous assurance that entire classes of vulnerabilities will not be introduced in the first place.<sup>13</sup></p> 
  <p>&nbsp;</p> 
  <h4>Acknowledgments</h4> 
  <p>The ideas presented in this article have been developed and refined over more than a decade by members of the Information Security Engineering team at Google. The framing of safe coding around safe abstractions has been influenced by the work and philosophy of the Rust community. I am grateful for the insightful feedback from reviewers of earlier drafts of this article.</p> 
  <p>&nbsp;</p> 
  <h4>References</h4> 
  <p>1. Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., Stubblefield, A. 2020. <i>Building Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining Systems</i>. O'Reilly Media.</p> 
  <p>2. Aftandilian, E., Sauciuc, R., Priya, S., Krishnan, S. 2012. Building useful program analysis tools using an extensible Java compiler. In <i>Proceedings of the IEEE 12th International Working Conference on Source Code Analysis and Manipulation</i>, 14–23; <a href="https://dl.acm.org/doi/10.1109/SCAM.2012.28">https://dl.acm.org/doi/10.1109/SCAM.2012.28</a>; <a href="https://doi.org/10.1109/SCAM.2012.28">https://doi.org/10.1109/SCAM.2012.28</a>.</p> 
  <p>3. Cardelli, L. 2004. Type systems. In <i>CRC Handbook of Computer Science and Engineering,</i> 2nd edition, ed. Allen B. Tucker; <a href="https://dl.acm.org/doi/book/10.5555/1027496">https://dl.acm.org/doi/book/10.5555/1027496</a>. </p> 
  <p>4. CWE (Common Weakness Enumeration). 2024. CWE 2024 Top 10 KEV weaknesses. The Mitre Corporation; <a href="https://cwe.mitre.org/top25/archive/2024/2024_kev_list.html">https://cwe.mitre.org/top25/archive/2024/2024_kev_list.html</a>.</p> 
  <p>5. CWE (Common Weakness Enumeration). 2024. Top 25 most dangerous software weaknesses. The MITRE Corporation; <a href="https://cwe.mitre.org/top25/">https://cwe.mitre.org/top25/</a>.</p> 
  <p>6. Dijkstra, E.W. 1970. On the reliability of programs. In <i>Edsger Wybe Dijkstra: His Life, Work, and Legacy</i>, eds. Krzysztof R. Apt and Tony Hoare. 2022, 359-370. ACM Books; <a href="https://doi.org/10.1145/3544585.3544608">https://doi.org/10.1145/3544585.3544608</a>.</p> 
  <p>7. Garavel, H., ter Beek, M. H., van de Pol, J. 2020. The 2020 expert survey on formal methods. In <i>Formal Methods for Industrial Critical Systems</i> (Lecture Notes in Computer Science 12327). Springer; <a href="https://doi.org/10.1007/978-3-030-58298-2_1">https://doi.org/10.1007/978-3-030-58298-2_1</a>.</p> 
  <p>8. Google. 2024. An overview of Google's commitment to Secure by Design. Safer with Google; <a href="https://publicpolicy.google/resources/google_commitment_secure_by_design_overview.pdf">https://publicpolicy.google/resources/google_commitment_secure_by_design_overview.pdf</a>.</p> 
  <p>9. Hoare, C. A. R. 1969. An axiomatic basis for computer programming. <i>Communications of the ACM</i> 12(10), 576–580; <a href="https://doi.org/10.1145/363235.363259">https://doi.org/10.1145/363235.363259</a>.</p> 
  <p>10. Huang, L., Ebersold, S., Kogtenkov, A., Meyer, B., Liu, Y. 2023. Lessons from formally verified deployed software systems (extended version). Arxiv2301.02206; <a href="https://doi.org/10.48550/arXiv.2301.02206">https://doi.org/10.48550/arXiv.2301.02206</a>.</p> 
  <p>11. International Organization for Standardization. 2024. ISO/IEC 9899:2024: Information technology—programming languages—C; <a href="https://www.iso.org/standard/82075.html">https://www.iso.org/standard/82075.html</a>.</p> 
  <p>12. Kern, C. 2014. Securing the tangled web. <i>Communications of the ACM</i> 57(9), 38–47. <a href="https://doi.org/10.1145/2643134">https://doi.org/10.1145/2643134</a>.</p> 
  <p>13. Kern, C. 2024. Developer ecosystems for software safety. <i>Communications of the ACM</i> 67(6), 52–60. <a href="https://doi.org/10.1145/3651621">https://doi.org/10.1145/3651621</a>.</p> 
  <p>14. Kern, C. 2025. Safe coding: rigorous modular reasoning about software safety (extended version). Google Security Engineering; <a href="https://research.google/pubs/pub1047167/">https://research.google/pubs/pub1047167/</a></p> 
  <p>15. Lipp, S., Banescu, S., Pretschner, A. 2022. An empirical study on the effectiveness of static C code analyzers for vulnerability detection. In <i>Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis</i>, 544–555; <a href="https://doi.org/10.1145/3533767.3534380">https://doi.org/10.1145/3533767.3534380</a>.</p> 
  <p>16. Livshits, B., Sridharan, M., Smaragdakis, Y., Lhoták, O., Amaral, J. N., Chang, B.-Y. E., Guyer, S. Z., Khedker, U. P., Møller, A., Vardoulakis, D. 2015. In defense of soundiness: a manifesto. <i>Communications of the ACM</i> 58(2), 44–46; <a href="https://doi.org/10.1145/2644805">https://doi.org/10.1145/2644805</a>.</p> 
  <p>17. Regehr, J. 2010. A guide to undefined behavior in C and C++. Embedded in Academia blog; <a href="https://blog.regehr.org/archives/213">https://blog.regehr.org/archives/213</a>.</p> 
  <p>18. Rust Foundation Team. 2024. Rust Foundation collaborates with AWS initiative to verify Rust standard libraries. <a href="https://rustfoundation.org/media/rust-foundation-collaborates-with-aws-initiative-to-verify-rust-standard-libraries/">https://rustfoundation.org/media/rust-foundation-collaborates-with-aws-initiative-to-verify-rust-standard-libraries/</a>.</p> 
  <p>19. Wang, P., Guðmundsson, B. A., Kotowicz, K. 2021. Adopting Trusted Types in production web frameworks to prevent DOM-based cross-site scripting: a case study. In <i>IEEE European Symposium on Security and Privacy Workshops</i>, 60–73; <a href="https://doi.org/10.1109/EuroSPW54576.2021.00013">https://doi.org/10.1109/EuroSPW54576.2021.00013</a>.</p> 
  <p>&nbsp;</p> 
  <p><b>Christoph Kern</b> is a principal software engineer in Google's Information Security Engineering organization. His primary focus is on developing scalable, principled approaches to software security.</p> 
  <p>&nbsp;</p> 
  <p>Copyright © 2025 held by owner/author. Publication rights licensed to ACM.</p>  
 <script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9c395a781a433970',t:'MTc2OTM2MDcwNS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script> 

	<p>
	
		<img class="floatLeft" src="img/q stamp_small.jpg" width="26" height="45" alt="acmqueue"><br><br>
	
	<em>Originally published in Queue vol. 23, no. 5</em>&#8212;
 	<br>
	Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3773098">ACM Digital Library</a>
	
	</p>
	





<br />
<!--
<a href="https://twitter.com/share" class="twitter-share-button" data-via="ACMQueue">Tweet</a>
-->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<br />

<!--
<fb:like></fb:like>
-->

<br />

<div class="g-plusone" data-size="small" data-annotation="inline" data-width="120"></div>

<!-- these get hooked up to js events -->
<script type="text/javascript">
	addthis_pub             = 'acm';
	addthis_logo            = 'http://queue.acm.org/img/logo_queue_small.gif';
	addthis_logo_background = '#ffffff';
	addthis_logo_color      = '000000';
	addthis_brand           = 'ACM Queue';
	addthis_options         = 'reddit, slashdot, facebook, favorites, email, delicious, digg, technorati, blinklist, furl, myspace, google, live, more';
</script>

<!-- FB Like -->
<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<div id="fb-root"></div>
-->

<!-- Place this tag after the last +1 button tag. -->

<!--
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>

<br />
<script src="https://connect.facebook.net/en_US/all.js#xfbml=1"></script>

<script>
FB.Event.subscribe('edge.create', function(targetUrl) {
  _gaq.push(['_trackSocial', 'facebook', 'like', targetUrl]);
});
</script>
-->



<hr noshade size=1 />




More related articles:

	  <p>
	  <span>Russ Cox</span> - <a href="detail.cfm?id=3786614"><b>Running the &quot;Reflections on Trusting Trust&quot; Compiler</b></a>
	  <br />
	  In October 1983, Dennis Ritchie and Ken Thompson received the Turing Award for their work on Unix. Thompson's lecture, reprinted in Communications of the ACM under the title "Reflections on Trusting Trust," explained in three steps how to modify a C compiler binary to insert a backdoor when compiling a target program, leaving no trace in any source code. This article revisits that backdoored compiler, presenting the original code Thompson wrote more than 50 years ago. First, a brief review of Thompson's three steps.
	  </p>
	  <br />

	  <p>
	  <span>Jeff Vander Stoep, Alex Rebert, Lars Bergstrom</span> - <a href="detail.cfm?id=3773096"><b>A Practical Guide to Transitioning to Memory-Safe Languages</b></a>
	  <br />
	  Traditional approaches to memory safety have often amounted to best-effort defect discovery after the fact, and sometimes more advanced strategies focused on threat modeling: identifying critical code, applying interventions, and repeating the cycle as the codebase evolves. While this approach is a valuable part of a defense-in-depth strategy, it is fundamentally flawed as a primary strategy. It traps teams in a reactive and never-ending cycle of treating symptoms with solutions empirically shown to be insufficiently complete without ever addressing the underlying cause.
	  </p>
	  <br />

	  <p>
	  <span>Louis Dionne, Alex Rebert, Max Shavrick, Konstantin Varlamov</span> - <a href="detail.cfm?id=3773097"><b>Practical Security in Production</b></a>
	  <br />
	  The challenge of improving the memory safety of the vast landscape of existing C++ code demands pragmatic solutions. Standard library hardening represents a powerful and practical approach, directly addressing common sources of spatial safety vulnerabilities within the foundational components used by nearly all C++ developers. Our collective experience at Apple and Google demonstrates that significant safety gains are achievable with surprisingly minimal performance overhead in production environments. This is made possible by a combination of careful library design, modern compiler technology, and profile-guided optimization.
	  </p>
	  <br />

	  <p>
	  <span>Andrew Lilley Brinker</span> - <a href="detail.cfm?id=3773095"><b>Memory Safety for Skeptics</b></a>
	  <br />
	  The state of possibility with memory safety today is similar to the state of automobile safety just prior to the widespread adoption of mandatory seat-belt laws. As car manufacturers began to integrate seat belts as a standard feature across their model lines and states began to require that drivers wear seat belts while driving, the rate of traffic fatalities and severity of traffic-related injuries dropped drastically. Seat belts did not solve automobile safety, but they credibly improved it, and at remarkably low cost.
	  </p>
	  <br />


<hr noshade size=1 />





<hr noshade size=1 />

	<p>
	<a href='#'><img src='https://queue.acm.org/img/logo_acm.gif' /></a>
	<br />
	&copy; ACM, Inc. All Rights Reserved.
	</p>

</div>



<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9c395a74bae293b7',t:'MTc2OTM2MDcwNS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
</html>