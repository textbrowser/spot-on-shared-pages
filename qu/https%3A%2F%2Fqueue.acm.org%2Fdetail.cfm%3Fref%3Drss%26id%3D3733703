<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">



<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-20JYM3ZFN0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-20JYM3ZFN0');
</script>



	  
	  <title>Develop, Deploy, Operate - ACM Queue</title>

	  

	  <meta name='description' value='' />
	  <meta name='keywords' value='Business/Management' />

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P52H78L');</script>
<!-- End Google Tag Manager -->

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" href="favicon.ico" />

<script type="text/javascript" src="/js/jquery-1.2.6.min.js"></script>
<script type="text/javascript" src="/js/jquery.validate.min.js"></script>
<script type="text/javascript" src="/js/global.js"></script>



<!--
<link rel="alternate" type="application/rss+xml" title="Latest Queue Content RSS 2.0" href="/rss/feeds/latestitems.xml" />
-->
<link rel="alternate" type="application/rss+xml" title="All Queue Content RSS 2.0" href="/rss/feeds/queuecontent.xml" />
<link rel="alternate" type="application/rss+xml" title="Curmudgeon RSS 2.0"     href="/rss/feeds/curmudgeon.xml" />
<link rel="alternate" type="application/rss+xml" title="Opinion RSS 2.0"        href="/rss/feeds/opinion.xml" />
<link rel="alternate" type="application/rss+xml" title="Kode Vicious RSS 2.0"   href="/rss/feeds/kodevicious.xml" />
<link rel="alternate" type="application/rss+xml" title="ACM TechNews RSS"       href="https://www.infoinc.com/acm/TechNews.rss" />
<link rel="alternate" type="application/rss+xml" title="Washington Updates RSS" href="https://usacm.acm.org/weblog2/?feed=rss2" />
<link rel="alternate" type="application/rss+xml" title="RISKS Forum RSS"        href="/rss/feeds/risksforum.xml" />


<link rel="alternate" type="application/rss+xml" title="AI RSS 2.0"        href="/rss/feeds/ai.xml" />

<link rel="alternate" type="application/rss+xml" title="API Design RSS 2.0"        href="/rss/feeds/apidesign.xml" />

<link rel="alternate" type="application/rss+xml" title="Bioscience RSS 2.0"        href="/rss/feeds/bioscience.xml" />

<link rel="alternate" type="application/rss+xml" title="Blockchain RSS 2.0"        href="/rss/feeds/blockchain.xml" />

<link rel="alternate" type="application/rss+xml" title="Business/Management RSS 2.0"        href="/rss/feeds/business/management.xml" />

<link rel="alternate" type="application/rss+xml" title="Compliance RSS 2.0"        href="/rss/feeds/compliance.xml" />

<link rel="alternate" type="application/rss+xml" title="Component Technologies RSS 2.0"        href="/rss/feeds/componenttechnologies.xml" />

<link rel="alternate" type="application/rss+xml" title="Computer Architecture RSS 2.0"        href="/rss/feeds/computerarchitecture.xml" />

<link rel="alternate" type="application/rss+xml" title="Concurrency RSS 2.0"        href="/rss/feeds/concurrency.xml" />

<link rel="alternate" type="application/rss+xml" title="Cryptocurrency RSS 2.0"        href="/rss/feeds/cryptocurrency.xml" />

<link rel="alternate" type="application/rss+xml" title="DSPs RSS 2.0"        href="/rss/feeds/dsps.xml" />

<link rel="alternate" type="application/rss+xml" title="Data RSS 2.0"        href="/rss/feeds/data.xml" />

<link rel="alternate" type="application/rss+xml" title="Databases RSS 2.0"        href="/rss/feeds/databases.xml" />

<link rel="alternate" type="application/rss+xml" title="Debugging RSS 2.0"        href="/rss/feeds/debugging.xml" />

<link rel="alternate" type="application/rss+xml" title="Development RSS 2.0"        href="/rss/feeds/development.xml" />

<link rel="alternate" type="application/rss+xml" title="Distributed Computing RSS 2.0"        href="/rss/feeds/distributedcomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="Distributed Development RSS 2.0"        href="/rss/feeds/distributeddevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Education RSS 2.0"        href="/rss/feeds/education.xml" />

<link rel="alternate" type="application/rss+xml" title="Email and IM RSS 2.0"        href="/rss/feeds/emailandim.xml" />

<link rel="alternate" type="application/rss+xml" title="Embedded Systems RSS 2.0"        href="/rss/feeds/embeddedsystems.xml" />

<link rel="alternate" type="application/rss+xml" title="Failure and Recovery RSS 2.0"        href="/rss/feeds/failureandrecovery.xml" />

<link rel="alternate" type="application/rss+xml" title="File Systems and Storage RSS 2.0"        href="/rss/feeds/filesystemsandstorage.xml" />

<link rel="alternate" type="application/rss+xml" title="Game Development RSS 2.0"        href="/rss/feeds/gamedevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Graphics RSS 2.0"        href="/rss/feeds/graphics.xml" />

<link rel="alternate" type="application/rss+xml" title="HCI RSS 2.0"        href="/rss/feeds/hci.xml" />

<link rel="alternate" type="application/rss+xml" title="Managing Megaservices RSS 2.0"        href="/rss/feeds/managingmegaservices.xml" />

<link rel="alternate" type="application/rss+xml" title="Mobile Computing RSS 2.0"        href="/rss/feeds/mobilecomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="Networks RSS 2.0"        href="/rss/feeds/networks.xml" />

<link rel="alternate" type="application/rss+xml" title="Object-Relational Mapping RSS 2.0"        href="/rss/feeds/object-relationalmapping.xml" />

<link rel="alternate" type="application/rss+xml" title="Open Source RSS 2.0"        href="/rss/feeds/opensource.xml" />

<link rel="alternate" type="application/rss+xml" title="Patching and Deployment RSS 2.0"        href="/rss/feeds/patchinganddeployment.xml" />

<link rel="alternate" type="application/rss+xml" title="Performance RSS 2.0"        href="/rss/feeds/performance.xml" />

<link rel="alternate" type="application/rss+xml" title="Power Management RSS 2.0"        href="/rss/feeds/powermanagement.xml" />

<link rel="alternate" type="application/rss+xml" title="Privacy and Rights RSS 2.0"        href="/rss/feeds/privacyandrights.xml" />

<link rel="alternate" type="application/rss+xml" title="Processors RSS 2.0"        href="/rss/feeds/processors.xml" />

<link rel="alternate" type="application/rss+xml" title="Programming Languages RSS 2.0"        href="/rss/feeds/programminglanguages.xml" />

<link rel="alternate" type="application/rss+xml" title="Purpose-built Systems RSS 2.0"        href="/rss/feeds/purpose-builtsystems.xml" />

<link rel="alternate" type="application/rss+xml" title="Quality Assurance RSS 2.0"        href="/rss/feeds/qualityassurance.xml" />

<link rel="alternate" type="application/rss+xml" title="RFID RSS 2.0"        href="/rss/feeds/rfid.xml" />

<link rel="alternate" type="application/rss+xml" title="SIP RSS 2.0"        href="/rss/feeds/sip.xml" />

<link rel="alternate" type="application/rss+xml" title="Search Engines RSS 2.0"        href="/rss/feeds/searchengines.xml" />

<link rel="alternate" type="application/rss+xml" title="Security RSS 2.0"        href="/rss/feeds/security.xml" />

<link rel="alternate" type="application/rss+xml" title="Semi-structured Data RSS 2.0"        href="/rss/feeds/semi-structureddata.xml" />

<link rel="alternate" type="application/rss+xml" title="Social Computing RSS 2.0"        href="/rss/feeds/socialcomputing.xml" />

<link rel="alternate" type="application/rss+xml" title="System Administration RSS 2.0"        href="/rss/feeds/systemadministration.xml" />

<link rel="alternate" type="application/rss+xml" title="System Evolution RSS 2.0"        href="/rss/feeds/systemevolution.xml" />

<link rel="alternate" type="application/rss+xml" title="Testing RSS 2.0"        href="/rss/feeds/testing.xml" />

<link rel="alternate" type="application/rss+xml" title="Virtual Machines RSS 2.0"        href="/rss/feeds/virtualmachines.xml" />

<link rel="alternate" type="application/rss+xml" title="Virtualization RSS 2.0"        href="/rss/feeds/virtualization.xml" />

<link rel="alternate" type="application/rss+xml" title="Visualization RSS 2.0"        href="/rss/feeds/visualization.xml" />

<link rel="alternate" type="application/rss+xml" title="VoIP RSS 2.0"        href="/rss/feeds/voip.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Development RSS 2.0"        href="/rss/feeds/webdevelopment.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Security RSS 2.0"        href="/rss/feeds/websecurity.xml" />

<link rel="alternate" type="application/rss+xml" title="Web Services RSS 2.0"        href="/rss/feeds/webservices.xml" />

<link rel="alternate" type="application/rss+xml" title="Workflow Systems RSS 2.0"        href="/rss/feeds/workflowsystems.xml" />

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-6562869-1']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<script type="text/javascript">
function plusone_vote( obj ) {
_gaq.push(['_trackEvent','plusone',obj.state]);
}
</script>



<style>
body {
	font-family: jaf-bernino-sans, 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans', Geneva, Verdana, sans-serif;
	color: #333;
	max-width: 100%;
}
div.container p {
	line-height: 1.65em;
}
h1 {
	font-size: 32px;
}
h3 {
	font-size: 18px;
}
h4 {
	font-size: 14px;
}

div.container {
	margin-left: auto;
	margin-right: auto;
}

div {
	margin: 64px;
//	max-width: 800px;
	position: relative;
}

@media only screen and (min-width: 1024px) {
	div {
		max-width: 800px;
	}
}

img {
    max-width: 100%;
    height: auto;
    width: auto\9; /* ie8 */
}
a {
	color: #009;
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
hr {
	margin:64px;
}
label {
	font-size: 0.8em;
	color: #666;
}
input {
	color: #999;
}

/* NAVBAR */
.navbar {
//	position: fixed;
	background: #EEEEEE;
	top: -64px;
	z-index: 10000;
	width: 100%;
	clear: both;
	padding: 0px;
	margin: 0px;
	padding-top: 10px;
	padding-left: 10px;
	padding-right: 10px;
}

/*  SECTIONS  */
.section {
	clear: both;
	padding: 0px;
	margin: 0px;
}

/*  COLUMN SETUP  */
.col {
	display: block;
	float:left;
	margin: 1% 0 1% 1.6%;
}
.col:first-child { margin-left: 0; }


/*  GROUPING  */
.group:before,
.group:after {
	content:"";
	display:table;
}
.group:after {
	clear:both;
}
.group {
    zoom:1; /* For IE 6/7 */
}

/*  GRID OF THREE  */
.span_3_of_3 {
	width: 100%;
}
.span_2_of_3 {
	width: 66.1%;
}
.span_1_of_3 {
	width: 32.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.col {
		margin: 1% 0 1% 0%;
	}
}

@media only screen and (max-width: 480px) {
	.span_3_of_3 {
		width: 100%;
	}
	.span_2_of_3 {
		width: 100%;
	}
	.span_1_of_3 {
		width: 100%;
	}
}

.span_2_of_2 {
	width: 100%;
}

.span_1_of_2 {
	width: 49.2%;
}

/*  GO FULL WIDTH AT LESS THAN 480 PIXELS */

@media only screen and (max-width: 480px) {
	.span_2_of_2 {
		width: 100%;
	}
	.span_1_of_2 {
		width: 100%;
	}
}
</style>


<style>
body {
	font-size: 19px;
}
#form-search > .st-default-search-input {
	width: 170px;
  display: inline-block;
  height: 16px;
  padding: 7px 11px 7px 28px;
  border: 1px solid #bbb;
  border: 1px solid rgba(0,0,0,0.25);
  font-weight: 400;
  color: #3B454F;
  font-size: 14px;
  line-height: 16px;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-box-shadow: none;
  -moz-box-shadow: none;
  box-shadow: none;
  font-family: system, -apple-system, BlinkMacSystemFont, "Helvetica Neue", "Lucida Grande", sans-serif;
}


blockquote
{
    color: #666;
    font-size: 1.1em;
    background: none;
    border-left: .2rem solid #d3d3d3;

    display: block;
    padding: 20px 20px 10px 45px;
    margin: 20px 0;
    font-style: italic;

    margin-block-start: 1em;
    margin-block-end: 1em;
    margin-inline-start: 40px;
    margin-inline-end: 40px;

	font-family: Georgia, Palatino, "Palatino Linotype", Times, "Times New Roman", serif;
}

.ldq {
	display: block;
    padding-left: 10px;
    content: "\201C";
    font-size: 60px;
    position: relative;
    left: -50px;
    top: 0;
    height: 0;
    color: #7a7a7a;
}
code {
//	font-size:1.25em;
}
a {overflow-wrap: break-word;}
pre {
	overflow-x: auto;
	white-space: pre-wrap;
	word-wrap: break-word;
}
</style>
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
<!-- Google Tag Manager (noscript)
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P52H78L"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
 End Google Tag Manager (noscript) -->



<div class=container>
	<div class="navbar">
		<form id="form-search" name="searchform" onsubmit="return false;" style='float:right;'>
				<input type="text" class="st-default-search-input">
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','UyYECD1kdsPnbHJtPyzG','2.0.0');
</script>
				<br />
			
			<a href="issuedetail.cfm?issue=3735580" style='width:150px;font-size:0.7em;'>Current Issue</a> &nbsp; <a href="pastissues.cfm" style='width:150px;font-size:0.7em;'>Past Issues</a> &nbsp; <a href="topics.cfm" style='width:150px;font-size:0.7em;'>Topics</a>
			
		</form>
		<a href='/'><img src='https://queue.acm.org/img/acmqueue_logo.gif' /></a>

	</div>

<!--
<p style='text-align:center;'>
<a href='/app/' target='_new'><img src='/app/2021_03-04_lrg.png' with=90 height=120 style='float:right;width:90px;height:120px;' alt='March/April 2021 issue of acmqueue' /></a>
<b><a href='/app/'>The March/April 2021 issue of acmqueue is out now</a></b>
<br />
<br />
<a href='https://cdn.coverstand.com/3rd_pty/acm/login.html?&btx_i=705849'>Subscribers and ACM Professional members login here</a>
<br clear=all />
<hr style='display:block;color:red;margin:5px;' />
</p>
-->
<br />



<label>May 22, 2025<br /><b><a class="descriptor" href="issuedetail.cfm?issue=3735580">Volume 23, issue 2 </a></b></label>


<p>
<!-- // Check for existence of associated MP3 file-->

 &nbsp;
	
			<a href="https://portal.acm.org/citation.cfm?id=3733703">
				<img src="img/icon_pdf.png" alt="Download PDF version of this article" />
				PDF
			</a>
		
</p>


 
  <h1 class="hidetitle">Develop, Deploy, Operate</h1> 
  <h2>A holistic model for understanding the costs and value of software development</h2> 
  <h3>Titus Winters, Leah Rivers, and Salim Virji</h3> 
  <p>How much time and money should organizations invest in developer tools? What is the business value of improvements to a development platform? How do companies quantify the business impact of optimizing an application's CPU utilization? What tests should run during development versus integration? Despite the ubiquity of commercial software and the accumulated wisdom of industry, open source, and academia, it's still hard to answer these basic questions about commercial software development.</p> 
  <p>For companies to be sustainable in the long run, they must balance the production cost of software development with the value of that software, and they must account for risks. Software value is created by product success, developer productivity, hardware resource efficiency, innovation, and risk reduction. These factors affect one another in both predictable and unexpected ways. Some factors have a precise dollar value (salaries, hardware), whereas the dollar value of other factors is hard to estimate precisely in the short term. </p> 
  <p>This article presents a holistic model for understanding the costs and commercial value of software development. It places software development in the context of business goals, provides insights about software development, and offers a shared vocabulary for use across product teams, functions, and management layers. Then it proposes a model of commercial software-development workflow, with four types of observable impact. The article closes with a discussion of the indirect impact of software infrastructure and architecture on the development workflow. </p> 
  <p>&nbsp;</p> 
  <h3>Software Development and Business Goals</h3> 
  <p>When a business invests in the software-development process, as distinct from the direct development of a specific software product or feature, its goal is to maximize business value with existing resources—which is to say, to reach an acceptable quality bar with sustainable costs. </p> 
  <p>Software has value when it is used. Software development is an ongoing process of tradeoffs among critical resources, especially people, hardware, strategic capabilities, and time. While software development workflows vary, the delivery of commercial software value generally requires three distinct processes: development, deployment, and operation (figure 1). These three processes are interdependent; changes to one can have effects across the system. Each process has a separate goal, which changes the cost-benefit analysis in critical resource tradeoffs.</p> 
  <img src="https://dl.acm.org/cms/attachment/html/10.1145/3733703/assets/html/rivers1.png" alt="FIGURE 1: Development, deployment, and operation" /> 
  <p>Why does this matter? In contrast to highly simplified aggregate views, a holistic view of software development accounts for the different needs and goals of each of the three processes. To maximize benefit and minimize waste, companies should look at how resources are being used across all processes. Viewing hardware capacity in isolation may delay release or reduce quality. For example, running fewer tests saves on hardware but resulting quality issues may result in real losses . Framing tradeoffs by process and giving costs and benefits in terms of all three processes can help decision-makers make more effective choices. </p> 
  <p>&nbsp;</p> 
  <h3>Insights </h3> 
  <p>This section lists a number of non-obvious but generally non-contentious points to provide a foundation for the ideas that follow.</p> 
  <p>&nbsp;</p> 
  <h4>Software has product value when it is used</h4> 
  <p>Software provides commercial <a href="https://ronjeffries.com/articles/019-01ff/value-nsd/"><i>value</i></a> only when it is being used (i.e., released or deployed). As in manufacturing, a buildup of work in progress is a common trap of value — if you've built a new feature or fixed a bug without deploying the feature or bugfix, you've spent time (money) for no benefit. An important corollary: If part of a software process adds too much latency, it delays or prevents the realization of value. At the same time, high process latency also adds strategic risk: When (not if) substantial bugs arise in deployed software, swift responses are more valuable than slower ones.</p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Software Value Creation Processes</i></h4> 
  <p>Each process in software development has a different calculation for tradeoffs between critical resources (figure 2).</p> 
  <img src="https://dl.acm.org/cms/attachment/html/10.1145/3733703/assets/html/rivers2.png" alt="FIGURE 2: Different calculation for tradeoffs between resources" /> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Development Is A Creative Process</i></h4> 
  <p>One of the great unsolved problems for software companies is the inability to directly measure productivity for software development or demonstrate the value of sharing knowledge. What would the units of such a measurement be? We've known since <a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html">Dijkstra that it isn't lines of code</a>. The number of proposed or submitted changes at best indicates activity, not outcomes. We aspire to measure "feature count" or "feature velocity," but in practice, that is still difficult, except in large aggregates. (What is the exact definition of a feature? How do we know if it was the right feature to implement?)</p> 
  <p>The difficulty is that every software change is unique, and it is rare for two features to be so similar that they can be directly compared. Software development involves creative, thoughtful human judgment and skills, including design, discovery, craft, and deep understanding of existing systems. The industry is founded on knowledge and creativity. In the development phase of the workflow, human creativity and innovation are paramount. Everything in the software workflow, up to the point of submitting a change, is fundamentally a human (creative) process, expressing human intent.<sup> </sup>As <a href="https://en.wikipedia.org/wiki/Mary_Shaw_(computer_scientist)">Mary Shaw</a> has been saying for 30-plus years, "Writing software should map to designing the products, not producing them." While business-level aggregate data can provide useful insights, statistics at the individual developer level are insufficient proxies for productivity. </p> 
  <p>The <a href="https://queue.acm.org/detail.cfm?id=3454124">SPACE paper</a> is a reminder that, while aggregates and proxies can guide the questions to be asked and identify levers of change in software systems, these proxies should be used in a larger constellation of metrics, preferably metrics in tension with one another. (This article does not attempt to define an appropriate collection of productivity metrics.) Even then, those metrics can rarely define and predict costs in such a way that those costs could be weighed against business revenue.</p> 
  <p>If the development process is seen as involving design, creation, and expression of intent, it's easier to target investments in productivity. In automotive design, investments would be made in design standards, common components, modeling tools, detailed designs for mechanical, electrical, and hydraulic systems, and so on. Only after the creative development phase would the new prototype go into factory production.</p> 
  <p>Software development is no different. First, we collaborate and figure out <i>what </i>to build. Then we decide <i>how</i> to build it. Finally, we build the software and release it to production. As in automotive design, it's critical to have time to prototype, experiment, and iterate on designs. At this stage, reusable components, higher-level tools, and better holistic design and understanding are key. </p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Deployment is a Factory Process</i></h4> 
  <p>In contrast, after changes are submitted, there is no fundamental reason for humans to be involved in software deployment. The deployment process involves the evaluation of tests and production signals to determine whether the current build, release candidate, or configuration is value-positive. These tasks can be handled by machines, which are better at signal processing than humans. Setting aside reliability engineers who are responsible for business-wide deployment capabilities and fixing outages and complex failures—and will always be needed—if we involve humans in the routine parts of software deployment, we have failed at automation, failed to generate necessary signals, or both.</p> 
  <p>Routine deployment processes that require human involvement are highly likely to be human-unfriendly toil. The need for human oversight and involvement will never be zero, but value is created by reducing both the practical and theoretical involvement of humans in build configuration, integration testing, release, and production reliability processes. </p> 
  <p><a href="https://dora.dev/guides/dora-metrics-four-keys/">DORA</a><sup> </sup>metrics (see recent publications such as the <a href="https://cloud.google.com/devops/state-of-devops">State of DevOps</a> reports or the older-but-formative book, <a href="https://www.amazon.com/Accelerate-Software-Performing-Technology-Organizations/dp/1942788339">Accelerate</a>) such as time "from commit to deploy" implicitly recognize this fact: Commit-to-deploy times can be measured and compared across teams because—at root—every release/deploy action can be mechanized.</p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Operational Processes</i></h4> 
  <p>Unlike development and deployment, the operation of software in production optimizes for the unexpected. </p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Deployments to Production:<br />Sooner is Almost Always Better</i></h4> 
  <p>The goals for most commercial software projects are similar, stable, and worth repeating: Deliver as much value in software as possible per unit of time, at a given quality level. This alone justifies the industry focus on CD (continuous deployment). For example, consider two teams that are capable of producing the same amount of valuable change per quarter. One team's workflow and release process allow one release per quarter. The other team's workflow and release process allow one release per day. Because value is cumulative over time, although both teams deliver the same value in total work product, the team with daily releases yields more aggregate value. There is as much as a 50 percent increase in the integral of deployed value over time, purely from improving release cadence.</p> 
  <p>In reality, that evaluation is more complicated:</p> 
  <p>• Some fraction of changes will fail in production, especially experimental changes in product design. A change may take multiple releases to become valuable, especially product-fit or UX (user experience) changes. This need for iteration is a strong secondary reason to increase release cadence.</p> 
  <p>• The release process has overhead costs. Every step of deployment has overhead costs for compute. A lack of good, cheap automation will slow the release cadence. As anyone who has had to wait for a phone to update and reboot knows, release processes also cost the end user.</p> 
  <p>By and large, however, these details are of lesser magnitude than the value derived from shipping more changes earlier.</p> 
  <p>&nbsp;</p> 
  <h4>Qualitative properties of software systems</h4> 
  <p>Software products are a combination of features and QAs (quality attributes), which refer to nonfunctional or extra-functional requirements. While functionality is local, meaning that you can point to the part of the code implementing a feature such as sign-in, QAs tend to be diffuse and harder to isolate, having to do with the interactions, dependencies, behaviors, and attributes of the system as a whole. QAs are often in tension with one another and with cost. Without providing a taxonomy or framework for QAs, the following covers some key insights.</p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Fixed Costs Are Not The Same as Losses</i></h4> 
  <p>Although both affect the bottom line, the cost to produce software is distinct from the loss of value (e.g., loss of revenue, reduction in customer satisfaction, brand reputation) caused by defects manifesting in production. Bugs that reach users can be serious and severe. Even if you will never achieve perfection (see <a href="https://sre.google/sre-book/embracing-risk/">error budgets</a>), you should aim to be close enough for most users not to notice. When you fail at quality, the losses are real and unpredictable.</p> 
  <p>By contrast, especially over short timeframes like quarters, the cost of both humans and computers is fixed: You can reasonably predict pay for engineers and the total cost of cloud or other infrastructure to deliver products. Secondary costs include the detection and repair of defects naturally introduced during development. This detection and remediation consumes some of the capacity to produce new software but is necessary to hit quality targets. Similarly, missed optimizations and wasted compute reduce available capacity in hardware resources, but the total cost of running the codebase over that timeframe is fixed.</p> 
  <p>In other words, a defect caught as part of the development process is regrettable but natural. It doesn't change the bottom line, apart from adding latency to the deployment of valuable software changes. By contrast, a defect caught after release poses a risk and, in many cases, causes a genuine impact on the bottom line. In this case, the "cost" is not so much that of a fix or a rollback, but a loss in reputation and perceived reliability. The value in quickly and automatically releasing software must be balanced against the risk of releasing software with too many defects.</p> 
  <p>It is important to account for defects that reach production separately from defects remediated internally as part of the development process. </p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Outage Reduction Isn't (Usually) A Good Impact Measure</i></h4> 
  <p>The total number of outages can be useful as an overall proxy of total software process reliability, but evaluating the impact of a particular tool or software process change in terms of outage reduction rarely yields statistically significant results. Thus, an organization could think tools and processes have no impact, that software quality is immaterial, or that another mechanism is needed to characterize impact. The proof of impact is in the relationship between team-level results and organizational performance: teams with better DORA metrics predict better organizational performance. That mechanism is presented in the next section.</p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Most Counterfactuals Can't Be Measured</i></h4> 
  <p>To measure an outcome against a counterfactual (an outcome that didn't happen) requires both a clearly identified baseline measurement and a well-understood divergence as the direct result of a defect or intervention. If there is no graph of the outcome over time showing a dip or spike correlated to the event in question, the outcome can't be measured against the counterfactual.</p> 
  <p>Since there isn't a singular measure of engineering productivity, nor an as-yet-agreed-upon index metric, this means that only very large aggregate proxy metrics are indicative of productivity, and only events that impact the aggregate population are potentially measurable. Direct measurement of the effect of a new tool or process on individual or small-group productivity is generally infeasible.</p> 
  <p>&nbsp;</p> 
  <h4 align="center" style="text-align:center"><i>Defect Cost and Test Fidelity are in Tension</i></h4> 
  <p>Testing software in production provides the only high-fidelity evidence of its value but risks substantial losses. Defects that reach production are costly, so lower-fidelity tests that can be implemented earlier are preferable to high-fidelity, high-cost tests in production. The earlier in the process that the detection of defects can be shifted, the less it will cost to fix them. </p> 
  <p>From a testing perspective, earlier phases of software development can serve as proxies for later phases. Development unit tests are a proxy for integration tests. Integration tests are a proxy for release qualification. Release qualification is a proxy for canary analysis. The further along a process, the higher the fidelity of the product-quality signal and the higher the capacity loss of a defect. Consider how much harder it is to root-cause a defect identified just before release, how much reengineering that may trigger, the communication costs, the need to produce a new release candidate, etc.</p> 
  <p>&nbsp;</p> 
  <h3>Evaluating Impact</h3> 
  <p>We suggest four forms of impact, three of which are quantitative. Specifically, considering the points made in the previous section, Fixed Costs Are Not the Same as Losses, there are three fundamental forms of measurable impact for a software organization: product success, hardware resource efficiency, and engineering capacity. Strategic capabilities are a fourth, qualitative factor worth considering.</p> 
  <p>&nbsp;</p> 
  <h4>Product success</h4> 
  <p>Is the product successful? This is the form of impact that matters most in commercial software development. The exact definition of success will differ from company to company and to some extent from product to product, but it often consists of some combination of revenue, reach, adoption, user trust, and customer satisfaction. It increases both when more value is deployed and when the rate of bugs and outages is reduced.</p> 
  <p>&nbsp;</p> 
  <h4>Hardware resource efficiency</h4> 
  <p>Are production hardware/cloud resources being used efficiently? This is a production-adjacent and highly measurable form of impact. Codebase optimization and compiler optimization work can be extraordinarily effective in this space. Efficiency improvements are visible in two similar but distinct areas:</p> 
  <p>• Customer-facing resource consumption, or how much compute is needed to provide the features shipped to customers.</p> 
  <p>• The resource consumption of internal software processes, or how much compute is spent to produce a novel change or evaluate a potential release. </p> 
  <p>For customer-facing resource consumption, efficiency gains are either value-neutral, in the case of pure optimization, or follow from customer input about product requirements and product success, in which case, they are positive.</p> 
  <p>For engineer-facing resource consumption, efficiency gains involve nuanced tradeoffs between developer time and potential reductions in hardware/cloud usage. Some efficiency problems are worth investing developer time in, given savings over time, and some are not.</p> 
  <p>&nbsp;</p> 
  <h4>Engineering capacity</h4> 
  <p>Are human resources being used effectively? Although this is challenging to measure in general, two insights can reduce the need for precise measurements of improved engineering capacity.</p> 
  <p>First, as discussed earlier in this article (Deployment is a Factory Process), nearly all human involvement in deployment processes is theoretically unnecessary overhead. DORA categorizes this involvement as "deployment pain." If a company can hold product success outcomes stable while reducing engineering involvement in the testing, deployment, and release process, that reduction in human toil is valuable. Reducing human involvement in deployment processes is a clear gain and often provides a higher-leverage ROI (return on investment) than additional hiring.</p> 
  <p>Second, because as stated earlier (Most Counterfactuals Can't be Measured), measuring improvement against avoided outcomes is likely impossible. There are at least two ways of measuring effects on capacity, however: You can measure the delta of an intervention (against the counterfactual, not usually possible); or you can estimate the capacity usage (cost) of the current process and look for aggregate cost reductions. The former asks: "How much did you save by acting now?" The latter asks: "Can you try X to make that wasteful process more efficient?"</p> 
  <p>When you ask about the value of preventing a specific defect at a specific stage, you attempt to measure against a counterfactual: Would that defect have made it to production and caused a loss, rather than a capacity cost? If it was caught by the workflow, which later phase(s) of the workflow would also have caught it? Over the probability distribution of detection chances for the rest of the workflow, what is the expected overall capacity cost for detection and remediation? This is nightmarishly complex to evaluate, especially in a defect-by-defect analysis; problems like this illustrate the difficulty in quantifying and categorizing the impacts of workflow and tooling improvements.</p> 
  <p>Instead, we suggest a consensus estimation function for DDR (defect detection and resolution). The cost of DDR scales linearly with both the number of engineers exposed to the defect and the time since introduction. This means that tools or process changes that diagnose a class of bugs earlier in the workflow are more valuable than those that do so later—for example, autoformatting and autofixing whitespace (i.e., tabs versus spaces, trailing spaces) issues inside the IDE (integrated development environment), a point-in-time intervention that addresses the defect at the earliest possible point. Only the engineer generating the change is exposed to the issue. Without this intervention, whitespace "defects" may not be detected until several minutes after introduction or as late as integration, hours or days after introduction. </p> 
  <p>With in-IDE autoformatting, the persistence of an undetected defect drops to seconds or minutes. Changing the DDR period for a class of bugs from hours to minutes is valuable. That investment, however, should be made only if the problem comes up often enough to warrant the cost of that change. Similarly, if the cost of fixing a class of problems late in the workflow is incommensurate with the value of that fix (e.g., blocking a release because of whitespace issues), those issues should be ignored.</p> 
  <p>Given this rough DDR estimate and remembering that human involvement in deployment should be minimal, we observe that engineering capacity can be improved either by reducing toil in deployment and integration while keeping DDR stable or by reducing the estimated aggregate cost of an individual defect or class of defects.</p> 
  <p>&nbsp;</p> 
  <h4>Strategic capabilities</h4> 
  <p>New strategic capability is an important but not quantitative fourth form of impact. The impact of being able to perform a task that was previously impossible or so inefficient as to be impractical or irrelevant can't be quantified in the same way as product success, hardware resource efficiency, or engineering capacity. Similarly, there is strategic value in being capable of providing good information to decision-makers, even if that information is used rarely: For example, telemetry that is used once a quarter may still create massive leverage. </p> 
  <p>The ability of AI to aggregate and summarize information is one such strategic capability that can support human decision-making. From the introduction of search engines in the 1990s onward, productivity has leaped from simply having accessible information in a usable format (i.e., aggregated and summarized rather than raw and needing human processing). Another example of strategic capability is a flame graph for usage by a function call, which helps developers decide where and how to optimize performance.</p> 
  <p>Strategic capabilities may include long-term or experimental investments in fundamentally shifting how a business operates: for example, by declaring a new domain a strategic priority. These shifts often have long lead times and uncertainty. Teams operating in experimental domains benefit from a principled investment approach to investing supported by executive leadership, including a thoughtful approach to performance reviews and promotion. This form of impact should be a bounded part of an overall understanding of impact in an infrastructure investment portfolio.</p> 
  <p>&nbsp;</p> 
  <h4>Tradeoffs</h4> 
  <p>Improvements to one of the following areas are likely to affect the two other workflow areas (table 1).</p> 
  <img src="https://dl.acm.org/cms/attachment/html/10.1145/3733703/assets/html/rivers-t1.png" alt="TABLE 1: Improvements to one area affects the others" /> 
  <p>&nbsp;</p> 
  <h4>The role of infrastructure and platform engineering</h4> 
  <p>Infrastructure and platform-engineering teams provide capacity and strategic impact and indirectly affect product-success metrics. While it is up to the product-development teams to build the right features for the products, infrastructure can accelerate their engineering work and provide quality assurance. Organizations that provide central software capabilities and infrastructure create impact in these ways:</p> 
  <p>• <b><i>Engineering capacity. </i></b>Can we reduce edit-build-test cycles and defect-generation rate, and increase developer satisfaction? Can we reduce rework and toil in deployment processes?</p> 
  <p>• <b><i>Hardware resource efficiency.</i></b> Can we reduce the consumption of hardware resources without impacting other forms of impact? Or can we analyze the tradeoffs between hardware resource efficiency gains and one of the other impacts?</p> 
  <p>• <b><i>Strategic capability.</i></b> Can we provide new forms of infrastructure, improved telemetry, etc.? Specializations within infrastructure have a different mix of effects. Some examples:</p> 
  <ul> 
   <li>Effective education improves engineering capacity and should be measured accordingly. Educational efforts can reduce defect generation, rework, and edit-build-test cycles. If possible, these should be primary impact metrics for investments in technical educational programs.</li> 
   <li>Developer tooling can improve engineering capacity through more efficient development, hardware resource efficiency through more efficient use of hardware resources, or strategic capability through telemetry, new platforms, etc.</li> 
   <li>Improved codebase efficiency significantly increases hardware resource efficiency.</li> 
  </ul> 
  <p>Top-level business reports include aggregate impact measures for product portfolios: revenue, costs, and other indicators of business performance. Aggregate impact measures for infrastructure and central developer teams are hardware/cloud resource utilization and DORA or similar metrics. DORA metrics ask: Across teams using the provided infrastructure, how does the infrastructure affect overall engineering (not product) performance and capacity? Infrastructure and development platform teams setting their KPIs (key performance indicators) might ask themselves: <i>How do your customer teams' DORA metrics change after adopting your service?</i></p> 
  <p>We have suggested reducing deployment toil and increasing release cadence. Subtle but important additions to that approach include measuring the right things at the right phases of the workflow, deploying effective defect-detection mechanisms at the right step of the workflow, and properly balancing latency, resource costs, and toil.</p> 
  <p>&nbsp;</p> 
  <h3>A Model for Software Development</h3> 
  <p>Combining these insights and defect cost estimates, we suggest a potential model for software development that focuses on the business side of software development and permits reasoning about efficiency and optimizing processes based on non-counterfactual measurements and estimates. </p> 
  <p>These are the values necessary for a stochastic simulation of the process:</p> 
  <p>• <b><i>Defect-generation rate (developer).</i></b> How skilled is the average developer for a given team? A developer's defect-generation rate decreases over time with practice and learning. Education can be a more targeted, effective, and expensive intervention.</p> 
  <p>• <b><i>Latency (phase, project).</i></b> How much wall clock latency does each phase of the workflow of a team/project add to the software-development process? As discussed earlier, deployment is a factory process, so reducing deployment latency is especially valuable.</p> 
  <p>• <b><i>Hardware capacity cost (phase, project).</i></b> For each phase of the workflow of a team/project, how much machine capacity is consumed to run necessary tests and computation?</p> 
  <p>• <b><i>Engineering capacity cost (phase, project).</i></b> How many people are working during each phase of each project's workflow? During deployment, developer time is often unnecessary. It can also be valuable to reduce human involvement in development, if and when it's reasonable to do so. </p> 
  <p>• <b><i>Defect detection (phase, project).</i></b> What fraction of defects that reach each phase of each project's workflow are filtered out at that point?</p> 
  <p>• <b><i>Defect false negative (phase, project).</i></b> Consider the false-negative rate. How often does each phase of each project's workflow miss defective changes? This delays detection to a later phase, increasing defect duration and resource usage in detection. It may also increase the number of developers affected, especially when defects reach deployment.</p> 
  <p>• <b><i>Defect false positives (phase, project). </i></b>How often does each phase of each project's workflow report failures that would not be defects in production?</p> 
  <p>The repetition of "each phase of each project's workflow" in these components emphasizes the organizational value in workflow consistency. The more tools and infrastructure shared across teams, the easier it is to achieve economies of scale and apply global optimizations. That said, if a given project's needs are substantially distinct from others, the standard workflow may be insufficient for their needs and require local divergence. </p> 
  <p>The last three components recall the earlier section, Defect Cost and Test Fidelity Are in Tension. Smaller, cheaper, faster tests and defect-detection mechanisms are essential but not always representative of quality/fitness signals in production. As a change approaches release, defect-detection signals have higher fidelity, but capacity costs are higher as well. Shifting everything into development testing would not work, because hardware resource efficiency and engineering capacity costs, as well as latency for individual changes, would skyrocket. Neither would shifting everything into release qualification tests, because detection and repair at that late point, with a much larger number of affected developers, is extremely expensive.</p> 
  <p>Thus, the goal should be to build a set of workflow phases that filter out defects to reach an acceptable level of quality with minimal developer latency while minimizing the sum of defect-resolution cost estimates. </p> 
  <p>The logic underpinning standard DORA metrics can be helpful here:</p> 
  <p>• <b><i>Change failure rate.</i></b> Filtering defects properly should lower the rate of outages and bugs that appear in production.</p> 
  <p>• <b><i>Failed deployment recovery time.</i></b> Well-tuned workflows and sufficient release automation can result in two types of positive outcomes:</p> 
  <ul> 
   <li><b>Good monitoring systems.</b> Progress between late-stage workflow phases (release qualification, canary release monitoring) often hinges on metrics and monitoring used by SREs (site reliability engineers) to detect outages. Improving defect-detection capability early in the workflow also improves production monitoring in general. Defects requiring a rollback to a previous version can be caught in monitoring.</li> 
   <li><b>Faster release cadence.</b> If non-catastrophic defects, which do not merit a full rollback, reach production, the faster a fixed release can be cut, validated, and deployed, and the lower the failed deployment recovery time.</li> 
  </ul> 
  <p>• <b><i>From commit to deploy.</i></b> DORA focuses on time "from commit to deploy" specifically, because it permits comparison across changes. This metric should be driven downward for most if not all teams.</p> 
  <p>• <b><i>Release cadence.</i></b> If there is sufficient automation and monitoring in place after workflow optimization, pushing more (smaller) releases minimizes work in progress and maximizes the duration in which changes are generating value. </p> 
  <p>These standard DORA metrics are downstream from submit. These are good metrics because they categorize team/project/product success at a mechanical level. Can you detect enough of the bugs? Can you deploy reliably? Can you meet your SLOs/SLAs (service-level objectives/service-level agreements)? Improvements to project-level technical systems (continuous integration or CI, release, canary, monitoring) improve these system-level metrics. Individual teams can adopt and optimize such systems to improve those metrics. Measuring and seeking to reduce the total amount of human involvement in a given period are important.</p> 
  <p>By contrast, metrics on the human side of the workflow (development) should be tailored to make individual engineers more effective in the creation process. Since individual changes are heterogeneous, usage-of-tools metrics and aggregates can apply to populations far larger than individual teams. The major levers are reducing the number of edit-build-test cycles, increasing usage of IDE features (automation, understanding), and reducing developer friction. The presubmit part of the workflow is affected through improved capabilities, improved documentation, improved education, better design process, better IDEs, improved diagnostics, reduced false-positive rates for presubmits, reduced latency for presubmits, and code review. </p> 
  <p>Developers can give good qualitative signals here. Given that we're speaking about the human/creative/design aspect of the software process, asking humans whether they are productive is a good, if limited, proxy. In DORA, productivity is considered a well-being metric for this reason. Likewise, issues or bottlenecks in the creative process can often appear as frustrations that impact well-being.</p> 
  <p>&nbsp;</p> 
  <h3>Conclusions</h3> 
  <p>By taking a holistic view of the commercial software-development process, we have identified tensions between various factors and where changes in one phase, or to infrastructure, affect other phases. We have distinguished four distinct forms of impact, warned against measuring against unknown counterfactuals, and suggested a consensus mechanism for estimating DDR (defect detection and resolution) costs. Our approach balances product outcomes and the strategic need for change with both the human and machine costs of producing valuable software. With this model, the process of commercial software development could become more comprehensible across roles and levels and therefore more easily improved within an organization. </p> 
  <p>&nbsp;</p> 
  <p><b>Titus Winters</b> is a senior principal scientist at Adobe, focusing on developer experience. He has served on the C++ standards committee, chairing the working group for the design and evolution of the C++ standard library. He also served on the ACM/IEEE/AAAI CS2023 steering committee, helping set curriculum requirements for computer science undergraduate degrees, focusing on the requirements for software engineering. As a thought leader at Google for many years, he focused on C++, software engineering practice, technical debt, and culture. He is the lead author of the book&nbsp;Software Engineering at Google (O'Reilly, 2020).</p> 
  <p><b>Leah Rivers</b> is the director of product management for Google's software foundations. She is a software engineering leader with decades of experience focusing on developers and the platforms and ecosystems crucial to their success. Her background includes working as an engineer and as an executive across a range of organizations including startups, SaaS (software as a service) companies, and high-tech companies including AWS and Google. She cares about the power and potential of harmonizing technology with the individual and social aspects of software development to drive innovation, create meaningful change, and deliver valuable software.</p> 
  <p><b>Salim Virji</b> develops reliable engineering practices and processes for Google's SRE organization, and has built consensus and storage services for Google infrastructure. Salim's interests include distributed systems and machine learning. He has contributed to several books on SRE, including The Site Reliability Workbook (O'Reilly, 2018) and Implementing Service Level Objectives (O'Reilly, 2020).</p> 
  <p>Copyright © 2025 held by owner/author. Publication rights licensed to ACM.</p>  
 <script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'94753dc96be17f86',t:'MTc0ODUxMzgzOS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script> 

	<p>
	
		<img class="floatLeft" src="img/q stamp_small.jpg" width="26" height="45" alt="acmqueue"><br><br>
	
	<em>Originally published in Queue vol. 23, no. 2</em>&#8212;
 	<br>
	Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3733703">ACM Digital Library</a>
	
	</p>
	



<br />
<!--
<a href="https://twitter.com/share" class="twitter-share-button" data-via="ACMQueue">Tweet</a>
-->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<br />

<!--
<fb:like></fb:like>
-->

<br />

<div class="g-plusone" data-size="small" data-annotation="inline" data-width="120"></div>

<!-- these get hooked up to js events -->
<script type="text/javascript">
	addthis_pub             = 'acm';
	addthis_logo            = 'http://queue.acm.org/img/logo_queue_small.gif';
	addthis_logo_background = '#ffffff';
	addthis_logo_color      = '000000';
	addthis_brand           = 'ACM Queue';
	addthis_options         = 'reddit, slashdot, facebook, favorites, email, delicious, digg, technorati, blinklist, furl, myspace, google, live, more';
</script>

<!-- FB Like -->
<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<div id="fb-root"></div>
-->

<!-- Place this tag after the last +1 button tag. -->

<!--
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>

<br />
<script src="https://connect.facebook.net/en_US/all.js#xfbml=1"></script>

<script>
FB.Event.subscribe('edge.create', function(targetUrl) {
  _gaq.push(['_trackSocial', 'facebook', 'like', targetUrl]);
});
</script>
-->



<hr noshade size=1 />




More related articles:

	  <p>
	  <span>Jo&#227;o Varaj&#227;o, Ant&#243;nio Trigo</span> - <a href="detail.cfm?id=3687999"><b>Assessing IT Project Success: Perception vs. Reality</b></a>
	  <br />
	  This study has significant implications for practice, research, and education by providing new insights into IT project success. It expands the body of knowledge on project management by reporting project success (and not exclusively project management success), grounded in several objective criteria such as deliverables usage by the client in the post-project stage, hiring of project-related support/maintenance services by the client, contracting of new projects by the client, and vendor recommendation by the client to potential clients. Researchers can find a set of criteria they can use when studying and reporting the success of IT projects, thus expanding the current perspective on evaluation and contributing to more accurate conclusions.
	  </p>
	  <br />

	  <p>
	  <span>Abi Noda, Margaret-Anne Storey, Nicole Forsgren, Michaela Greiler</span> - <a href="detail.cfm?id=3595878"><b>DevEx: What Actually Drives Productivity</b></a>
	  <br />
	  Developer experience focuses on the lived experience of developers and the points of friction they encounter in their everyday work. In addition to improving productivity, DevEx drives business performance through increased efficiency, product quality, and employee retention. This paper provides a practical framework for understanding DevEx, and presents a measurement framework that combines feedback from developers with data about the engineering systems they interact with. These two frameworks provide leaders with clear, actionable insights into what to measure and where to focus in order to improve developer productivity.
	  </p>
	  <br />

	  <p>
	  <span>Jenna Butler, Catherine Yeh</span> - <a href="detail.cfm?id=3534860"><b>Walk a Mile in Their Shoes</b></a>
	  <br />
	  Covid has changed how people work in many ways, but many of the outcomes have been paradoxical in nature. What works for one person may not work for the next (or even the same person the next day), and we have yet to figure out how to predict exactly what will work for everyone. As you saw in the composite personas described here, some people struggle with isolation and loneliness, have a hard time connecting socially with their teams, or find the time pressures of hybrid work with remote teams to be overwhelming. Others relish this newfound way of working, enjoying more time with family, greater flexibility to exercise during the day, a better work/life balance, and a stronger desire to contribute to the world.
	  </p>
	  <br />

	  <p>
	  <span>Bridget Kromhout</span> - <a href="detail.cfm?id=3185224"><b>Containers Will Not Fix Your Broken Culture (and Other Hard Truths)</b></a>
	  <br />
	  We focus so often on technical anti-patterns, neglecting similar problems inside our social structures. Spoiler alert: the solutions to many difficulties that seem technical can be found by examining our interactions with others. Let&#8217;s talk about five things you&#8217;ll want to know when working with those pesky creatures known as humans.
	  </p>
	  <br />


<hr noshade size=1 />





<hr noshade size=1 />

	<p>
	<a href='#'><img src='https://queue.acm.org/img/logo_acm.gif' /></a>
	<br />
	&copy; ACM, Inc. All Rights Reserved.
	</p>

</div>



</body>
</html>